[
  {
    "objectID": "cloud-paradigm.html",
    "href": "cloud-paradigm.html",
    "title": "NASA and the Cloud Paradigm",
    "section": "",
    "text": "Slides that introduce NASA Earthdata Cloud & the Cloud Paradigm."
  },
  {
    "objectID": "schedule.html#workshop-schedule",
    "href": "schedule.html#workshop-schedule",
    "title": "Schedule",
    "section": "Workshop Schedule",
    "text": "Workshop Schedule\n\nDay 1 - April 12, 2022\n\n\n\nTime, PST (UTC-7)\nEvent\nLeads/Instructors\n\n\n\n\n2:00 pm\nWelcome / Workshop Expectations\nAaron Friesz (LP DAAC) / Christine Lee (JPL)\n\n\n2:10 pm\nECOSTRESS Build 7 collection Overview\nChristine Lee & Gregory Halverson (JPL)\n\n\n2:25 pm\nEarthdata Cloud Overview\nAaron Friesz (LP DAAC)\n\n\n2:55 pm\nBreak/QA\n\n\n\n3:00 pm\nEarthdata Search Client (GUI)\nAaron Friesz (LP DAAC)\n\n\n3:30 pm\nBreak\n\n\n\n4:00 pm\nGetting set up / Intro to Pangeo\nAaron Friesz (LP DAAC)\n\n\n4:30 pm\nIntro to xarray & hvplot\nAaron Friesz (LP DAAC)\n\n\n5:15 pm\nBreak/QA\n\n\n\n\n\nClosing Day 1\n\nThank you!\nJupyterHub: close out.\n\nclose out your JupyterHub instance if you are finished for the day, following these instructions.\n\nContinued work.\n\nYou’re welcome to continue working beyond the workshop daily scheduled session, using JupyterHub.\n\nAgenda for tomorrow: what’s coming next.\n\n\n\n\nDay 2 - April 13, 2022\n\n\n\nTime, PST (UTC-7)\nEvent\nLeads/Instructors\n\n\n\n\n2:00 pm\nWelcome Back / Earthdata Authentication: Set up netrc file & Generate EDL Tokens\nAaron Friesz (LP DAAC)\n\n\n2:10 pm\nEarthdata Cloud: Search and Discovery - CMR API\nAaron Friesz (LP DAAC)\n\n\n2:45 pm\nBreak/QA\n\n\n\n2:50 pm\nEarthdata Cloud: Search and Discovery - Data Access\nAaron Friesz (LP DAAC)\n\n\n3:30 pm\nBreak\n\n\n\n4:00 pm\nComparing TNC Tree Data to ECOSTRESS Use Case\nGregory Halverson (JPL)\n\n\n5:00 pm\nBreak/QA\n\n\n\n\n\nClosing Day 2\n\nThank you!\nYou will continue to have access to the 2i2c JupyterHub in AWS for two weeks following the ECOSTRESS Cloud Workshop. You may use that time to continue work and all learn more about migrating data accass routines and science workflows to the Cloud. This cloud compute environment is supported by the NASA Openscapes project."
  },
  {
    "objectID": "schedule.html#getting-help-during-the-workshop",
    "href": "schedule.html#getting-help-during-the-workshop",
    "title": "Schedule",
    "section": "Getting help during the Workshop",
    "text": "Getting help during the Workshop\nWe will use the ECOSTRESS Slack Workspace as our main channels for help. Please use Slack to post questions."
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Login_Token.html#read-netrc-file-to-get-edl-information",
    "href": "how-tos/authentication/NASA_Earthdata_Login_Token.html#read-netrc-file-to-get-edl-information",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Read netrc file to get EDL information",
    "text": "Read netrc file to get EDL information\n\ndef get_edl_creds():\n    nc = netrc.netrc()\n    remoteHostName = \"urs.earthdata.nasa.gov\"\n    edl_creds = nc.authenticators(remoteHostName)\n    return {'username':edl_creds[0], 'password':edl_creds[2]}"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Login_Token.html#specify-the-urls-for-generating-new-tokens-listing-available-tokens-and-revoking-available-tokens",
    "href": "how-tos/authentication/NASA_Earthdata_Login_Token.html#specify-the-urls-for-generating-new-tokens-listing-available-tokens-and-revoking-available-tokens",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Specify the URLs for generating new tokens, listing available tokens, and revoking available tokens",
    "text": "Specify the URLs for generating new tokens, listing available tokens, and revoking available tokens\n\nedl_token_urls = {\n    'generate_token':'https://urs.earthdata.nasa.gov/api/users/token',\n    'list_token':'https://urs.earthdata.nasa.gov/api/users/tokens',\n    'revoke_token': 'https://urs.earthdata.nasa.gov/api/users/revoke_token'\n}"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Login_Token.html#create-a-hidden-directory-to-store-the-output-json-file-with-edl-tokens",
    "href": "how-tos/authentication/NASA_Earthdata_Login_Token.html#create-a-hidden-directory-to-store-the-output-json-file-with-edl-tokens",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Create a hidden directory to store the output json file with EDL tokens",
    "text": "Create a hidden directory to store the output json file with EDL tokens\n\nif not os.path.isdir('../../../.hidden_dir'):\n    os.mkdir('../../../.hidden_dir')"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Login_Token.html#check-if-a-valid-token-exists-or-generate-a-new-token",
    "href": "how-tos/authentication/NASA_Earthdata_Login_Token.html#check-if-a-valid-token-exists-or-generate-a-new-token",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Check if a valid token exists or generate a new token",
    "text": "Check if a valid token exists or generate a new token\n\nif len(list_tokens := requests.get(edl_token_urls['list_token'], auth=(get_edl_creds()['username'], get_edl_creds()['password'])).json()) < 1:\n    #print('No tokens available. Generating new Earthdata Login Token ...')\n    generate_token_url = \"https://urs.earthdata.nasa.gov/api/users/token\"\n    generate_token_req = requests.post(edl_token_urls['generate_token'], auth=(get_edl_creds()['username'], get_edl_creds()['password']))\n    token = generate_token_req.json()\n    with open(\"../../../.hidden_dir/edl_token.json\", \"w\") as outfile:\n        json.dump(token, outfile)\n    print(f'Your EDL token information can be found here: {os.path.abspath(\"../../../.hidden_dir/edl_token.json\")}')\nelif datetime.strptime(list_tokens[0]['expiration_date'], \"%m/%d/%Y\") < datetime.now():\n    #print('Available token is expired. Generating a new Earthdata Login Token ...')\n    revoke_token = requests.post(f\"{edl_token_urls['revoke_token']}?token={list_tokens[0]}\", auth=(get_edl_creds()['username'], get_edl_creds()['password']))\n    generate_token_req = requests.post(edl_token_urls['generate_token'], auth=(get_edl_creds()['username'], get_edl_creds()['password']))\n    token = generate_token_req.json()\n    with open(\"../../../.hidden_dir/edl_token.json\", \"w\") as outfile:\n        json.dump(token, outfile)\n    print(f'Your EDL token information can be found here: {os.path.abspath(\"../../../.hidden_dir/edl_token.json\")}')\nelse:\n    #print('Earthdata Login Token Found ...')\n    with open(\"../../../.hidden_dir/edl_token.json\", \"w\") as outfile:\n        json.dump(list_tokens[0], outfile)\n    print(f'Your EDL token information can be found here: {os.path.abspath(\"../../../.hidden_dir/edl_token.json\")}')"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Login_Token.html#resources",
    "href": "how-tos/authentication/NASA_Earthdata_Login_Token.html#resources",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Resources",
    "text": "Resources\n\nhttps://wiki.earthdata.nasa.gov/display/EL/How+to+Generate+a+User+Token\nhttps://urs.earthdata.nasa.gov/documentation/for_users/user_token"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Authentication.html#summary",
    "href": "how-tos/authentication/NASA_Earthdata_Authentication.html#summary",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nThis notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\nEarthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\nAuthentication via netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin <USERNAME>\npassword <PASSWORD>\n<USERNAME> and <PASSWORD> would be replaced by your actual Earthdata Login username and password respectively."
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Authentication.html#import-required-packages",
    "href": "how-tos/authentication/NASA_Earthdata_Authentication.html#import-required-packages",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\nfrom netrc import netrc\nfrom subprocess import Popen\nfrom platform import system\nfrom getpass import getpass\nimport os\n\nThe code below will:\n\ncheck what operating system (OS) is being used to determine which netrc file to check for/create (.netrc or _netrc)\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\n\nurs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\nprompts = ['Enter NASA Earthdata Login Username: ',\n           'Enter NASA Earthdata Login Password: ']\n\n# Determine the OS (Windows machines usually use an '_netrc' file)\nnetrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n\n# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\ntry:\n    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n    netrc(netrcDir).authenticators(urs)[0]\n\n# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\nexcept FileNotFoundError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('touch {0}{2} | echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n    # Set restrictive permissions\n    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n\n    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\nexcept TypeError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n\n\nSee if the file was created\nIf the file was created, we’ll see a .netrc file (_netrc for Window OS) in the list printed below. To view the contents from a Jupyter environment, click File on the top toolbar, select Open from Path…, type .netrc, and click Open. The .netrc file will open within the text editor.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n\n!ls -al ~/"
  },
  {
    "objectID": "how-tos/additional_resources/Direct_Access_netCDF_simple.html",
    "href": "how-tos/additional_resources/Direct_Access_netCDF_simple.html",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "Direct Access - ECCO netCDF example\n\nGetting Started\nIn this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF datasets into a single xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\n\n\nRequirements\n\nAWS\nThis notebook should be running in an EC2 instance in AWS region us-west-2, as previously mentioned. We recommend using an EC2 with at least 8GB of memory available.\nThe notebook was developed and tested using a t2.small instance (_ CPUs; 8GB memory). Python 3\nMost of these imports are from the Python standard library. However, you will need to install these packages into your Python 3 environment if you have not already done so:\n\ns3fs\nrequests\npandas\nxarray\nmatplotlib\ncartopy\n\n\n\n\nLearning Objectives\n\nimport needed libraries\ndefine dataset of interest\nauthenticate for NASA Earthdata archive (Earthdata Login)\nobtain AWS credentials for Earthdata DAAC archive in AWS S3\naccess DAAC data directly from the in-region S3 bucket without moving or downloading any files to your local (cloud) workspace\nplot the first time step in the data\n\n\nimport os\nimport subprocess\nfrom os.path import dirname, join\n\n# Access EDS\nimport requests\n\n# Access AWS S3\nimport boto3\nimport s3fs\n\n# Read and work with datasets\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport cartopy\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeat\n\n\nDefine dataset of interest\nIn this case it’s the following string that unique identifies the collection of monthly, 0.5-degree sea surface height data.\n\nShortName = \"ECCO_L4_SSH_05DEG_MONTHLY_V4R4\"\n\n\n\n\nEarthdata login\nYou should have a .netrc file set up like:\nmachine urs.earthdata.nasa.gov login <username> password <password>\nSee the following (Authentication for NASA Earthdata tutorial)[https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/04_NASA_Earthdata_Authentication.html]\n\n\nAWS credentials to Access Data from S3\nPass credentials and configuration to AWS so we can interact with S3 objects from applicable buckets. For now, each DAAC has different AWS credentials endpoints. LP DAAC and PO.DAAC are listed here:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\n}\n\nIn this example we’re interested in the ECCO data collection from PO.DAAC in Earthdata Cloud in AWS S3, so we specify the podaac endpoint in the next code block.\nSet up an s3fs session for authneticated access to ECCO netCDF files in s3:\n\ndef begin_s3_direct_access(url: str=s3_cred_endpoint['podaac']):\n    response = requests.get(url).json()\n    return s3fs.S3FileSystem(key=response['accessKeyId'],\n                             secret=response['secretAccessKey'],\n                             token=response['sessionToken'],\n                             client_kwargs={'region_name':'us-west-2'})\n\nfs = begin_s3_direct_access()\n\nGet a list of netCDF files located at the S3 path corresponding to the ECCO V4r4 monthly sea surface height dataset on the 0.5-degree latitude/longitude grid, for year 2015.\n\nssh_Files = fs.glob(join(\"podaac-ops-cumulus-protected/\", ShortName, \"*2015*.nc\"))\n\nlen(ssh_Files)\n\n12\n\n\n\nAccess in-region S3 cloud data without moving files\nNow that we have authenticated in AWS, this next code block accesses data directly from the NASA Earthdata archive in an S3 bucket in us-west-2 region, without downloading or moving any files into your user cloud workspace (instnace).\nOpen with the netCDF files using the s3fs package, then load them all at once into a concatenated xarray dataset.\n\nssh_Dataset = xr.open_mfdataset(\n    paths=[fs.open(f) for f in ssh_Files],\n    combine='by_coords',\n    mask_and_scale=True,\n    decode_cf=True,\n    chunks={'latitude': 60,   # These were chosen arbitrarily. You must specify \n            'longitude': 120, # chunking that is suitable to the data and target\n            'time': 100}      # analysis.\n)\n\nssh = ssh_Dataset.SSH\n\nprint(ssh)\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              [-1.88057721]\n    valid_max:              [1.42077196]\n\n\n\n\n\nPlot the gridded sea surface height time series\nBut only the timesteps beginning in 2015:\n\nssh_after_201x = ssh[ssh['time.year']>=2015,:,:]\n\nprint(ssh_after_201x)\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              [-1.88057721]\n    valid_max:              [1.42077196]\n\n\nPlot the grid for the first time step using a Robinson projection. Define a helper function for consistency throughout the notebook:\n\ndef make_figure(proj):\n    fig = plt.figure(figsize=(16,6))\n    ax = fig.add_subplot(1, 1, 1, projection=proj)\n    ax.add_feature(cfeat.LAND)\n    ax.add_feature(cfeat.OCEAN)\n    ax.add_feature(cfeat.COASTLINE)\n    ax.add_feature(cfeat.BORDERS, linestyle='dotted')\n    return fig, ax\n\nfig, ax = make_figure(proj=ccrs.Robinson())\n\nssh_after_201x.isel(time=0).plot(ax=ax, transform=ccrs.PlateCarree(), cmap='Spectral_r')\n\n<cartopy.mpl.geocollection.GeoQuadMesh at 0x7fd040602b20>\n\n\n\n\n\n\n\nAdditional Resources\n\nFull example with additional plots and use cases here: https://github.com/podaac/ECCO/blob/main/Data_Access/cloud_direct_access_s3.ipynb"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#summary",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#summary",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will access data for the Harmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002) data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band.\nWe will access a single COG file, L30 red band (0.64 – 0.67 μm), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#requirements",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#requirements",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#learning-objectives",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#learning-objectives",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to configure you Python work environment to access Cloud Optimized geoTIFF (COG) files\nhow to access HLS COG files\nhow to plot the data"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#cloud-optimized-geotiff-cog",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#cloud-optimized-geotiff-cog",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Cloud Optimized GeoTIFF (COG)",
    "text": "Cloud Optimized GeoTIFF (COG)\nUsing Harmonized Landsat Sentinel-2 (HLS) version 2.0\n\nImport Packages\n\nimport os\nfrom osgeo import gdal\nimport rasterio as rio\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#workspace-environment-setup",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#workspace-environment-setup",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Workspace Environment Setup",
    "text": "Workspace Environment Setup\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL configurations we need to access the data from Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nGDAL environment variables must be configured to access COGs from Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\nIn this example we’re interested in the HLS L30 data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\nhttps_url = 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#https-data-access",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html#https-data-access",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "HTTPS Data Access",
    "text": "HTTPS Data Access\nRead in the HLS s3 URL for the L30 red band (0.64 – 0.67 μm) into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(https_url)\nda\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_red = da.squeeze('band', drop=True)\nda_red\n\nPlot the dataarray, representing the L30 red band, using hvplot.\n\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n\nExit the context manager.\n\nrio_env.__exit__()"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#summary",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#summary",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access a single netCDF file from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataset. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#requirements",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#requirements",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#learning-objectives",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#learning-objectives",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of ECCO_L4_SSH_05DEG_MONTHLY_V4R4 data in S3\nhow to plot the data"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#import-packages",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#import-packages",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Import Packages",
    "text": "Import Packages\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport os\nimport requests\nimport s3fs\nfrom osgeo import gdal\nimport xarray as xr\nimport hvplot.xarray\nimport holoviews as hv"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#get-temporary-aws-credentials",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#get-temporary-aws-credentials",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Get Temporary AWS Credentials",
    "text": "Get Temporary AWS Credentials\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('podaac')\n#temp_creds_req"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#set-up-an-s3fs-session-for-direct-access",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#set-up-an-s3fs-session-for-direct-access",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Set up an s3fs session for Direct Access",
    "text": "Set up an s3fs session for Direct Access\ns3fs sessions are used for authenticated access to s3 bucket and allows for typical file-system style operations. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\nfs_s3 = s3fs.S3FileSystem(anon=False, \n                          key=temp_creds_req['accessKeyId'], \n                          secret=temp_creds_req['secretAccessKey'], \n                          token=temp_creds_req['sessionToken'])\n\nIn this example we’re interested in the ECCO data collection from NASA’s PO.DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\ns3_url = 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-01_ECCO_V4r4_latlon_0p50deg.nc'"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#direct-in-region-access",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#direct-in-region-access",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Direct In-region Access",
    "text": "Direct In-region Access\nOpen with the netCDF file using the s3fs package, then load the cloud asset into an xarray dataset.\n\ns3_file_obj = fs_s3.open(s3_url, mode='rb')\n\n\nssh_ds = xr.open_dataset(s3_file_obj, engine='h5netcdf')\nssh_ds\n\nGet the SSH variable as an xarray dataarray\n\nssh_da = ssh_ds.SSH\nssh_da\n\nPlot the SSH dataarray for time 2015-01-16T12:00:00 using hvplot.\n\nssh_da.hvplot.image(x='longitude', y='latitude', cmap='Spectral_r', aspect='equal').opts(clim=(ssh_da.attrs['valid_min'][0],ssh_da.attrs['valid_max'][0]))"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#resources",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html#resources",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Resources",
    "text": "Resources\nDirect access to ECCO data in S3 (from us-west-2)\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#outline",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#outline",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Outline:",
    "text": "Outline:\n\nIntroduction to NASA Earthdata’s move to the cloud\n\nBackground and motivation\nEnabling Open Science via “Analysis-in-Place”\nResources for cloud adopters: NASA Earthdata Openscapes\n\nNASA Earthdata discovery and access in the cloud\n\nPart 1: Explore Earthdata cloud data availablity\nPart 2: Working with Cloud-Optimized GeoTIFFs using NASA’s Common Metadata Repository Spatio-Temporal Assett Catalog (CMR-STAC)\nPart 3: Working with Zarr-formatted data using NASA’s Harmony cloud transformation service\n\n\n\nTutorial materials are adapted from repos on the NASA Openscapes public Github:\n\nThis notebook source code: update https://github.com/NASA-Openscapes/2021-Cloud-Workshop-AGU/tree/main/how-tos\nAlso available via online Quarto book: update https://nasa-openscapes.github.io/2021-Cloud-Workshop-AGU/"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#the-nasa-earthdata-archive-continues-to-grow",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#the-nasa-earthdata-archive-continues-to-grow",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "The NASA Earthdata archive continues to grow",
    "text": "The NASA Earthdata archive continues to grow\n\n\n\nEOSDIS Data Archive"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#the-nasa-earthdata-cloud-evolution",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#the-nasa-earthdata-cloud-evolution",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "The NASA Earthdata Cloud Evolution",
    "text": "The NASA Earthdata Cloud Evolution\n \n\nNASA Distributed Active Archive Centers (DAACs) are continuing to migrate data to the Earthdata Cloud\n\nSupporting increased data volume as new, high-resolution remote sensing missions launch in the coming years\nData hosted via Amazon Web Services, or AWS\nDAACs continuing to support tools, services, and tutorial resources for our user communities"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#nasa-earthdata-cloud-as-an-enabler-of-open-science",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#nasa-earthdata-cloud-as-an-enabler-of-open-science",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "NASA Earthdata Cloud as an enabler of Open Science",
    "text": "NASA Earthdata Cloud as an enabler of Open Science\n\nReducing barriers to large-scale scientific research in the era of “big data”\nIncreasing community contributions with hands-on engagement\nPromoting reproducible and shareable workflows without relying on local storage systems\n\n\n\n\nOpen Data"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#data-and-analysis-co-located-in-place",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#data-and-analysis-co-located-in-place",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Data and Analysis co-located “in place”",
    "text": "Data and Analysis co-located “in place”\n\n\n\nEarthdata Cloud Paradigm"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#building-nasa-earthdata-cloud-resources",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#building-nasa-earthdata-cloud-resources",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Building NASA Earthdata Cloud Resources",
    "text": "Building NASA Earthdata Cloud Resources\nShow slide with 3 panels of user resources\nEmphasize that the following tutorials are short examples that were taken from the tutorial resources we have been building for our users"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#nasa-earthdata-cloud-discovery-and-access-using-open-source-technologies",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#nasa-earthdata-cloud-discovery-and-access-using-open-source-technologies",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "NASA Earthdata Cloud: Discovery and access using open source technologies",
    "text": "NASA Earthdata Cloud: Discovery and access using open source technologies\nThe following tutorial demonstrates several basic end-to-end workflows to interact with data “in-place” from the NASA Earthdata Cloud, accessing Amazon Web Services (AWS) Single Storage Solution (S3) data locations without the need to download data. While the data can be downloaded locally, the cloud offers the ability to scale compute resources to perform analyses over large areas and time spans, which is critical as data volumes continue to grow.\nAlthough the examples we’re working with in this notebook only focuses on a small time and area for demonstration purposes, this workflow can be modified and scaled up to suit a larger time range and region of interest.\n\nDatasets of interest:\n\nHarmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002)\n\nSurface reflectance (SR) and top of atmosphere (TOA) brightness data\nGlobal observations of the land every 2–3 days at 30-meter (m)\nCloud Optimized GeoTIFF (COG) format\n\nECCO Sea Surface Height - Daily Mean 0.5 Degree (Version 4 Release 4)(10.5067/ECG5D-SSH44).\n\nDaily-averaged dynamic sea surface height\nTime series of monthly NetCDFs on a 0.5-degree latitude/longitude grid."
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#part-1-explore-data-hosted-in-the-earthdata-cloud",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#part-1-explore-data-hosted-in-the-earthdata-cloud",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Part 1: Explore Data hosted in the Earthdata Cloud",
    "text": "Part 1: Explore Data hosted in the Earthdata Cloud\n\nEarthdata Search Demonstration\nFrom Earthdata Search https://search.earthdata.nasa.gov, use your Earthdata login credentials to log in. You can create an Earthdata Login account at https://urs.earthdata.nasa.gov.\nIn this example we are interested in the ECCO dataset, hosted by the PO.DAAC. This dataset is available from the NASA Earthdata Cloud archive hosted in AWS cloud.\nClick on the “Available from AWS Cloud” filter option on the left. Here, 39 matching collections were found with the ECCO monthly SSH search, and for the time period for year 2015. The latter can be done using the calendar icon on the left under the search box. Scroll down the list of returned matches until we see the dataset of interest, in this case ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4).\n\n\nView and Select Data Access Options\nClicking on the ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4) dataset, we now see a list of files (granules) that are part of the dataset (collection). We can click on the green + symbol to add a few files to our project. Here we added the first 3 listed for 2015. Then click on the green button towards the bottom that says “Download”. This will take us to another page with options to customize our download or access link(s).\n\n\n\nFigure caption: Select granules and click download"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#access-options",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#access-options",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Access Options",
    "text": "Access Options\nSelect the “Direct Download” option to view Access options via Direct Download and from the AWS Cloud. Additional options to customize the data are also available for this dataset.\n\n\n\nFigure caption: Customize your download or access"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#earthdata-cloud-access-information",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#earthdata-cloud-access-information",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Earthdata Cloud access information",
    "text": "Earthdata Cloud access information\nClicking the green Download Data button again, will take us to the final page for instructions to download and links for data access in the cloud. The AWS S3 Access tab provides the S3:// links, which is what we would use to access the data directly in-region (us-west-2) within the AWS cloud.\nE.g.: s3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc where s3 indicates data is stored in AWS S3 storage, podaac-ops-cumulus-protected is the bucket, and ECCO_L4_SSH_05DEG_MONTHLY_V4R4 is the object prefix (the latter two are also listed in the dataset collection information under Cloud Access (step 3 above)).\n\nIntegrate file links into programmatic workflow, locally or in the AWS cloud.\nIn the next two examples we will work programmatically in the cloud to access datasets of interest, to get us set up for further scientific analysis of choice. There are several ways to do this. One way to connect the search part of the workflow we just did in Earthdata Search to our next steps working in the cloud is to simply copy/paste the s3:// links provides in Step 4 above into a JupyterHub notebook or script in our cloud workspace, and continue the data analysis from there.\nOne could also copy/paste the s3:// links and save them in a text file, then open and read the text file in the notebook or script in the JupyterHub in the cloud.\n\n\n\nFigure caption: Direct S3 access"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#part-2-working-with-cloud-optimized-geotiffs-using-nasas-common-metadata-repository-spatio-temporal-assett-catalog-cmr-stac",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#part-2-working-with-cloud-optimized-geotiffs-using-nasas-common-metadata-repository-spatio-temporal-assett-catalog-cmr-stac",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Part 2: Working with Cloud-Optimized GeoTIFFs using NASA’s Common Metadata Repository Spatio-Temporal Assett Catalog (CMR-STAC)",
    "text": "Part 2: Working with Cloud-Optimized GeoTIFFs using NASA’s Common Metadata Repository Spatio-Temporal Assett Catalog (CMR-STAC)\nIn this example we will access the NASA’s Harmonized Landsat Sentinel-2 (HLS) version 2 assets, which are archived in cloud optimized geoTIFF (COG) format archived by the Land Processes (LP) DAAC. The COGs can be used like any other GeoTIFF file, but have some added features that make them more efficient within the cloud data access paradigm. These features include: overviews and internal tiling.\n\nBut first, what is STAC?\nSpatioTemporal Asset Catalog (STAC) is a specification that provides a common language for interpreting geospatial information in order to standardize indexing and discovering data.\nThe STAC specification is made up of a collection of related, yet independent specifications that when used together provide search and discovery capabilities for remote assets.\n\nFour STAC Specifications\nSTAC Catalog (aka DAAC Archive)\nSTAC Collection (aka Data Product)\nSTAC Item (aka Granule)\nSTAC API\n\n\n\nCMR-STAC API\nThe CMR-STAC API is NASA’s implementation of the STAC API specification for all NASA data holdings within EOSDIS. The current implementation does not allow for querries accross the entire NASA catalog. Users must execute searches within provider catalogs (e.g., LPCLOUD) to find the STAC Items they are searching for. All the providers can be found at the CMR-STAC endpoint here: https://cmr.earthdata.nasa.gov/stac/.\nIn this example, we will query the LPCLOUD provider to identify STAC Items from the Harmonized Landsat Sentinel-2 (HLS) collection that fall within our region of interest (ROI) and within our specified time range."
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#import-packages",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#import-packages",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Import packages",
    "text": "Import packages\n\nimport os\nimport requests \nimport boto3\nfrom osgeo import gdal\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv\n\nfrom pystac_client import Client  \nfrom collections import defaultdict    \nimport json\nimport geopandas\nimport geoviews as gv\nfrom cartopy import crs\ngv.extension('bokeh', 'matplotlib')"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#connect-to-the-cmr-stac-api",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#connect-to-the-cmr-stac-api",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Connect to the CMR-STAC API",
    "text": "Connect to the CMR-STAC API\n\nSTAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n\n\nprovider_cat = Client.open(STAC_URL)\n\n\nConnect to the LPCLOUD Provider/STAC Catalog\nFor this next step we need the provider title (e.g., LPCLOUD). We will add the provider to the end of the CMR-STAC API URL (i.e., https://cmr.earthdata.nasa.gov/stac/) to connect to the LPCLOUD STAC Catalog.\n\ncatalog = Client.open(f'{STAC_URL}/LPCLOUD/')\n\nSince we are using a dedicated client (i.e., pystac-client.Client) to connect to our STAC Provider Catalog, we will have access to some useful internal methods and functions (e.g., get_children() or get_all_items()) we can use to get information from these objects."
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#search-for-stac-items",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#search-for-stac-items",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Search for STAC Items",
    "text": "Search for STAC Items\nWe will define our ROI using a geojson file containing a small polygon feature in western Nebraska, USA. We’ll also specify the data collections and a time range for our example.\n\nRead in a geojson file and plot\nReading in a geojson file with geopandas and extract coodinates for our ROI.\n\nfield = geopandas.read_file('../data/ne_w_agfields.geojson')\nfieldShape = field['geometry'][0]\nroi = json.loads(field.to_json())['features'][0]['geometry']\n\nWe can plot the polygon using the geoviews package that we imported as gv with ‘bokeh’ and ‘matplotlib’ extensions. The following has reasonable width, height, color, and line widths to view our polygon when it is overlayed on a base tile map.\n\nbase = gv.tile_sources.EsriImagery.opts(width=650, height=500)\nfarmField = gv.Polygons(fieldShape).opts(line_color='yellow', line_width=10, color=None)\nbase * farmField\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe will now start to specify the search criteria we are interested in, i.e, the date range, the ROI, and the data collections, that we will pass to the STAC API."
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#search-the-cmr-stac-api-with-our-search-criteria",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#search-the-cmr-stac-api-with-our-search-criteria",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Search the CMR-STAC API with our search criteria",
    "text": "Search the CMR-STAC API with our search criteria\nNow we can put all our search criteria together using catalog.search from the pystac_client package. STAC Collection is synonomous with what we usually consider a NASA data product. Desired STAC Collections are submitted to the search API as a list containing the collection id. Let’s focus on S30 and L30 collections.\n\ncollections = ['HLSL30.v2.0', 'HLSS30.v2.0']\n\ndate_range = \"2021-05/2021-08\"\n\nsearch = catalog.search(\n    collections=collections,\n    intersects=roi,\n    datetime=date_range,\n    limit=100\n)\n\n\nView STAC Items that matched our search query\n\nprint('Matching STAC Items:', search.matched())\nitem_collection = search.get_all_items()\nitem_collection[0].to_dict()\n\nMatching STAC Items: 113\n\n\n{'type': 'Feature',\n 'stac_version': '1.0.0',\n 'id': 'HLS.L30.T13TGF.2021124T173013.v2.0',\n 'properties': {'datetime': '2021-05-04T17:30:13.428000Z',\n  'start_datetime': '2021-05-04T17:30:13.428Z',\n  'end_datetime': '2021-05-04T17:30:37.319Z',\n  'eo:cloud_cover': 36},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-101.5423534, 40.5109845],\n    [-101.3056118, 41.2066375],\n    [-101.2894253, 41.4919436],\n    [-102.6032964, 41.5268623],\n    [-102.638891, 40.5386175],\n    [-101.5423534, 40.5109845]]]},\n 'links': [{'rel': 'self',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/items/HLS.L30.T13TGF.2021124T173013.v2.0'},\n  {'rel': 'parent',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0'},\n  {'rel': 'collection',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0'},\n  {'rel': <RelType.ROOT: 'root'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/',\n   'type': <MediaType.JSON: 'application/json'>,\n   'title': 'LPCLOUD'},\n  {'rel': 'provider', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.json'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.umm_json'}],\n 'assets': {'B11': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B11.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B11.tif'},\n  'B07': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B07.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B07.tif'},\n  'SAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.SAA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.SAA.tif'},\n  'B06': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B06.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B06.tif'},\n  'B09': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B09.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B09.tif'},\n  'B10': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B10.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B10.tif'},\n  'VZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.VZA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.VZA.tif'},\n  'SZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.SZA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.SZA.tif'},\n  'B01': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B01.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B01.tif'},\n  'VAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.VAA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.VAA.tif'},\n  'B05': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B05.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B05.tif'},\n  'B02': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B02.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B02.tif'},\n  'Fmask': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.Fmask.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.Fmask.tif'},\n  'B03': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B03.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B03.tif'},\n  'B04': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B04.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B04.tif'},\n  'browse': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.jpg',\n   'type': 'image/jpeg',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.jpg'},\n  'metadata': {'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.xml',\n   'type': 'application/xml'}},\n 'bbox': [-102.638891, 40.510984, -101.289425, 41.526862],\n 'stac_extensions': ['https://stac-extensions.github.io/eo/v1.0.0/schema.json'],\n 'collection': 'HLSL30.v2.0'}"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#filtering-stac-items",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#filtering-stac-items",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Filtering STAC Items",
    "text": "Filtering STAC Items\nBelow we will loop through and filter the item_collection by a specified cloud cover as well as extract the band we’d need to do an Enhanced Vegetation Index (EVI) calculation for a future analysis. We will also specify the STAC Assets (i.e., bands/layers) of interest for both the S30 and L30 collections (also in our collections variable above) and print out the first ten links, converted to s3 locations:\n\ncloudcover = 25\n\ns30_bands = ['B8A', 'B04', 'B02', 'Fmask']    # S30 bands for EVI calculation and quality filtering -> NIR, RED, BLUE, Quality \nl30_bands = ['B05', 'B04', 'B02', 'Fmask']    # L30 bands for EVI calculation and quality filtering -> NIR, RED, BLUE, Quality \n\nevi_band_links = []\n\nfor i in item_collection:\n    if i.properties['eo:cloud_cover'] <= cloudcover:\n        if i.collection_id == 'HLSS30.v2.0':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = s30_bands\n        elif i.collection_id == 'HLSL30.v2.0':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = l30_bands\n\n        for a in i.assets:\n            if any(b==a for b in evi_bands):\n                evi_band_links.append(i.assets[a].href)\n                \ns3_links = [l.replace('https://data.lpdaac.earthdatacloud.nasa.gov/', 's3://') for l in evi_band_links]\ns3_links[:10]\n\n['s3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B05.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.Fmask.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B02.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B02.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B05.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.Fmask.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T14TKL.2021133T173859.v2.0/HLS.S30.T14TKL.2021133T173859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T14TKL.2021133T173859.v2.0/HLS.S30.T14TKL.2021133T173859.v2.0.B8A.tif']"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#access-s3-storage-location",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#access-s3-storage-location",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Access s3 storage location",
    "text": "Access s3 storage location\nAccess s3 credentials from LP.DAAC and create a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\ns3_cred_endpoint = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\ntemp_creds_req = requests.get(s3_cred_endpoint).json()\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nGDAL Configurations\nGDAL is a foundational piece of geospatial software that is leveraged by several popular open-source, and closed, geospatial software. The rasterio package is no exception. Rasterio leverages GDAL to, among other things, read and write raster data files, e.g., GeoTIFFs/Cloud Optimized GeoTIFFs. To read remote files, i.e., files/objects stored in the cloud, GDAL uses its Virtual File System API. In a perfect world, one would be able to point a Virtual File System (there are several) at a remote data asset and have the asset retrieved, but that is not always the case. GDAL has a host of configurations/environmental variables that adjust its behavior to, for example, make a request more performant or to pass AWS credentials to the distribution system. Below, we’ll identify the evironmental variables that will help us get our data from cloud\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\n<rasterio.env.Env at 0x7f64510812e0>\n\n\n\ns3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'\n# s3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif'"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#read-cloud-optimized-geotiff-into-rioxarray",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#read-cloud-optimized-geotiff-into-rioxarray",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Read Cloud-Optimized GeoTIFF into rioxarray",
    "text": "Read Cloud-Optimized GeoTIFF into rioxarray\n\nda = rioxarray.open_rasterio(s3_url)\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 3660, x: 3660)>\n[13395600 values with dtype=int16]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.1e+06 4.1e+06 4.1e+06 ... 3.99e+06 3.99e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayband: 1y: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (4)band(band)int641array([1])x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.1e+06 4.1e+06 ... 3.99e+06array([4100025., 4099995., 4099965., ..., 3990315., 3990285., 3990255.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 11, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4100040.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nWhen GeoTIFFS/Cloud Optimized GeoTIFFS are read in, a band coordinate variable is automatically created (see the print out above). In this exercise we will not use that coordinate variable, so we will remove it using the squeeze() function to avoid confusion.\n\nda_red = da.squeeze('band', drop=True)\nda_red\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 3660, x: 3660)>\n[13395600 values with dtype=int16]\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.1e+06 4.1e+06 4.1e+06 ... 3.99e+06 3.99e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayy: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (3)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.1e+06 4.1e+06 ... 3.99e+06array([4100025., 4099995., 4099965., ..., 3990315., 3990285., 3990255.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 11, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4100040.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#plot-using-hvplot",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#plot-using-hvplot",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Plot using hvplot",
    "text": "Plot using hvplot\n\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nrio_env.__exit__()"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#part-3-working-with-zarr-formatted-data-using-nasas-harmony-cloud-transformation-service",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#part-3-working-with-zarr-formatted-data-using-nasas-harmony-cloud-transformation-service",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Part 3: Working with Zarr-formatted data using NASA’s Harmony cloud transformation service",
    "text": "Part 3: Working with Zarr-formatted data using NASA’s Harmony cloud transformation service\nWe have already explored direct access to the NASA EOSDIS archive in the cloud via the Amazon Simple Storage Service (S3). In addition to directly accessing the files archived and distributed by each of the NASA DAACs, many datasets also support services that allow us to customize the data via subsetting, reformatting, reprojection, and other transformations.\nThis example demonstrates “analysis in place” using customized ECCO Level 4 monthly sea surface height data, in this case reformatted to Zarr, from a new ecosystem of services operating within the NASA Earthdata Cloud: NASA Harmony:\n\nConsistent access patterns to EOSDIS holdings make cross-data center data access easier\nData reduction services allow us to request only the data we want, in the format and projection we want\nAnalysis Ready Data and cloud access will help reduce time-to-science\nCommunity Development helps reduce the barriers for re-use of code and sharing of domain knowledge"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#import-packages-1",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#import-packages-1",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Import packages",
    "text": "Import packages\n\nfrom harmony import BBox, Client, Collection, Request, LinkType\nfrom harmony.config import Environment\nfrom pprint import pprint\nimport datetime as dt\nimport s3fs\nfrom pqdm.threads import pqdm\nimport xarray as xr"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#using-harmony-py-to-customize-data",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#using-harmony-py-to-customize-data",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Using Harmony-Py to customize data",
    "text": "Using Harmony-Py to customize data\nHarmony-Py provides a pip installable Python alternative to directly using Harmony’s RESTful API to make it easier to request data and service options, especially when interacting within a Python Jupyter Notebook environment.\n\nCreate Harmony Client object\nFirst, we need to create a Harmony Client, which is what we will interact with to submit and inspect a data request to Harmony, as well as to retrieve results.\n\nharmony_client = Client()"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#create-harmony-request",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#create-harmony-request",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Create Harmony Request",
    "text": "Create Harmony Request\nSpecify a temporal range over 2015, and Zarr as an output format.\nZarr is an open source library for storing N-dimensional array data. It supports multidimensional arrays with attributes and dimensions similar to NetCDF4, and it can be read by XArray. Zarr is often used for data held in cloud object storage (like Amazon S3), because it is better optimized for these situations than NetCDF4.\n\nshort_name = 'ECCO_L4_SSH_05DEG_MONTHLY_V4R4'\n\nrequest = Request(\n    collection=Collection(id=short_name),\n    temporal={\n        'start': dt.datetime(2015, 1, 2),\n        'stop': dt.datetime(2015, 12, 31),\n    },\n    format='application/x-zarr'\n)\n\njob_id = harmony_client.submit(request)"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#check-request-status-and-view-output-urls",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#check-request-status-and-view-output-urls",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Check request status and view output URLs",
    "text": "Check request status and view output URLs\nHarmony data outputs can be accessed within the cloud using the s3 URLs and AWS credentials provided in the Harmony job response:\n\nharmony_client.wait_for_processing(job_id, show_progress=True)\n\nresults = harmony_client.result_urls(job_id, link_type=LinkType.s3)\ns3_urls = list(results)\ns3_urls\n\n [ Processing:  83% ] |##########################################         | [/]\n\n\n\nAWS credential retrieval\nUsing aws_credentials you can retrieve the credentials needed to access the Harmony s3 staging bucket and its contents.\n\ncreds = harmony_client.aws_credentials()"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#open-staged-files-with-s3fs-and-xarray",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#open-staged-files-with-s3fs-and-xarray",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Open staged files with s3fs and xarray",
    "text": "Open staged files with s3fs and xarray\nAccess AWS credentials for the Harmony bucket, and use the AWS s3fs package to create a file system that can then be read by xarray. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\ncreds = harmony_client.aws_credentials()\n\ns3_fs = s3fs.S3FileSystem(\n    key=creds['aws_access_key_id'],\n    secret=creds['aws_secret_access_key'],\n    token=creds['aws_session_token'],\n    client_kwargs={'region_name':'us-west-2'},\n)\n\nOpen the Zarr stores using the s3fs package, then load them all at once into a concatenated xarray dataset:\n\nstores = [s3fs.S3Map(root=url, s3=s3_fs, check=False) for url in s3_urls]\ndef open_zarr_xarray(store):\n    return xr.open_zarr(store=store, consolidated=True)\n\ndatasets = pqdm(stores, open_zarr_xarray, n_jobs=12)\n\nds = xr.concat(datasets, 'time', coords='minimal', )\nds = xr.decode_cf(ds, mask_and_scale=True, decode_coords=True)\nds\n\n\nssh_da = ds.SSH\n\nssh_da.to_masked_array(copy=False)\n\nssh_da"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#plot-the-sea-surface-height-time-series-using-hvplot",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#plot-the-sea-surface-height-time-series-using-hvplot",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Plot the Sea Surface Height time series using hvplot",
    "text": "Plot the Sea Surface Height time series using hvplot\nNow we can start looking at aggregations across the time dimension. In this case, plot the standard deviation of the temperature at each point to get a visual sense of how much temperatures fluctuate over the course of the month.\n\nssh_da = ds.SSH\n\nstdev_ssh = ssh_da.std('time')\nstdev_ssh.name = 'stdev of analysed_sst [Kelvin]'\nstdev_ssh.plot();\n\nssh_da.hvplot.image(x='longitude', y='latitude', cmap='Spectral_r', aspect='equal').opts(clim=(ssh_da.attrs['valid_min'],ssh_da.attrs['valid_max']))"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#further-resources",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html#further-resources",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Further Resources",
    "text": "Further Resources\n\nReference Hackathon/workshop tutorials that go into more detail!\nEarthdata Cloud Cookbook\nEarthdata Cloud Primer\n\nGetting started with Amazon Web Services outside of the Workshop to access and work with data with a cloud environment."
  },
  {
    "objectID": "how-tos/additional_resources/Multi-File_Direct_S3_Access_NetCDF_Example.html#summary",
    "href": "how-tos/additional_resources/Multi-File_Direct_S3_Access_NetCDF_Example.html#summary",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF datasets into an xarray dataset. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/additional_resources/Multi-File_Direct_S3_Access_NetCDF_Example.html#requirements",
    "href": "how-tos/additional_resources/Multi-File_Direct_S3_Access_NetCDF_Example.html#requirements",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/additional_resources/Multi-File_Direct_S3_Access_NetCDF_Example.html#learning-objectives",
    "href": "how-tos/additional_resources/Multi-File_Direct_S3_Access_NetCDF_Example.html#learning-objectives",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to define a dataset of interest and find netCDF files in S3 bucket\nhow to perform in-region direct access of ECCO_L4_SSH_05DEG_MONTHLY_V4R4 data in S3\nhow to plot the data"
  },
  {
    "objectID": "how-tos/additional_resources/Multi-File_Direct_S3_Access_NetCDF_Example.html#import-packages",
    "href": "how-tos/additional_resources/Multi-File_Direct_S3_Access_NetCDF_Example.html#import-packages",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Import Packages",
    "text": "Import Packages\n\nimport os\nimport requests\nimport s3fs\nimport xarray as xr\nimport hvplot.xarray"
  },
  {
    "objectID": "how-tos/additional_resources/Multi-File_Direct_S3_Access_NetCDF_Example.html#get-temporary-aws-credentials",
    "href": "how-tos/additional_resources/Multi-File_Direct_S3_Access_NetCDF_Example.html#get-temporary-aws-credentials",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Get Temporary AWS Credentials",
    "text": "Get Temporary AWS Credentials\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('podaac')\n#temp_creds_req"
  },
  {
    "objectID": "how-tos/additional_resources/Multi-File_Direct_S3_Access_NetCDF_Example.html#set-up-an-s3fs-session-for-direct-access",
    "href": "how-tos/additional_resources/Multi-File_Direct_S3_Access_NetCDF_Example.html#set-up-an-s3fs-session-for-direct-access",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Set up an s3fs session for Direct Access",
    "text": "Set up an s3fs session for Direct Access\ns3fs sessions are used for authenticated access to s3 bucket and allows for typical file-system style operations. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\nfs_s3 = s3fs.S3FileSystem(anon=False, \n                          key=temp_creds_req['accessKeyId'], \n                          secret=temp_creds_req['secretAccessKey'], \n                          token=temp_creds_req['sessionToken'],\n                          client_kwargs={'region_name':'us-west-2'})\n\nIn this example we’re interested in the ECCO data collection from NASA’s PO.DAAC in Earthdata Cloud. In this case it’s the following string that unique identifies the collection of monthly, 0.5-degree sea surface height data (ECCO_L4_SSH_05DEG_MONTHLY_V4R4).\n\nshort_name = 'ECCO_L4_SSH_05DEG_MONTHLY_V4R4'\n\n\nbucket = os.path.join('podaac-ops-cumulus-protected/', short_name, '*2015*.nc')\nbucket\n\nGet a list of netCDF files located at the S3 path corresponding to the ECCO V4r4 monthly sea surface height dataset on the 0.5-degree latitude/longitude grid, for year 2015.\n\nssh_files = fs_s3.glob(bucket)\nssh_files"
  },
  {
    "objectID": "how-tos/additional_resources/Multi-File_Direct_S3_Access_NetCDF_Example.html#direct-in-region-access",
    "href": "how-tos/additional_resources/Multi-File_Direct_S3_Access_NetCDF_Example.html#direct-in-region-access",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Direct In-region Access",
    "text": "Direct In-region Access\nOpen with the netCDF files using the s3fs package, then load them all at once into a concatenated xarray dataset.\n\nfileset = [fs_s3.open(file) for file in ssh_files]\n\nCreate an xarray dataset using the open_mfdataset() function to “read in” all of the netCDF4 files in one call.\n\nssh_ds = xr.open_mfdataset(fileset,\n                           combine='by_coords',\n                           mask_and_scale=True,\n                           decode_cf=True,\n                           chunks='auto')\nssh_ds\n\nGet the SSH variable as an xarray dataarray\n\nssh_da = ssh_ds.SSH\nssh_da\n\nPlot the SSH time series using hvplot\n\nssh_da.hvplot.image(y='latitude', x='longitude', cmap='Viridis',).opts(clim=(ssh_da.attrs['valid_min'][0],ssh_da.attrs['valid_max'][0]))"
  },
  {
    "objectID": "how-tos/additional_resources/Multi-File_Direct_S3_Access_NetCDF_Example.html#resources",
    "href": "how-tos/additional_resources/Multi-File_Direct_S3_Access_NetCDF_Example.html#resources",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Resources",
    "text": "Resources\nDirect access to ECCO data in S3 (from us-west-2)\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#import-packages",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#import-packages",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Import Packages",
    "text": "Import Packages\n\nimport xarray as xr\nimport dask\nimport hvplot.xarray"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-on-prem-opendap-hyrax-server---open",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-on-prem-opendap-hyrax-server---open",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Access On-prem OPeNDAP (Hyrax Server) - Open",
    "text": "Access On-prem OPeNDAP (Hyrax Server) - Open\n\nopd_sst_url = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/NCEI/AVHRR_OI/v2/1981/244/19810901120000-NCEI-L4_GHRSST-SSTblend-AVHRR_OI-GLOB-v02.0-fv02.0.nc'\n\n\nopd_sst_ds = xr.open_dataset(opd_sst_url)\nopd_sst_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:           (lat: 720, lon: 1440, time: 1, nv: 2)\nCoordinates:\n  * lat               (lat) float32 -89.88 -89.62 -89.38 ... 89.38 89.62 89.88\n  * lon               (lon) float32 -179.9 -179.6 -179.4 ... 179.4 179.6 179.9\n  * time              (time) datetime64[ns] 1981-09-01\nDimensions without coordinates: nv\nData variables:\n    lat_bnds          (lat, nv) float32 -90.0 -89.75 -89.75 ... 89.75 89.75 90.0\n    lon_bnds          (lon, nv) float32 -180.0 -179.8 -179.8 ... 179.8 180.0\n    time_bnds         (time, nv) datetime64[ns] 1981-09-01 1981-09-02\n    analysed_sst      (time, lat, lon) float32 ...\n    analysis_error    (time, lat, lon) float32 ...\n    mask              (time, lat, lon) float32 ...\n    sea_ice_fraction  (time, lat, lon) float32 ...\nAttributes: (12/48)\n    product_version:                 Version 2.0\n    spatial_resolution:              0.25 degree\n    Conventions:                     CF-1.6,ACDD-1.3\n    title:                           NCEI global 0.25 deg daily sea surface t...\n    references:                      Reynolds, et al.(2009) What is New in Ve...\n    institution:                     NCEI\n    ...                              ...\n    source:                          AVHRR_Pathfinder-NODC-L3C-v5.1,ICOADS_SH...\n    summary:                         NOAA's 1/4-degree Daily Optimum Interpol...\n    time_coverage_start:             19810901T000000Z\n    time_coverage_end:               19810902T000000Z\n    uuid:                            39832cc3-d409-438a-820e-2bb1b38ebca8\n    DODS_EXTRA.Unlimited_Dimension:  timexarray.DatasetDimensions:lat: 720lon: 1440time: 1nv: 2Coordinates: (3)lat(lat)float32-89.88 -89.62 ... 89.62 89.88long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northcomment :Uniform grid with centers from -89.875 to 89.875 by 0.25 degrees.bounds :lat_bndsvalid_max :90.0valid_min :-90.0array([-89.875, -89.625, -89.375, ...,  89.375,  89.625,  89.875],\n      dtype=float32)lon(lon)float32-179.9 -179.6 ... 179.6 179.9long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastcomment :Uniform grid with centers from -179.875 to 179.875 by 0.25 degrees.bounds :lon_bndsvalid_max :180.0valid_min :-180.0array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875],\n      dtype=float32)time(time)datetime64[ns]1981-09-01long_name :reference time of sst fieldstandard_name :timeaxis :Tbounds :time_bndscomment :Nominal time because observations are from different sources and are made at different times of the day.array(['1981-09-01T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (7)lat_bnds(lat, nv)float32...comment :This variable defines the latitude values at the north and south bounds of every 0.25-degree pixel.array([[-90.  , -89.75],\n       [-89.75, -89.5 ],\n       [-89.5 , -89.25],\n       ...,\n       [ 89.25,  89.5 ],\n       [ 89.5 ,  89.75],\n       [ 89.75,  90.  ]], dtype=float32)lon_bnds(lon, nv)float32...comment :This variable defines the longitude values at the west and east bounds of every 0.25-degree pixel.array([[-180.  , -179.75],\n       [-179.75, -179.5 ],\n       [-179.5 , -179.25],\n       ...,\n       [ 179.25,  179.5 ],\n       [ 179.5 ,  179.75],\n       [ 179.75,  180.  ]], dtype=float32)time_bnds(time, nv)datetime64[ns]...comment :This variable defines the start and end of the time span for the data.array([['1981-09-01T00:00:00.000000000', '1981-09-02T00:00:00.000000000']],\n      dtype='datetime64[ns]')analysed_sst(time, lat, lon)float32...long_name :analysed sea surface temperaturestandard_name :sea_surface_temperatureunits :kelvinvalid_min :-300valid_max :4500comment :Single-sensor Pathfinder 5.0/5.1 AVHRR SSTs used until 2005; two AVHRRs at a time are used 2007 onward. Sea ice and in-situ data used also are 'near real time' quality for recent period.  SST (bulk) is at ambiguous depth because multiple types of observations are used.source :AVHRR_Pathfinder-NODC-L3C-v5.1,ICOADS_SHIP-NCAR-IN_SITU-v2.4,ICOADS_BUOY-NCAR-IN_SITU-v2.4,GSFC_25KM-NSIDC-ICE[1036800 values with dtype=float32]analysis_error(time, lat, lon)float32...long_name :estimated error standard deviation of analysed_sstunits :kelvinvalid_min :0valid_max :32767comment :Sum of bias, sampling and random errors.[1036800 values with dtype=float32]mask(time, lat, lon)float32...long_name :sea/land field composite maskflag_meanings :water landcomment :Binary mask distinguishing water and land only.flag_masks :[1 2]source :RWReynolds_landmask_V1.0valid_max :2valid_min :1[1036800 values with dtype=float32]sea_ice_fraction(time, lat, lon)float32...long_name :sea ice area fractionvalid_min :0valid_max :100standard_name :sea_ice_area_fractionunits :1comment :7-day median filtered .  Switch from 25 km NASA team ice (http://nsidc.org/data/nsidc-0051.html)  to 50 km NCEP ice (http://polar.ncep.noaa.gov/seaice) after 2004 results in artificial increase in ice coverage.source :GSFC_25KM-NSIDC-ICE[1036800 values with dtype=float32]Attributes: (48)product_version :Version 2.0spatial_resolution :0.25 degreeConventions :CF-1.6,ACDD-1.3title :NCEI global 0.25 deg daily sea surface temperature analysis based mainly on Advanced Very High Resolution Radiometer, finalreferences :Reynolds, et al.(2009) What is New in Version 2. Available at http://www.ncdc.noaa.gov/sites/default/files/attachments/Reynolds2009_oisst_daily_v02r00_version2-features.pdf; Daily 1/4 Degree Optimum Interpolation Sea Surface Temperature (OISST)- Climate Algorithm Theoretical Theoretical Basis Document, NOAA Climate Data Record Program CDRP-ATBD-0303 Rev. 2 (2013). Available at http://www1.ncdc.noaa.gov/pub/data/sds/cdr/CDRs/Sea_Surface_Temperature_Optimum_Interpolation/AlgorithmDescription.pdf.institution :NCEInetcdf_version_id :4.3.2history :2015-11-02T19:52:40Z: Modified format and attributes with NCO to match the GDS 2.0 rev 5 specification.start_time :19810901T000000Zstop_time :19810902T000000Zwesternmost_longitude :-180.0easternmost_longitude :180.0southernmost_latitude :-90.0northernmost_latitude :90.0comment :The daily OISST version 2.0 data contained in this file are the same as those in the equivalent GDS 1.0 file.Metadata_Conventions :ACDD-1.3acknowledgment :This project was supported in part by a grant from the NOAA Climate Data Record (CDR) Program. Cite this dataset when used as a source. The recommended citation and DOI depends on the data center from which the files were acquired. For data accessed from NOAA in near real-time or from the GHRSST LTSRF, cite as: Richard W. Reynolds, Viva F. Banzon, and NOAA CDR Program (2008): NOAA Optimum Interpolation 1/4 Degree Daily Sea Surface Temperature (OISST) Analysis, Version 2. [indicate subset used]. NOAA National Centers for Environmental Information. http://doi.org/doi:10.7289/V5SQ8XB5 [access date]. For data accessed from the NASA PO.DAAC, cite as: Richard W. Reynolds, Viva F. Banzon, and NOAA CDR Program (2008): NOAA Optimum Interpolation 1/4 Degree Daily Sea Surface Temperature (OISST) Analysis, Version 2. [indicate subset used]. PO.DAAC, CA, USA. http://doi.org/10.5067/GHAAO-4BC01 [access date].cdm_data_type :Gridcreator_name :Viva Banzoncreator_email :viva.banzon@noaa.govcreator_url :http://www.ncdc.noaa.govdate_created :20091203T000000Zfile_quality_level :3gds_version_id :2.0r5geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastid :NCEI-L4LRblend-GLOB-AVHRR_OIkeywords :Oceans>Ocean Temperature>Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywords, Version 8.1license :No constraints on data access or use.metadata_link :http://doi.org/10.7289/V5SQ8XB5naming_authority :org.ghrsstplatform :NOAA-7processing_level :L4project :Group for High Resolution Sea Surface Temperaturepublisher_email :oisst_contacts@noaa.govpublisher_name :OISST Operations Teampublisher_url :http://www.ncdc.noaa.gov/sstsensor :AVHRR_GACstandard_name_vocabulary :CF Standard Name Table v29source :AVHRR_Pathfinder-NODC-L3C-v5.1,ICOADS_SHIP-NCAR-IN_SITU-v2.4,ICOADS_BUOY-NCAR-IN_SITU-v2.4,GSFC_25KM-NSIDC-ICEsummary :NOAA's 1/4-degree Daily Optimum Interpolation Sea Surface Temperature (OISST) (sometimes referred to as Reynold's SST, which however also refers to earlier products at different resolution), currently available as version 2,  is created by interpolating and extrapolating SST observations from different sources, resulting in a smoothed complete field. The sources of data are satellite (AVHRR) and in situ platforms (i.e., ships and buoys), and the specific datasets employed may change over. At the marginal ice zone, sea ice concentrations are used to generate proxy SSTs.  A preliminary version of this file is produced in near-real time (1-day latency), and then replaced with a final version after 2 weeks. Note that this is the AVHRR-ONLY DOISST, available from Oct 1981, but there is a companion DOISST product that includes microwave satellite data, available from June 2002.time_coverage_start :19810901T000000Ztime_coverage_end :19810902T000000Zuuid :39832cc3-d409-438a-820e-2bb1b38ebca8DODS_EXTRA.Unlimited_Dimension :time\n\n\n\nopd_sst_ds.analysed_sst.isel(time=0).hvplot.image(cmap='Inferno')\n\nUnable to display output for mime type(s):"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-on-prem-opendap-hyrax-server---authentication",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-on-prem-opendap-hyrax-server---authentication",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Access On-prem OPeNDAP (Hyrax Server) - Authentication",
    "text": "Access On-prem OPeNDAP (Hyrax Server) - Authentication\n\nimport opendap_auth\n\n\nopendap_auth.create_dodsrc()\n\n'.dodsrc file created: /home/jovyan/.dodsrc'\n\n\nIntegrated Multi-satellitE Retrievals for GPM (IMERG) Level 3 IMERG Final Daily 10 x 10 km (GPM_3IMERGDF)\n\nopd_prec_url = 'https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGDF.06/2021/07/3B-DAY.MS.MRG.3IMERG.20210704-S000000-E235959.V06.nc4' \n\n\nopd_prec_ds = xr.open_dataset(opd_prec_url)\nopd_prec_ds\n\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\n\n\nKeyboardInterrupt: \n\n\n\nopd_prec_ds.precipitationCal.isel(time=0).hvplot.image(cmap='rainbow')"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-earthdata-cloud-opendap-hyrax-server---authentication",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html#access-earthdata-cloud-opendap-hyrax-server---authentication",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Access Earthdata cloud OPeNDAP (Hyrax Server) - Authentication",
    "text": "Access Earthdata cloud OPeNDAP (Hyrax Server) - Authentication\n\nedc_odp_ssh_url = 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Sea%20Surface%20Height%20-%20Daily%20Mean%200.5%20Degree%20(Version%204%20Release%204)/granules/SEA_SURFACE_HEIGHT_day_mean_1992-01-01_ECCO_V4r4_latlon_0p50deg.dap.nc'\n\n\nedc_odp_ssh_ds = xr.open_dataset(edc_odp_ssh_url)\nedc_odp_ssh_ds\n\n\nurl = 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/GHRSST%20Level%204%20MUR%20Global%20Foundation%20Sea%20Surface%20Temperature%20Analysis%20(v4.1)/granules/20190201090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.dap.nc4'\n\n\nxr.open_dataset(url)\n\n\nurl = 'https://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_001_20210713T162644_20210713T182234_F02.nc4'\n\n\nxr.open_dataset(url)\n\n\nurl = 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Sea%20Surface%20Height%20-%20Daily%20Mean%200.5%20Degree%20(Version%204%20Release%204)/granules/SEA_SURFACE_HEIGHT_day_mean_1992-01-01_ECCO_V4r4_latlon_0p50deg.dap.nc4'\n\n\nxr.open_dataset(url)"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#summary",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#summary",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will access data for the Harmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002) data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band.\nWe will access a single COG file, L30 red band (0.64 – 0.67 μm), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#requirements",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#requirements",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#learning-objectives",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#learning-objectives",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of HLS Cloud Optimized geoTIFF (COG) files in S3\nhow to plot the data\n\n\n\nImport Packages\n\nimport os\nimport requests \nimport boto3\nfrom osgeo import gdal\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#get-temporary-aws-credentials",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#get-temporary-aws-credentials",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Get Temporary AWS Credentials",
    "text": "Get Temporary AWS Credentials\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('lpdaac')\n#temp_creds_req"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#workspace-environment-setup",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#workspace-environment-setup",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Workspace Environment Setup",
    "text": "Workspace Environment Setup\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nGDAL environment variables must be configured to access COGs in Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\nIn this example we’re interested in the HLS L30 data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\ns3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#direct-in-region-access",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#direct-in-region-access",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Direct In-region Access",
    "text": "Direct In-region Access\nRead in the HLS s3 URL for the L30 red band (0.64 – 0.67 μm) into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(s3_url)\nda\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_red = da.squeeze('band', drop=True)\nda_red\n\nPlot the dataarray, representing the L30 red band, using hvplot.\n\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n\nExit the context manager.\n\nrio_env.__exit__()"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#resources",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html#resources",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Resources",
    "text": "Resources\nDirect S3 Data Access with rioxarray\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nGetting Started with Cloud-Native Harmonized Landsat Sentinel-2 (HLS) Data in R"
  },
  {
    "objectID": "how-tos/additional_resources/Multi-File_Direct_S3_Access_COG_Example.html",
    "href": "how-tos/additional_resources/Multi-File_Direct_S3_Access_COG_Example.html",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "from pystac_client import Client\nimport stackstac\n\n\nSTAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n\n\ncatalog = Client.open(f\"{STAC_URL}/LPCLOUD\")\n\n\nsearch = catalog.search(\n    collections = ['HLSL30.v2.0', 'HLSS30.v2.0'],\n    intersects = {'type': 'Polygon',\n                  'coordinates': [[[-101.67271614074707, 41.04754380304359],\n                                   [-101.65344715118408, 41.04754380304359],\n                                   [-101.65344715118408, 41.06213891056728],\n                                   [-101.67271614074707, 41.06213891056728],\n                                   [-101.67271614074707, 41.04754380304359]]]},\n    datetime = '2021-05/2021-08'\n)               \n\n\nsearch.matched()\n\n\nic = search.get_all_items()\n\n\nil = list(search.get_items())\n\n\ntic = [x for x in ic if 'T13TGF' in x.id]\n\n\nimport pystac\n\n\nitem_collection = pystac.ItemCollection(items=tic)\n\n\nitem_collection\n\n\nil\n\n\ndata = stackstac.stack(item_collection, assets=['B04', 'B02'], epsg=32613, resolution=30)\n\n\ndata.sel(band='B04').isel(time=[0])\n\n\nimport stackstac\nimport pystac_client\n\nURL = \"https://earth-search.aws.element84.com/v0\"\ncatalog = pystac_client.Client.open(URL)\n\n\ncatalog\n\n\nstac_items = catalog.search(\n    intersects=dict(type=\"Point\", coordinates=[-105.78, 35.79]),\n    collections=[\"sentinel-s2-l2a-cogs\"],\n    datetime=\"2020-04-01/2020-05-01\"\n).get_all_items()\n\n\nstac_items\n\n\nstack = stackstac.stack(stac_items)\n\n\nstack"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#summary",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#summary",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will access data for the ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each variable.\nWe will access a single COG file, Land Surface Temperature (LST), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#requirements",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#requirements",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. ECOSTRESS Early Adopter¶\nECOSTRESS build 7 is only open to individuals identified as early adopters. As such ECOSTRESS discovery and access is managed by an access control list. If you are not on the access control list, you will not be able to complete the exercise as written below.\n\n\n3. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n4. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#learning-objectives",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#learning-objectives",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of HLS Cloud Optimized geoTIFF (COG) files in S3\nhow to plot the data\n\n\n\nImport Packages\n\nimport os\nimport requests \nimport boto3\nfrom osgeo import gdal\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray\nimport geopandas\nfrom shapely.geometry import Polygon\nfrom shapely.ops import transform\nimport pyproj\nfrom pyproj import Proj\nimport hvplot.xarray\nimport holoviews as hv\nimport geoviews as gv\ngv.extension('bokeh', 'matplotlib')"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#get-temporary-aws-credentials",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#get-temporary-aws-credentials",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Get Temporary AWS Credentials",
    "text": "Get Temporary AWS Credentials\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('lpdaac')\n#temp_creds_req"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#workspace-environment-setup",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#workspace-environment-setup",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Workspace Environment Setup",
    "text": "Workspace Environment Setup\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nGDAL environment variables must be configured to access COGs in Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\n<rasterio.env.Env at 0x7fd309cf8d60>\n\n\nIn this example we’re interested in the ECOSTRESS Tiled Land Surface Temperature and Emissivity data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the S3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\n#s3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020272T183449.v2.0/HLS.L30.T10SGD.2020272T183449.v2.0.B04.tif'\ns3_url = 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_21241_015_10SGD_20220405T034009_0700_01/ECOv002_L2T_LSTE_21241_015_10SGD_20220405T034009_0700_01_LST.tif'"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#direct-in-region-access",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#direct-in-region-access",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Direct In-region Access",
    "text": "Direct In-region Access\nRead in the ECOSTRESS Tiles LST S3 URL into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(s3_url, chunks='auto')\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 1568, x: 1568)>\ndask.array<open_rasterio-d1d4613f005e6ea0be9b5c4c040f9aaf<this-array>, shape=(1, 1568, 1568), dtype=float32, chunksize=(1, 1568, 1568), chunktype=numpy.ndarray>\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 7e+05 7.001e+05 7.001e+05 ... 8.096e+05 8.097e+05\n  * y            (y) float64 3.9e+06 3.9e+06 3.9e+06 ... 3.79e+06 3.79e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    nan\n    scale_factor:  1.0\n    add_offset:    0.0xarray.DataArrayband: 1y: 1568x: 1568dask.array<chunksize=(1, 1568, 1568), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         9.38 MiB \n                         9.38 MiB \n                    \n                    \n                    \n                         Shape \n                         (1, 1568, 1568) \n                         (1, 1568, 1568) \n                    \n                    \n                         Count \n                         2 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  1568\n  1568\n  1\n\n        \n    \nCoordinates: (4)band(band)int641array([1])x(x)float647e+05 7.001e+05 ... 8.097e+05array([699995., 700065., 700135., ..., 809545., 809615., 809685.])y(y)float643.9e+06 3.9e+06 ... 3.79e+06array([3899965., 3899895., 3899825., ..., 3790415., 3790345., 3790275.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 10N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32610\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 10Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-123.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 10N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32610\"]]GeoTransform :699960.0 70.0 0.0 3900000.0 0.0 -70.0array(0)Attributes: (3)_FillValue :nanscale_factor :1.0add_offset :0.0\n\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_lst = da.squeeze('band', drop=True)\nda_lst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 1568, x: 1568)>\ndask.array<getitem, shape=(1568, 1568), dtype=float32, chunksize=(1568, 1568), chunktype=numpy.ndarray>\nCoordinates:\n  * x            (x) float64 7e+05 7.001e+05 7.001e+05 ... 8.096e+05 8.097e+05\n  * y            (y) float64 3.9e+06 3.9e+06 3.9e+06 ... 3.79e+06 3.79e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    nan\n    scale_factor:  1.0\n    add_offset:    0.0xarray.DataArrayy: 1568x: 1568dask.array<chunksize=(1568, 1568), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         9.38 MiB \n                         9.38 MiB \n                    \n                    \n                    \n                         Shape \n                         (1568, 1568) \n                         (1568, 1568) \n                    \n                    \n                         Count \n                         3 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  1568\n  1568\n\n        \n    \nCoordinates: (3)x(x)float647e+05 7.001e+05 ... 8.097e+05array([699995., 700065., 700135., ..., 809545., 809615., 809685.])y(y)float643.9e+06 3.9e+06 ... 3.79e+06array([3899965., 3899895., 3899825., ..., 3790415., 3790345., 3790275.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 10N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32610\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 10Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-123.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 10N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32610\"]]GeoTransform :699960.0 70.0 0.0 3900000.0 0.0 -70.0array(0)Attributes: (3)_FillValue :nanscale_factor :1.0add_offset :0.0\n\n\nPlot the dataarray, representing the L30 red band, using hvplot.\n\nda_lst.hvplot.image(x = 'x', y = 'y', crs = 'EPSG:32610', cmap='jet', rasterize=False, width=800, height=600, tiles='EsriImagery', colorbar=True)\n\nUnable to display output for mime type(s):"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#define-the-region-of-interest-for-clipping",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#define-the-region-of-interest-for-clipping",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Define the Region of Interest for Clipping",
    "text": "Define the Region of Interest for Clipping\nWe’ll read in our GeoJSON file of our points of interest and create bounding box that contains a points coordinates\n\npoints = field = geopandas.read_file('../../data/TNC_fall_2020.geojson')\n\nExtract the min/max values for the y and x axis\n\nminx, miny, maxx, maxy = points.geometry.total_bounds\nminx, miny, maxx, maxy\n\n(-120.45264628215773,\n 34.51050622261265,\n -120.40432447545712,\n 34.532398755692206)\n\n\nOrder the coordinates for the bounding box counterclockwise\n\ncoords = [\n    (minx, miny),\n    (maxx, miny),\n    (maxx, maxy),\n    (minx, maxy)\n]\n\nCreate a shapely polygon\n\nfeature_shape = Polygon(coords)\nfeature_shape\n\n\n\n\n\nbase = gv.tile_sources.EsriImagery.opts(width=650, height=500)\nfarmField = gv.Polygons(feature_shape).opts(line_color='yellow', line_width=10, color=None)\nbase * farmField\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nLet’s take a look at the bounding coordinate values.\nNote, the values above are in decimal degrees and represent the longitude and latitude for the lower left corner and upper right corner respectively.\n\nfeature_shape.bounds\n\n(-120.45264628215773,\n 34.51050622261265,\n -120.40432447545712,\n 34.532398755692206)\n\n\nGet the projection information from the HLS file\n\nsrc_proj = da_lst.rio.crs\nsrc_proj\n\nCRS.from_epsg(32610)\n\n\nTransform coordinates from lat lon (units = dd) to UTM (units = m)\n\ngeo_CRS = Proj('+proj=longlat +datum=WGS84 +no_defs', preserve_units=True)   # Source coordinate system of the ROI\n\n\nproject = pyproj.Transformer.from_proj(geo_CRS, src_proj)                    # Set up the transformation\n\n\nfsUTM = transform(project.transform, feature_shape)\nfsUTM.bounds\n\n(733792.5759279637, 3821708.6775865103, 738290.9586659562, 3824250.087197832)\n\n\nThe coordinates for our feature have now been converted to source raster projection. Note the difference in the values between feature_shape.bounds (in geographic) and fsUTM.bounds (in UTM projection).\nNow we can clip our ECOSTRESS LST file to our region of insterest!"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#access-and-clip-the-ecostress-lst-cog",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#access-and-clip-the-ecostress-lst-cog",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Access and clip the ECOSTRESS LST COG",
    "text": "Access and clip the ECOSTRESS LST COG\nWe can now use our transformed ROI bounding box to clip the HLS S3 object we accessed before. We’ll use the rio.clip\n\nda_lst_clip = rioxarray.open_rasterio(s3_url, chunks='auto').squeeze('band', drop=True).rio.clip([fsUTM])\n\n\nda_lst_clip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 36, x: 65)>\ndask.array<copy, shape=(36, 65), dtype=float32, chunksize=(36, 65), chunktype=numpy.ndarray>\nCoordinates:\n  * y            (y) float64 3.824e+06 3.824e+06 ... 3.822e+06 3.822e+06\n  * x            (x) float64 7.338e+05 7.339e+05 ... 7.382e+05 7.383e+05\n    spatial_ref  int64 0\nAttributes:\n    scale_factor:  1.0\n    add_offset:    0.0\n    _FillValue:    nanxarray.DataArrayy: 36x: 65dask.array<chunksize=(36, 65), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         9.14 kiB \n                         9.14 kiB \n                    \n                    \n                    \n                         Shape \n                         (36, 65) \n                         (36, 65) \n                    \n                    \n                         Count \n                         7 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  65\n  36\n\n        \n    \nCoordinates: (3)y(y)float643.824e+06 3.824e+06 ... 3.822e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([3824225., 3824155., 3824085., 3824015., 3823945., 3823875., 3823805.,\n       3823735., 3823665., 3823595., 3823525., 3823455., 3823385., 3823315.,\n       3823245., 3823175., 3823105., 3823035., 3822965., 3822895., 3822825.,\n       3822755., 3822685., 3822615., 3822545., 3822475., 3822405., 3822335.,\n       3822265., 3822195., 3822125., 3822055., 3821985., 3821915., 3821845.,\n       3821775.])x(x)float647.338e+05 7.339e+05 ... 7.383e+05axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([733805., 733875., 733945., 734015., 734085., 734155., 734225., 734295.,\n       734365., 734435., 734505., 734575., 734645., 734715., 734785., 734855.,\n       734925., 734995., 735065., 735135., 735205., 735275., 735345., 735415.,\n       735485., 735555., 735625., 735695., 735765., 735835., 735905., 735975.,\n       736045., 736115., 736185., 736255., 736325., 736395., 736465., 736535.,\n       736605., 736675., 736745., 736815., 736885., 736955., 737025., 737095.,\n       737165., 737235., 737305., 737375., 737445., 737515., 737585., 737655.,\n       737725., 737795., 737865., 737935., 738005., 738075., 738145., 738215.,\n       738285.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 10N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32610\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 10Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-123.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 10N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32610\"]]GeoTransform :733770.0 70.0 0.0 3824260.0 0.0 -70.0array(0)Attributes: (3)scale_factor :1.0add_offset :0.0_FillValue :nan\n\n\n\nda_lst_clip.hvplot.image(x = 'x', y = 'y', crs = 'EPSG:32610', cmap='jet', rasterize=False, width=800, height=600, tiles='EsriImagery', colorbar=True)\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nExit the context manager.\n\nrio_env.__exit__()"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#resources",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#resources",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Resources",
    "text": "Resources\nDirect S3 Data Access with rioxarray\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nGetting Started with Cloud-Native Harmonized Landsat Sentinel-2 (HLS) Data in R"
  },
  {
    "objectID": "prerequisites/index.html#prerequisites",
    "href": "prerequisites/index.html#prerequisites",
    "title": "Prerequisites",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow along hands-on during the Workshop, please do the following (20 minutes). All software or accounts are free.\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com. Follow optional advice on choosing your username\nYour GitHub username is used to enable you access to a cloud environment during the workshop. Please add your GitHub username to the following form: GitHub Username Google Form\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to download or access cloud data during the workshop and beyond.\n\nLaptop or tablet\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2.\n\nSlack\n\nWhile we will be taking questions in person, we will also be communicating and recording questions via Slack. To take part in the conversations please ask Christine Lee or Gregory Halverson to add you to the ECOSTRESS Slack workspace."
  },
  {
    "objectID": "further-resources.html#a-growing-list-of-resources",
    "href": "further-resources.html#a-growing-list-of-resources",
    "title": "Additional resources",
    "section": "A growing list of resources!",
    "text": "A growing list of resources!\n\nOne stop for PO.DAAC Cloud Information: Cloud Data page with About, Cloud Datasets, Access Data, FAQs, Resources, and Migration information\nAsk questions or find resources: PO.DAAC in the CLOUD Forum\nCloud user migration overview, guidance, and resources: PO.DAAC Webinar\nSearch and get access links: Earthdata Search Client and guide\nSearch and get access links: PO.DAAC Cloud Earthdata Search Portal\nBrowse cloud data in web-based browser: CMR Virtual Browse and guiding video\nScripted data search end-point: Earthdata Common Metadata Repository (CMR) API\nEnable data download or access: Obtain Earthdata Login Account\nDownload data regularly: PO.DAAC Data Subscriber Access video and PO.DAAC Data Subscriber instructions\nBulk Download guide\nOPeNDAP in the cloud\nPO.DAAC scripts and notebooks: PO.DAAC Github\nHow to get started in the AWS cloud: Earthdata Cloud Primer documents\n2021 NASA Cloud Hackathon - November 2021, co-hosted by PODAAC, NSIDC DAAC, and LPDAAC. Additional support is provided by ASDC, GESDISC, IMPACT, and Openscapes.\nNASA Earthdata: How to Cloud\nSetting up Jupyter Notebooks in a user EC2 instance in AWS - helpful blog post for setting up jupyter notebooks in an EC2 instance in AWS. (Builds on the Cloud Primer tutorials, which are missing that next step)"
  },
  {
    "objectID": "further-resources.html#additional-tutorials",
    "href": "further-resources.html#additional-tutorials",
    "title": "Additional resources",
    "section": "Additional tutorials",
    "text": "Additional tutorials\n\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links\nDirect access to ECCO data in S3 (from us-west-2) - Direct S3 access example with netCDF data\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nCalculate black-sky, white-sky, and actual albedo (MCD43A) from MCD43A1 BRDF Parameters using R\nXarray Zonal Statistics"
  },
  {
    "objectID": "tutorials/Comparing_TNC_Tree_Data_to_ECOSTRESS.html",
    "href": "tutorials/Comparing_TNC_Tree_Data_to_ECOSTRESS.html",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "import xarray as xr\nimport rioxarray\nimport rasterio as rio\nimport os\nimport holoviews as hv\nimport hvplot.xarray\nimport hvplot.pandas\nimport geopandas as gpd\nimport shapely\nfrom shapely.geometry import box\nfrom shapely.geometry import Point\nfrom pyproj import Transformer\nfrom matplotlib.colors import ListedColormap\nimport rasterstats\nimport numpy as np\nimport seaborn as sns\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom os.path import basename\nimport xarray\nimport pandas as pd\nfrom scipy.stats import zscore\nfrom matplotlib.colors import LinearSegmentedColormap\nimport matplotlib.pyplot as plt\n\n\n\n\n\n\n\n\n\n\nModuleNotFoundError: No module named 'rasterstats'\n\n\n\n!mamba install rasterstats -q -y\n\n\nimport rasterstats\n\n\nFIG_WIDTH_PX = 1080\nFIG_HEIGHT_PX = 720\nFIG_WIDTH_IN = 16\nFIG_HEIGHT_IN = 9\n\n\nET_filename = '/home/jovyan/shared/2022-ecostress-workshop/ECOv002_L3T_ET_PT-JPL_12653_004_10SGD_20200928T224329_0700_01/ECOv002_L3T_ET_PT-JPL_12653_004_10SGD_20200928T224329_0700_01_ETdaily.tif'\nST_filename = '/home/jovyan/shared/2022-ecostress-workshop/ECOv002_L2T_LSTE_12653_004_10SGD_20200928T224329_0700_01/ECOv002_L2T_LSTE_12653_004_10SGD_20200928T224329_0700_01_LST.tif'\nSM_filename = '/home/jovyan/shared/2022-ecostress-workshop/ECOv002_L3T_SM_PT-JPL_12653_004_10SGD_20200928T224329_0700_01/ECOv002_L3T_SM_PT-JPL_12653_004_10SGD_20200928T224329_0700_01_SM.tif'\nNDVI_filename = '/home/jovyan/shared/2022-ecostress-workshop/ECOv002_L2T_STARS_12653_004_10SGD_20200928_0700_01/ECOv002_L2T_STARS_12653_004_10SGD_20200928_0700_01_NDVI.tif'\n\nLet’s use rioxarray to open the surface temperature layer from the L2T_LSTE product on the 10SGD tile covering the Dangermond Preserve and take a first look at this image using hvplot.\n\nET_image = rioxarray.open_rasterio(ET_filename, chunks='auto').squeeze('band', drop=True)\nET_image\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 1568, x: 1568)>\ndask.array<getitem, shape=(1568, 1568), dtype=float32, chunksize=(1568, 1568), chunktype=numpy.ndarray>\nCoordinates:\n  * x            (x) float64 7e+05 7.001e+05 7.001e+05 ... 8.096e+05 8.097e+05\n  * y            (y) float64 3.9e+06 3.9e+06 3.9e+06 ... 3.79e+06 3.79e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    nan\n    scale_factor:  1.0\n    add_offset:    0.0xarray.DataArrayy: 1568x: 1568dask.array<chunksize=(1568, 1568), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         9.38 MiB \n                         9.38 MiB \n                    \n                    \n                    \n                         Shape \n                         (1568, 1568) \n                         (1568, 1568) \n                    \n                    \n                         Count \n                         3 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  1568\n  1568\n\n        \n    \nCoordinates: (3)x(x)float647e+05 7.001e+05 ... 8.097e+05array([699995., 700065., 700135., ..., 809545., 809615., 809685.])y(y)float643.9e+06 3.9e+06 ... 3.79e+06array([3899965., 3899895., 3899825., ..., 3790415., 3790345., 3790275.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 10N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32610\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 10Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-123.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 10N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32610\"]]GeoTransform :699960.0 70.0 0.0 3900000.0 0.0 -70.0array(0)Attributes: (3)_FillValue :nanscale_factor :1.0add_offset :0.0\n\n\nThe ECOSTRESS Collection 2 tiled products are gridded in local UTM, following Sentinel convention, and this gridding is sampled at 70 m.\n\ncrs = ET_image.rio.crs\nprint(f\"CRS: {crs}\")\ncell_width, cell_height = ET_image.rio.resolution()\nprint(f\"resolution: {cell_width} m\")\n\nCRS: EPSG:32610\nresolution: 70.0 m\n\n\n\ncentroid_UTM = Point(np.nanmean(ET_image.x), np.nanmean(ET_image.y))\ncentroid_latlon = shapely.ops.transform(Transformer.from_crs(crs, \"EPSG:4326\", always_xy=True).transform, centroid_UTM)\ncentroid_latlon.wkt\n\n'POINT (-120.2172393005597 34.71640207843576)'\n\n\nThe date for this ECOSTRESS scene is September 28th, 2020. This overpass was around 2:45 in the afternoon.\n\ndt = datetime.strptime(basename(ET_filename).split(\"_\")[-4], \"%Y%m%dT%H%M%S\")\ndt_solar = dt + timedelta(hours=(np.radians(centroid_latlon.x) / np.pi * 12))\nprint(f\"date/time UTC: {dt:%Y-%m-%d %H:%M:%S}\")\nprint(f\"date/time solar: {dt_solar:%Y-%m-%d %H:%M:%S}\")\n\ndate/time UTC: 2020-09-28 22:43:29\ndate/time solar: 2020-09-28 14:42:36\n\n\nTo plot this image on top of a basemap, we’ll reproject it on the fly to match the basemap.\n\nET_CMAP = [\n    \"#f6e8c3\",\n    \"#d8b365\",\n    \"#99974a\",\n    \"#53792d\",\n    \"#6bdfd2\",\n    \"#1839c5\"\n]\n\n\nET_map = ET_image.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=ET_CMAP, \n    tiles=\"ESRI\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(ET_image.quantile(0.02), ET_image.quantile(0.98)),\n    title=basename(ET_filename)\n)\n\nET_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nST_image = rioxarray.open_rasterio(ST_filename).squeeze(\"band\", drop=True)\n\n\nST_CMAP = \"jet\"\n\nattrs = ST_image.attrs\nST_image -= 273.15\nST_image.attrs = attrs\n\n\nST_map = ST_image.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=ST_CMAP, \n    tiles=\"ESRI\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(ST_image.quantile(0.02), ST_image.quantile(0.98)),\n    title=basename(ST_filename)\n)\n\nST_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nSM_image = rioxarray.open_rasterio(SM_filename).squeeze(\"band\", drop=True)\n\n\nSM_CMAP = [\n    \"#f6e8c3\",\n    \"#d8b365\",\n    \"#99894a\",\n    \"#2d6779\",\n    \"#6bdfd2\",\n    \"#1839c5\"\n]\n\n\nSM_map = SM_image.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=SM_CMAP, \n    tiles=\"ESRI\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(SM_image.quantile(0.02), SM_image.quantile(0.98)),\n    title=basename(SM_filename)\n)\n\nSM_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nNDVI_image = rioxarray.open_rasterio(NDVI_filename).squeeze(\"band\", drop=True)\n\n\nNDVI_CMAP = [\n    \"#0000ff\",\n    \"#000000\",\n    \"#745d1a\",\n    \"#e1dea2\",\n    \"#45ff01\",\n    \"#325e32\"\n]\n\n\nNDVI_map = NDVI_image.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=NDVI_CMAP, \n    tiles=\"ESRI\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(NDVI_image.quantile(0.02), NDVI_image.quantile(0.98)),\n    title=basename(NDVI_filename)\n)\n\nNDVI_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nLet’s open our ground observations provided by the Nature Conservancy. TNC recorded observations of tree health for each tree and the location of the tree in latitude and longitude. These are qualitative categories of good, poor, and dead.\n\nTNC_fall_2020 = gpd.read_file(\"../data/TNC_fall_2020.geojson\")\nprint(TNC_fall_2020.crs)\nTNC_fall_2020.total_bounds\n\nepsg:4326\n\n\narray([-120.45264628,   34.51050622, -120.40432448,   34.53239876])\n\n\nTo compare our in situ point data with the projected raster data, we need to project these coordinates into the local UTM projection of the raster image.\n\nTNC_fall_2020 = TNC_fall_2020.to_crs(ET_image.rio.crs)\nprint(TNC_fall_2020.crs)\n\nPROJCS[\"WGS 84 / UTM zone 10N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32610\"]]\n\n\n\nTNC_fall_2020.head()\n\n\n\n\n  \n    \n      \n      planting_date\n      health\n      geometry\n    \n  \n  \n    \n      0\n      2019-05-10\n      good\n      POINT Z (737195.535 3823512.913 191.300)\n    \n    \n      1\n      2019-05-10\n      good\n      POINT Z (737183.595 3823525.373 194.800)\n    \n    \n      2\n      2019-05-10\n      poor\n      POINT Z (737184.237 3823525.408 194.200)\n    \n    \n      3\n      2019-05-10\n      good\n      POINT Z (737182.283 3823532.980 192.800)\n    \n    \n      4\n      2019-05-10\n      good\n      POINT Z (737174.601 3823536.391 188.900)\n    \n  \n\n\n\n\n\nTNC_fall_2020.total_bounds\n\narray([ 733844.5431781 , 3821733.03818613,  738246.43045996,\n       3824220.8625196 ])\n\n\nLet’s map out the in situ data. We’ll again reproject these points on the fly to match the basemap. Let’s assign colors to these categories as well, which we’ll use throughout the notebook.\n\n# is there a way to get the point colors in a legend?\ntree_palette = {\n    \"dead\": \"black\",\n    \"poor\": \"red\",\n    \"other\": \"white\",\n    \"good\": \"green\"\n}\n\nTNC_fall_2020.to_crs(\"EPSG:3857\").hvplot.points(\n    color=TNC_fall_2020[\"health\"].apply(lambda health: tree_palette[health]), \n    tiles=\"ESRI\", \n    size=1.5, \n    width=FIG_WIDTH_PX, \n    height=FIG_HEIGHT_PX,\n    title=\"The Nature Conservancy Fall 2020 Tree Survey\"\n)\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nIn this projected space, let’s get the bounds of our study area in meters from the convex hull of our observation locations with a 100 meter buffer.\n\nxmin, ymin, xmax, ymax = TNC_fall_2020.unary_union.convex_hull.buffer(100).bounds\nxmin, ymin, xmax, ymax\n\n(733744.5626735839, 3821633.1073998627, 738346.4301366343, 3824320.7421952724)\n\n\n\nTNC_fall_2020.buffer(100).total_bounds ### Similar to above...\n\narray([ 733744.5431781 , 3821633.03818613,  738346.43045996,\n       3824320.8625196 ])\n\n\nLet’s look at the tree health points overlayed on top of ECOSTRESS evapotranspiration.\n\nET_subset = ET_image.rio.clip([box(xmin, ymin, xmax, ymax)])\n\n\nraster_map = ET_subset.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=ET_CMAP, \n    tiles=\"OSM\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(ET_subset.quantile(0.02), ET_subset.quantile(0.98))\n)\n\npoint_map = TNC_fall_2020.to_crs(\"EPSG:3857\").hvplot.points(\n    color=TNC_fall_2020[\"health\"].apply(lambda health: tree_palette[health]), \n    size=1.5, \n    alpha=0.7,\n    width=FIG_WIDTH_PX, \n    height=FIG_HEIGHT_PX\n)\n\nraster_map * point_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nNow let’s look at the tree health points on top of ECOSTRESS surface temperature.\n\nST_subset = ST_image.rio.clip([box(xmin, ymin, xmax, ymax)])\n\n\nraster_map = ST_subset.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=ST_CMAP, \n    tiles=\"OSM\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(ST_subset.quantile(0.02), ST_subset.quantile(0.98)),\n    title=\"ECOSTRESS Surface Temperature and in situ Tree Health\"\n)\n\npoint_map = TNC_fall_2020.to_crs(\"EPSG:3857\").hvplot.points(\n    color=TNC_fall_2020[\"health\"].apply(lambda health: tree_palette[health]), \n    size=1.5, \n    alpha=0.7,\n    width=FIG_WIDTH_PX, \n    height=FIG_HEIGHT_PX\n)\n\nraster_map * point_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nAnd let’s look at tree health on top of soil moisture.\n\nSM_subset = SM_image.rio.clip([box(xmin, ymin, xmax, ymax)])\n\n\nraster_map = SM_subset.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=SM_CMAP, \n    tiles=\"OSM\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(SM_subset.quantile(0.02), SM_subset.quantile(0.98)),\n    title=\"ECOSTRESS Soil Moisture and in situ Tree Health\"\n)\n\npoint_map = TNC_fall_2020.to_crs(\"EPSG:3857\").hvplot.points(\n    color=TNC_fall_2020[\"health\"].apply(lambda health: tree_palette[health]), \n    size=1.5, \n    alpha=0.7,\n    width=FIG_WIDTH_PX, \n    height=FIG_HEIGHT_PX\n)\n\nraster_map * point_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nAnd here’s tree health on top of vegetation index.\n\nNDVI_subset = NDVI_image.rio.clip([box(xmin, ymin, xmax, ymax)])\n\n\nraster_map = NDVI_subset.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=NDVI_CMAP, \n    tiles=\"OSM\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(NDVI_subset.quantile(0.02), NDVI_subset.quantile(0.98)),\n    title=\"ECOSTRESS Vegetation Index and in situ Tree Health\"\n)\n\npoint_map = TNC_fall_2020.to_crs(\"EPSG:3857\").hvplot.points(\n    color=TNC_fall_2020[\"health\"].apply(lambda health: tree_palette[health]), \n    size=1.5, \n    alpha=0.7,\n    width=FIG_WIDTH_PX, \n    height=FIG_HEIGHT_PX\n)\n\nraster_map * point_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nTo match these datasets together, let’s interpolate the ECOSTRESS rasters for evapotranspiration, surface temperature, soil moisture, and vegetation index to the Nature Conservency tree locations and make these sampled remote sensing data new columns in our table of tree observations.\n\nTNC_fall_2020[\"ET\"] = rasterstats.point_query(\n    vectors=TNC_fall_2020.geometry,\n    raster=np.array(ET_subset),\n    nodata=np.nan,\n    affine=ET_subset.rio.transform()\n)\n\nTNC_fall_2020[\"ST\"] = rasterstats.point_query(\n    vectors=TNC_fall_2020.geometry,\n    raster=np.array(ST_subset),\n    nodata=np.nan,\n    affine=ST_subset.rio.transform()\n)\n\nTNC_fall_2020[\"SM\"] = rasterstats.point_query(\n    vectors=TNC_fall_2020.geometry,\n    raster=np.array(SM_subset),\n    nodata=np.nan,\n    affine=SM_subset.rio.transform()\n)\n\nTNC_fall_2020[\"NDVI\"] = rasterstats.point_query(\n    vectors=TNC_fall_2020.geometry,\n    raster=np.array(NDVI_subset),\n    nodata=np.nan,\n    affine=NDVI_subset.rio.transform()\n)\n\n\nTNC_fall_2020 = TNC_fall_2020[[\"health\", \"ET\", \"ST\", \"SM\", \"NDVI\", \"geometry\"]]\nTNC_fall_2020.head()\n\n\n\n\n  \n    \n      \n      health\n      ET\n      ST\n      SM\n      NDVI\n      geometry\n    \n  \n  \n    \n      0\n      good\n      0.379620\n      34.051858\n      0.095231\n      0.322466\n      POINT Z (737195.535 3823512.913 191.300)\n    \n    \n      1\n      good\n      0.382044\n      34.182116\n      0.094925\n      0.323227\n      POINT Z (737183.595 3823525.373 194.800)\n    \n    \n      2\n      poor\n      0.381920\n      34.179540\n      0.094932\n      0.323164\n      POINT Z (737184.237 3823525.408 194.200)\n    \n    \n      3\n      good\n      0.378730\n      34.101747\n      0.095152\n      0.320645\n      POINT Z (737182.283 3823532.980 192.800)\n    \n    \n      4\n      good\n      0.378280\n      34.085753\n      0.095204\n      0.319984\n      POINT Z (737174.601 3823536.391 188.900)\n    \n  \n\n\n\n\nLet’s plot the distributions of ECOSTRESS evapotranspiration for each of the tree health categories. The dead tree points match to a lower distribution of interpolated ECOSTRESS evapotranspiration estimates.\n\nfig = plt.figure(figsize=(FIG_WIDTH_IN, FIG_HEIGHT_IN))\nax = sns.boxplot(\n    x=TNC_fall_2020.health, \n    y=TNC_fall_2020.ET,\n    palette=tree_palette\n)\nax.set(ylabel=\"ECOSTRESS Evapotranspiration (mm/day)\", xlabel=\"In Situ Tree Health\")\nplt.title(\"Distribution of ECOSTRESS Evapotranspiration by Tree Health\")\nplt.show()\nplt.close(fig)\n\n\n\n\nLet’s look at the distribution of ECOSTRESS surface temperature according to in situ tree health. The dead trees appear to have higher temperatures than other categories.\n\nfig = plt.figure(figsize=(FIG_WIDTH_IN, FIG_HEIGHT_IN))\nax = sns.boxplot(\n    x=TNC_fall_2020.health, \n    y=TNC_fall_2020.ST,\n    palette=tree_palette\n)\nax.set(ylabel=\"ECOSTRESS Surface Temperature (Celsius)\", xlabel=\"In Situ Tree Health\")\nplt.title(\"Distribution of ECOSTRESS Surface Temperature by Tree Health\")\nplt.show()\nplt.close(fig)\n\n\n\n\nLooking at this again with ECOSTRESS soil moisture, the dead trees appear to be associated with lower soil moisture estimates.\n\nfig = plt.figure(figsize=(FIG_WIDTH_IN, FIG_HEIGHT_IN))\nax = sns.boxplot(\n    x=TNC_fall_2020.health, \n    y=TNC_fall_2020.SM,\n    palette=tree_palette\n)\nax.set(ylabel=\"ECOSTRESS Soil Moisture ($m^3/m^3$)\", xlabel=\"In Situ Tree Health\")\nplt.title(\"Distribution of ECOSTRESS Soil Moisture by Tree Health\")\nplt.show()\nplt.close(fig)\n\n\n\n\nAnd the range of ECOSTRESS vegetation index at the dead trees appears to be lower than other categories.\n\nfig = plt.figure(figsize=(FIG_WIDTH_IN, FIG_HEIGHT_IN))\nax = sns.boxplot(\n    x=TNC_fall_2020.health, \n    y=TNC_fall_2020.NDVI,\n    palette=tree_palette\n)\nax.set(ylabel=\"ECOSTRESS Normalized Difference Vegetation Index\", xlabel=\"In Situ Tree Health\")\nplt.title(\"Distribution of ECOSTRESS Vegetation Index by Tree Health\")\nplt.show()\nplt.close(fig)\n\n\n\n\nLet’s also approach this match-up the other way, by rasterizing the in situ point data. We’ll spatially bin counts of the tree health categories within each 70 m ECOSTRESS pixel and calculate the ratio of these category counts to the total count of observations in each pixel. Let’s start with rasterizing the total counts first.\n\nx_bins = np.sort(ET_subset.x)\nx_bins = np.append(x_bins, x_bins[-1] + (x_bins[-1] - x_bins[-2]))\ny_bins = np.sort(ET_subset.y)\ny_bins = np.insert(y_bins, 0, y_bins[0] + (y_bins[0] - y_bins[1]))\n\nx = TNC_fall_2020.geometry.apply(lambda point: point.x)\ny = TNC_fall_2020.geometry.apply(lambda point: point.y)\n\ncounts, _, _, = np.histogram2d(\n    x=x,\n    y=y,\n    bins=(x_bins, y_bins)\n)\n\ncounts = np.rot90(counts)\ncounts = np.where(counts <= 0, np.nan, counts)\n\ncounts_image = xarray.core.dataarray.DataArray(\n    data=counts,\n    coords=ET_subset.coords,\n    dims=ET_subset.dims,\n    attrs=ET_subset.attrs\n)\n\ncounts_map = counts_image.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=\"jet\",\n    tiles=\"OSM\",\n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    title=\"Total Count of Tree Observations within Each 70 m ECOSTRESS Pixel\"\n)\n\nNow we’ll count the number of dead trees in each pixel and divide that by the total count to get a proportion of dead trees in each pixel. This proportion of dead trees appears to range from 5% to 25% across the rasterized image.\n\ndead_gdf = TNC_fall_2020[TNC_fall_2020.health == \"dead\"]\nx = dead_gdf.geometry.apply(lambda point: point.x)\ny = dead_gdf.geometry.apply(lambda point: point.y)\n\ncounts, _, _, = np.histogram2d(\n    x=x,\n    y=y,\n    bins=(x_bins, y_bins)\n)\n\ncounts = np.rot90(counts)\ncounts = np.where(counts <= 0, np.nan, counts)\n\ndead_counts_image = xarray.core.dataarray.DataArray(\n    data=counts,\n    coords=ET_subset.coords,\n    dims=ET_subset.dims,\n    attrs=ET_subset.attrs\n)\n\ndead_counts_image.rio.reproject(\"EPSG:3857\").hvplot.image(cmap=\"jet\", tiles=\"OSM\", alpha=0.7)\ndead_proportion_image = dead_counts_image / counts_image\ndead_proportion_image.attrs = ET_subset.attrs\n\ndead_proportion_map = dead_proportion_image.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=\"jet\",\n    tiles=\"OSM\",\n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX\n)\n\ndead_proportion_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe’ll do this same rasterization for good trees.\n\ngood_gdf = TNC_fall_2020[TNC_fall_2020.health == \"good\"]\nx = good_gdf.geometry.apply(lambda point: point.x)\ny = good_gdf.geometry.apply(lambda point: point.y)\n\ncounts, _, _, = np.histogram2d(\n    x=x,\n    y=y,\n    bins=(x_bins, y_bins)\n)\n\ncounts = np.rot90(counts)\ncounts = np.where(counts <= 0, np.nan, counts)\n\ngood_counts_image = xarray.core.dataarray.DataArray(\n    data=counts,\n    coords=ET_subset.coords,\n    dims=ET_subset.dims,\n    attrs=ET_subset.attrs\n)\n\ngood_counts_image.rio.reproject(\"EPSG:3857\").hvplot.image(cmap=\"jet\", tiles=\"OSM\", alpha=0.7)\ngood_proportion_image = good_counts_image / counts_image\ngood_proportion_image.attrs = ET_subset.attrs\n\ngood_proportion_map = good_proportion_image.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=\"jet\", \n    tiles=\"OSM\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX\n)\n\ngood_proportion_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\npoor_gdf = TNC_fall_2020[TNC_fall_2020.health == \"poor\"]\nx = poor_gdf.geometry.apply(lambda point: point.x)\ny = poor_gdf.geometry.apply(lambda point: point.y)\n\ncounts, _, _, = np.histogram2d(\n    x=x,\n    y=y,\n    bins=(x_bins, y_bins)\n)\n\ncounts = np.rot90(counts)\ncounts = np.where(counts <= 0, np.nan, counts)\n\npoor_counts_image = xarray.core.dataarray.DataArray(\n    data=counts,\n    coords=ET_subset.coords,\n    dims=ET_subset.dims,\n    attrs=ET_subset.attrs\n)\n\npoor_counts_image.rio.reproject(\"EPSG:3857\").hvplot.image(cmap=\"jet\", tiles=\"OSM\", alpha=0.7)\npoor_proportion_image = poor_counts_image / counts_image\npoor_proportion_image.attrs = ET_subset.attrs\n\npoor_proportion_map = poor_proportion_image.rio.reproject(\"EPSG:3857\").hvplot.image(\n    cmap=\"jet\", \n    tiles=\"OSM\", \n    alpha=0.7,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX\n)\n\npoor_proportion_map\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nNow let’s bring it all together by collapsing each of these images into columns of a table so we can analyze these matching data points.\n\nproportion_table = pd.DataFrame({\n    \"dead\": dead_proportion_image.values.ravel(), \n    \"poor\": poor_proportion_image.values.ravel(), \n    \"good\": good_proportion_image.values.ravel(), \n    \"ET\": ET_subset.values.ravel(),\n    \"ST\": ST_subset.values.ravel(),\n    \"SM\": SM_subset.values.ravel(),\n    \"NDVI\": NDVI_subset.values.ravel()\n}).dropna()\n\nproportion_table = proportion_table[np.abs(zscore(proportion_table.ET)) < 2]\nproportion_table.head()\n\n\n\n\n  \n    \n      \n      dead\n      poor\n      good\n      ET\n      ST\n      SM\n      NDVI\n    \n  \n  \n    \n      177\n      0.250000\n      0.050000\n      0.075000\n      0.374550\n      32.950012\n      0.099246\n      0.315343\n    \n    \n      179\n      0.114286\n      0.085714\n      0.057143\n      0.379082\n      32.250000\n      0.100922\n      0.343636\n    \n    \n      244\n      0.052632\n      0.289474\n      0.342105\n      0.357015\n      32.250000\n      0.100832\n      0.309682\n    \n    \n      310\n      0.041667\n      0.083333\n      0.250000\n      0.391165\n      32.570007\n      0.100025\n      0.352369\n    \n    \n      311\n      0.033333\n      0.016667\n      0.233333\n      0.371817\n      33.270020\n      0.098373\n      0.330158\n    \n  \n\n\n\n\nLet’s investigate the correlations between these variables with a correlogram. Focusing on dead trees, it seems that there are rather weak relationships between the dead tree observations and the evapotranspiration input variables, but there is an inverse correlation between the dead tree observations and the evapotranspiration estimate itself.\n\nfig = plt.figure(figsize=(FIG_WIDTH_IN, FIG_HEIGHT_IN))\nax = sns.heatmap(proportion_table.corr(), annot=True)\nplt.title(\"Correlogram of ECOSTRESS Products to Tree Health Proportions\")\nplt.show()\nplt.close(fig)\n\n\n\n\nWe can break down these correlation coefficients in to individual scatterplots to better see how each pair of variables line up together.\n\nfig = plt.figure()\nax = sns.pairplot(proportion_table)\nplt.title(\"Pair-Plot of ECOSTRESS Products to Tree Health Proportions\")\nplt.show()\nplt.close(fig)\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\nLet’s focus on the strongest relationship involving dead trees and ECOSTRESS evapotranspiration. We can visualize this inverse correlation with a scatterplot and decreasing trendline.\n\nfig = plt.figure(figsize=(FIG_WIDTH_IN, FIG_HEIGHT_IN))\nax = sns.regplot(x=proportion_table.dead, y=proportion_table.ET)\nax.set(ylabel=\"ECOSTRESS Evapotranspiration (mm/day)\", xlabel=\"Proportion of Dead Tree Observations\")\nplt.title(\"Scatterplot of ECOSTRESS Evapotranspiration to Proportion of Dead Tree Observations\")\nplt.show()\nplt.close(fig)"
  },
  {
    "objectID": "tutorials/Earthdata_search.html",
    "href": "tutorials/Earthdata_search.html",
    "title": "01. Earthdata Search",
    "section": "",
    "text": "This tutorial guides you through how to use Earthdata Search for NASA Earth observations search and discovery, and how to connect the search output (e.g. download or access links) to a programmatic workflow (locally or from within the cloud).\n\nStep 1. Go to Earthdata Search and Login\nGo to Earthdata Search https://search.earthdata.nasa.gov and use your Earthdata login credentials to log in. If you do not have an Earthdata account, please see the Workshop Prerequisites for guidance.\n\n\nStep 2. Search for dataset of interest\nUse the search box in the upper left to type key words. In this example we are interested in the ECOSTRESS LSTE which is managed by the LP DAAC and made available from the NASA Earthdata Cloud archive hosted in AWS cloud.\nType ECOSTRESS in the search bar Click on the “Available from AWS Cloud” filter option on the left.\n\n\n\nFigure caption: Search for ECOSTRESS data available in AWS cloud in Earthdata Search portal\n\n\nLet’s refine our search further. Let’s search for ECOSTRESS ECO_L2T_LSTE in the search box. A single Earthdata Seach Collection is returned.\nWe can click on the (i) icon for the dataset to read more details, including the dataset shortname (helpful for programmatic workflows) just below the dataset name; here ECO_L2T_LSTE.\n\n\n\nFigure caption: Refine search\n\n\n\n\nStep 3. Explore the dataset details, including Cloud Access information\nOnce we clicked the (i), scrolling down the info page for the dataset we will see Cloud Access information, such as:\n\nwhether the dataset is available in the cloud\n\nthe cloud Region (all NASA Earthdata Cloud data is/will be in us-west-2 region)\n\nthe S3 storage bucket and object prefix where this data is located\n\nlink that generates AWS S3 Credentials for in-cloud data access (we will cover this in the Direct Data Access Tutorials)\n\nlink to documentation describing the In-region Direct S3 Access to Buckets. Note: these will be unique depending on the DAAC where the data is archived. (We will show examples of direct in-region access in Tutorial 3.)\n\n\n\n\nFigure caption: Cloud access info in EDS\n\n\n\n\n\nFigure caption: Documentation describing the In-region Direct S3 Access to Buckets\n\n\nNote: Clicking on “For Developers” to exapnd will provide programmatic endpoints such as those for the CMR API, and more.\nFor now, let’s say we are intersted in getting download link(s) or access link(s) for specific data files (granules) within this collection.\nAt the top of the dataset info section, click on Search Results, which will take us back to the list of datasets matching our search parameters. Clicking on the dataset (ECOSTRESS ECO_L2T_LSTE) we now see a list of files (granules) that are part of the dataset (collection).\n\n\nStep 4a. Download or data access for a single granule\nTo download files for a granule click the download arrow on the card (or list row)\n\n\n\nFigure caption: Download granules\n\n\nYou can also get the S3 information (e.g., AWS region, bucket, temperary credentials for S3 access, and file names) by selecting the AWS S3 Access tab.\n\n\n\nFigure caption: S3 access for granules\n\n\n\nStep 4b. Download or data access for multiple granule\nTo download multiple granules, click on the green + symbol to add files to our project. Click on the green button towards the bottom that says “Download”. This will take us to another page with options to customize our download or access link(s).\n\n\n\nFigure caption: Select granules and click download\n\n\nOn the next page click the Direct Download option and click the green Download Data on the bottom left side of the page.\n\n\n\nFigure caption: Direct download multiple granules\n\n\nWe’re now taked to the final page for instructions to download and links for data access in the cloud. You should see three tabs: Download Files, AWS S3 Access, Download Script:\n\n\n\nFigure caption: Download to local\n\n\n\n\n\nFigure caption: Direct S3 access\n\n\nThe Download Files tab provides the https:// links for downloading the files locally\nThe AWS S3 Access tab provides the S3:// links, which is what we would use to access the data directly in-region (us-west-2) within the AWS cloud."
  },
  {
    "objectID": "tutorials/Intro_xarray_hvplot.html#why-do-we-need-xarray",
    "href": "tutorials/Intro_xarray_hvplot.html#why-do-we-need-xarray",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Why do we need xarray?",
    "text": "Why do we need xarray?\nAs Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "tutorials/Intro_xarray_hvplot.html#what-is-xarray",
    "href": "tutorials/Intro_xarray_hvplot.html#what-is-xarray",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "What is xarray",
    "text": "What is xarray\nxarray is an open-source project and python package to work with labelled multi-dimensional arrays. It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with built-in methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. It will change your life for the better. You’ll be more attractive, more interesting, and better equiped to take on lifes challenges."
  },
  {
    "objectID": "tutorials/Intro_xarray_hvplot.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials/Intro_xarray_hvplot.html#what-you-will-learn-from-this-tutorial",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\nIn this tutorial you will learn how to:\n\nload a netcdf file into xarray\ninterrogate the Dataset and understand the difference between DataArray and Dataset\nsubset a Dataset\ncalculate annual and monthly mean fields\ncalculate a time series of zonal means\nplot these results\n\nAs always, we’ll start by importing xarray. We’ll follow convention by giving the module the shortname xr\n\nimport xarray as xr\nxr.set_options(keep_attrs=True)\nimport hvplot.xarray\n\nImportError: cannot import name 'Markup' from 'jinja2' (/srv/conda/envs/notebook/lib/python3.9/site-packages/jinja2/__init__.py)\n\n\nI’m going to use one of xarray’s tutorial datasets. In this case, air temperature from the NCEP reanalysis. I’ll assign the result of the open_dataset to ds. I may change this to access a dataset directly\n\nds = xr.tutorial.open_dataset(\"air_temperature\")\n\nAs we are in an interactive environment, we can just type ds to see what we have.\n\nds\n\nFirst thing to notice is that ds is an xarray.Dataset object. It has dimensions, lat, lon, and time. It also has coordinate variables with the same names as these dimensions. These coordinate variables are 1-dimensional. This is a NetCDF convention. The Dataset contains one data variable, air. This has dimensions (time, lat, lon).\nClicking on the document icon reveals attributes for each variable. Clicking on the disk icon reveals a representation of the data.\nEach of the data and coordinate variables can be accessed and examined using the variable name as a key.\n\nds.air\n\n\nds['air']\n\nThese are xarray.DataArray objects. This is the basic building block for xarray.\nVariables can also be accessed as attributes of ds.\n\nds.time\n\nA major difference between accessing a variable as an attribute versus using a key is that the attribute is read-only but the key method can be used to update the variable. For example, if I want to convert the units of air from Kelvin to degrees Celsius.\n\nds['air'] = ds.air - 273.15\n\nThis approach can also be used to add new variables\n\nds['air_kelvin'] = ds.air + 273.15\n\n\nds\n\nIt is helpful to update attributes such as units, this saves time, confusion and mistakes, especially when you save the dataset.\n\nds['air'].attrs['units'] = 'degC'\n\n\nds"
  },
  {
    "objectID": "tutorials/Intro_xarray_hvplot.html#subsetting-and-indexing",
    "href": "tutorials/Intro_xarray_hvplot.html#subsetting-and-indexing",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Subsetting and Indexing",
    "text": "Subsetting and Indexing\nSubsetting and indexing methods depend on whether you are working with a Dataset or DataArray. A DataArray can be accessed using positional indexing just like a numpy array. To access the temperature field for the first time step, you do the following.\n\nds['air'][0,:,:]\n\nNote this returns a DataArray with coordinates but not attributes.\nHowever, the real power is being able to access variables using coordinate variables. I can get the same subset using the following. (It’s also more explicit about what is being selected and robust in case I modify the DataArray and expect the same output.)\n\nds['air'].sel(time='2013-01-01').time\n\n\nds.air.sel(time='2013-01-01')\n\nI can also do slices. I’ll extract temperatures for the state of Colorado. The bounding box for the state is [-109 E, -102 E, 37 N, 41 N].\nIn the code below, pay attention to both the order of the coordinates and the range of values. The first value of the lat coordinate variable is 41 N, the second value is 37 N. Unfortunately, xarray expects slices of coordinates to be in the same order as the coordinates. Note lon is 0 to 360 not -180 to 180, and I let python calculate it for me within the slice.\n\nds.air.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\nWhat if we want temperature for a point, for example Denver, CO (39.72510678889283 N, -104.98785545855408 E). xarray can handle this! If we just want data from the nearest grid point, we can use sel and specify the method as “nearest”.\n\ndenver_lat, denver_lon = 39.72510678889283, -104.98785545855408\n\n\nds.air.sel(lat=denver_lat, lon=360+denver_lon, method='nearest').hvplot()\n\nIf we want to interpolate, we can use interp(). In this case I use linear or bilinear interpolation.\ninterp() can also be used to resample data to a new grid and even reproject data\n\nds.air.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\nsel() and interp() can also be used on Dataset objects.\n\nds.sel(lat=slice(41,37), lon=slice(360-109,360-102))\n\n\nds.interp(lat=denver_lat, lon=360+denver_lon, method='linear')"
  },
  {
    "objectID": "tutorials/Intro_xarray_hvplot.html#analysis",
    "href": "tutorials/Intro_xarray_hvplot.html#analysis",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Analysis",
    "text": "Analysis\nAs a simple example, let’s try to calculate a mean field for the whole time range.\n\nds.mean(dim='time').hvplot()\n\nWe can also calculate a zonal mean (averaging over longitude)\n\nds.mean(dim='lon').hvplot()\n\nOther aggregation methods include min(), max(), std(), along with others.\n\nds.std(dim='time').hvplot()\n\nThe data we have are in 6h timesteps. This can be resampled to daily or monthly. If you are familiar with pandas, xarray uses the same methods.\n\nds_mon = ds.resample(time='M').mean()\nds_mon\n\nThis is a really short time series but as an example, let’s calculate a monthly climatology (at least for 2 months). For this we can use groupby()\n\nds_clim = ds_mon.groupby(ds_mon.time.dt.month).mean()\nds_clim"
  },
  {
    "objectID": "tutorials/Intro_xarray_hvplot.html#plot-results",
    "href": "tutorials/Intro_xarray_hvplot.html#plot-results",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Plot results",
    "text": "Plot results\nFinally, let’s plot the results! This will plot the lat/lon axes of the original ds DataArray.\n\nds_clim.air.sel(month=10).hvplot()"
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#timing",
    "href": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#timing",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Timing",
    "text": "Timing\n\nExercise: 30 min"
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#summary",
    "href": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#summary",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nIn this example we will access the NASA’s Harmonized Landsat Sentinel-2 (HLS) version 2 assets, which are archived in cloud optimized geoTIFF (COG) format in the LP DAAC Cumulus cloud space. The COGs can be used like any other geoTIFF file, but have some added features that make them more efficient within the cloud data access paradigm. These features include: overviews and internal tiling. Below we will demonstrate how to leverage these features.\n\nBut first, what is STAC?\nSpatioTemporal Asset Catalog (STAC) is a specification that provides a common language for interpreting geospatial information in order to standardize indexing and discovering data.\nThe STAC specification is made up of a collection of related, yet independent specifications that when used together provide search and discovery capabilities for remote assets.\n\nFour STAC Specifications\nSTAC Catalog (aka DAAC Archive)\nSTAC Collection (aka Data Product)\nSTAC Item (aka Granule)\nSTAC API\nIn the following sections, we will explore each of STAC element using NASA’s Common Metadata Repository (CMR) STAC application programming interface (API), or CMR-STAC API for short.\n\n\n\nCMR-STAC API\nThe CMR-STAC API is NASA’s implementation of the STAC API specification for all NASA data holdings within EOSDIS. The current implementation does not allow for querries accross the entire NASA catalog. Users must execute searches within provider catalogs (e.g., LPCLOUD) to find the STAC Items they are searching for. All the providers can be found at the CMR-STAC endpoint here: https://cmr.earthdata.nasa.gov/stac/.\nIn this exercise, we will query the LPCLOUD provider to identify STAC Items from the Harmonized Landsat Sentinel-2 (HLS) collection that fall within our region of interest (ROI) and within our specified time range."
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#what-you-will-learn-from-this-tutorial",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\n\nhow to connect to NASA CMR-STAC API using Python’s pystac-client\n\nhow to navigate CMR-STAC records\n\nhow to read in a geojson file using geopandas to specify your region of interest\nhow to use the CMR-STAC API to search for data\nhow to perform post-search filtering of CMR-STAC API search result in Python\n\nhow to extract and save data access URLs for geospatial assets\n\nThis exercise can be found in the 2021 Cloud Hackathon Book"
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#import-required-packages",
    "href": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#import-required-packages",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\nfrom pystac_client import Client  \nfrom collections import defaultdict    \nimport json\nimport geopandas\nfrom shapely.geometry import Polygon\nimport geoviews as gv\nfrom cartopy import crs\nfrom pyproj import CRS\ngv.extension('bokeh', 'matplotlib')"
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#explored-available-nasa-providers",
    "href": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#explored-available-nasa-providers",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Explored available NASA Providers",
    "text": "Explored available NASA Providers\n\nSTAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n\n\nConnect to the CMR-STAC API\n\nprovider_cat = Client.open(STAC_URL)\n\nWe’ll create a providers variable so we can take a deeper look into available data providers - subcategories are referred to as “children”. We can then print them as a for loop.\n\nproviders = [p for p in provider_cat.get_children()]\n\nfor count, provider in enumerate(providers):\n    print(f'{count} - {provider.title}')\n\n0 - LARC_ASDC\n1 - USGS_EROS\n2 - ESA\n3 - GHRC\n4 - LAADS\n5 - OBPG\n6 - OB_DAAC\n7 - ECHO\n8 - ISRO\n9 - LPCUMULUS\n10 - EDF_DEV04\n11 - GES_DISC\n12 - ASF\n13 - OMINRT\n14 - EUMETSAT\n15 - NCCS\n16 - NSIDCV0\n17 - PODAAC\n18 - LARC\n19 - USGS\n20 - SCIOPS\n21 - LANCEMODIS\n22 - CDDIS\n23 - JAXA\n24 - AU_AADC\n25 - ECHO10_OPS\n26 - LPDAAC_ECS\n27 - NSIDC_ECS\n28 - ORNL_DAAC\n29 - LM_FIRMS\n30 - SEDAC\n31 - LANCEAMSR2\n32 - NOAA_NCEI\n33 - USGS_LTA\n34 - GESDISCCLD\n35 - GHRSSTCWIC\n36 - ASIPS\n37 - ESDIS\n38 - POCLOUD\n39 - NSIDC_CPRD\n40 - ORNL_CLOUD\n41 - FEDEO\n42 - MLHUB\n43 - XYZ_PROV\n44 - GHRC_DAAC\n45 - CSDA\n46 - NRSCC\n47 - CEOS_EXTRA\n48 - AMD_KOPRI\n49 - MOPITT\n50 - GHRC_CLOUD\n51 - LPCLOUD\n52 - CCMEO"
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#connect-to-the-lpcloud-providerstac-catalog",
    "href": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#connect-to-the-lpcloud-providerstac-catalog",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Connect to the LPCLOUD Provider/STAC Catalog",
    "text": "Connect to the LPCLOUD Provider/STAC Catalog\nFor this next step we need the provider title (e.g., LPCLOUD) from above. We will add the provider to the end of the CMR-STAC API URL (i.e., https://cmr.earthdata.nasa.gov/stac/) to connect to the LPCLOUD STAC Catalog.\n\ncatalog = Client.open(f'{STAC_URL}/LPCLOUD/')\n\nSince we are using a dedicated client (i.e., pystac-client.Client) to connect to our STAC Provider Catalog, we will have access to some useful internal methods and functions (e.g., get_children() or get_all_items()) we can use to get information from these objects."
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#list-stac-collections",
    "href": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#list-stac-collections",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "List STAC Collections",
    "text": "List STAC Collections\nWe’ll create a products variable to view deeper in the STAC Catalog.\n\nproducts = [c for c in catalog.get_children()]\n\n\nPrint one of the STAC Collection records\nTo view the products variable we just created, let’s look at one entry as a dictionary.\n\nproducts[1].to_dict()\n\n{'type': 'Collection',\n 'id': 'HLSL30.v2.0',\n 'stac_version': '1.0.0',\n 'description': 'The Harmonized Landsat and Sentinel-2 (HLS) project provides consistent surface reflectance (SR) and top of atmosphere (TOA) brightness data from the Operational Land Imager (OLI) aboard the joint NASA/USGS Landsat 8 satellite and the Multi-Spectral Instrument (MSI) aboard Europe’s Copernicus Sentinel-2A and Sentinel-2B satellites. The combined measurement enables global observations of the land every 2–3 days at 30-meter (m) spatial resolution. The HLS project uses a set of algorithms to obtain seamless products from OLI and MSI that include atmospheric correction, cloud and cloud-shadow masking, spatial co-registration and common gridding, illumination and view angle normalization, and spectral bandpass adjustment.\\r\\n\\r\\nThe HLSL30 product provides 30-m Nadir Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Landsat 8 OLI data products. The HLSS30 and HLSL30 products are gridded to the same resolution and Military Grid Reference System ([MGRS](https://hls.gsfc.nasa.gov/products-description/tiling-system/)) tiling system, and thus are “stackable” for time series analysis.\\r\\n\\r\\nThe HLSL30 product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate file. There are 11 bands included in the HLSL30 product along with one quality assessment (QA) band and four angle bands. See the User Guide for a more detailed description of the individual bands provided in the HLSL30 product.',\n 'links': [{'rel': <RelType.ROOT: 'root'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/',\n   'type': <MediaType.JSON: 'application/json'>,\n   'title': 'NASA CMR STAC Proxy'},\n  {'rel': 'items',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/items',\n   'type': 'application/json',\n   'title': 'Granules in this collection'},\n  {'rel': 'about',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2021957657-LPCLOUD.html',\n   'type': 'text/html',\n   'title': 'HTML metadata for collection'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2021957657-LPCLOUD.json',\n   'type': 'application/json',\n   'title': 'CMR JSON metadata for collection'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2013',\n   'type': 'application/json',\n   'title': '2013 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2014',\n   'type': 'application/json',\n   'title': '2014 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2015',\n   'type': 'application/json',\n   'title': '2015 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2016',\n   'type': 'application/json',\n   'title': '2016 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2017',\n   'type': 'application/json',\n   'title': '2017 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2018',\n   'type': 'application/json',\n   'title': '2018 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2019',\n   'type': 'application/json',\n   'title': '2019 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2020',\n   'type': 'application/json',\n   'title': '2020 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2021',\n   'type': 'application/json',\n   'title': '2021 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2022',\n   'type': 'application/json',\n   'title': '2022 catalog'},\n  {'rel': <RelType.SELF: 'self'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0',\n   'type': <MediaType.JSON: 'application/json'>},\n  {'rel': <RelType.PARENT: 'parent'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/',\n   'type': <MediaType.JSON: 'application/json'>,\n   'title': 'LPCLOUD'}],\n 'stac_extensions': [],\n 'title': 'HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0',\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['2013-04-11T00:00:00Z', None]]}},\n 'license': 'not-provided'}\n\n\n\n\nPrint the STAC Collection ids with their title\nIn the above output, id and title are two elements of interest that we can print for all products using a for loop.\n\nfor p in products: \n    print(f\"{p.id}: {p.title}\")\n\nASTGTM.v003: ASTER Global Digital Elevation Model V003\nHLSL30.v2.0: HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0\nHLSS30.v2.0: HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0"
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#search-for-granulesstac-items---set-up-query-parameters-to-submit-to-the-cmr-stac-api",
    "href": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#search-for-granulesstac-items---set-up-query-parameters-to-submit-to-the-cmr-stac-api",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Search for Granules/STAC Items - Set up query parameters to submit to the CMR-STAC API",
    "text": "Search for Granules/STAC Items - Set up query parameters to submit to the CMR-STAC API\nWe will define our ROI using a geojson file containing a small polygon feature in western Nebraska, USA. The geojson file is found in the ~/data directory. We’ll also specify the data collections and a time range for our example.\n\nRead in a geojson file\nReading in a geojson file with geopandas will return the geometry of our polygon (our ROI).\nNOTE: If you are running the notebook from the tutorials-templates directory, please use the following path to connect to the geojson file: “../tutorials/data/ne_w_agfields.geojson”\n\npoints = geopandas.read_file('../data/TNC_fall_2020.geojson')\npoints\n\n\n\n\n  \n    \n      \n      planting_date\n      health\n      geometry\n    \n  \n  \n    \n      0\n      2019-05-10\n      good\n      POINT Z (-120.41578 34.52600 191.30000)\n    \n    \n      1\n      2019-05-10\n      good\n      POINT Z (-120.41590 34.52611 194.80000)\n    \n    \n      2\n      2019-05-10\n      poor\n      POINT Z (-120.41589 34.52611 194.20000)\n    \n    \n      3\n      2019-05-10\n      good\n      POINT Z (-120.41591 34.52618 192.80000)\n    \n    \n      4\n      2019-05-10\n      good\n      POINT Z (-120.41600 34.52621 188.90000)\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      4058\n      2020-10-05\n      good\n      POINT Z (-120.41725 34.53041 0.00000)\n    \n    \n      4059\n      2020-10-05\n      good\n      POINT Z (-120.41718 34.53040 0.00000)\n    \n    \n      4060\n      2020-10-05\n      good\n      POINT Z (-120.41710 34.53039 0.00000)\n    \n    \n      4061\n      2020-10-06\n      good\n      POINT Z (-120.40952 34.52643 0.00000)\n    \n    \n      4062\n      2021-08-26\n      good\n      POINT Z (-120.41266 34.52768 207.56970)\n    \n  \n\n4063 rows × 3 columns\n\n\n\n\n\nVisualize contents of geojson file\nWe can use that geometry to visualize the polygon: here, a square. But wait for it –\n\nminx, miny, maxx, maxy = points.geometry.total_bounds\nminx, miny, maxx, maxy\n\n(-120.45264628215773,\n 34.51050622261265,\n -120.40432447545712,\n 34.532398755692206)\n\n\nGeoJSON polygon boundary coordinates must be ordered counterclockwise\n\ncoords = [\n    (minx, miny),\n    (maxx, miny),\n    (maxx, maxy),\n    (minx, maxy)\n]\n\n\nfeature_shape = Polygon(coords)\nfeature_shape\n\n\n\n\nWe can plot the polygon using the geoviews package that we imported as gv with ‘bokeh’ and ‘matplotlib’ extensions. The following has reasonable width, height, color, and line widths to view our polygon when it is overlayed on a base tile map.\n\nbase = gv.tile_sources.EsriImagery.opts(width=650, height=500)\nfarmField = gv.Polygons(feature_shape).opts(line_color='yellow', line_width=10, color=None)\nbase * farmField\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe will now start to specify the search criteria we are interested in, i.e, the date range, the ROI, and the data collections, that we will pass to the STAC API.\n\n\nExtract the coordinates for the ROI\n\ncrs = CRS('epsg:4326')\npolygon = geopandas.GeoDataFrame(index=[0], crs=crs, geometry=[feature_shape])\npolygon\n\n\n\n\n  \n    \n      \n      geometry\n    \n  \n  \n    \n      0\n      POLYGON ((-120.45265 34.51051, -120.40432 34.5...\n    \n  \n\n\n\n\n\nroi = json.loads(polygon.to_json())['features'][0]['geometry']\nroi\n\n{'type': 'Polygon',\n 'coordinates': [[[-120.45264628215773, 34.51050622261265],\n   [-120.40432447545712, 34.51050622261265],\n   [-120.40432447545712, 34.532398755692206],\n   [-120.45264628215773, 34.532398755692206],\n   [-120.45264628215773, 34.51050622261265]]]}\n\n\nSo, what just happen there? Let’s take a quick detour to break it down.\n\n\n\nSpecify date range\nNext up is to specify our date range using ISO_8601 date formatting.\n\n#date_range = \"2021-05-01T00:00:00Z/2021-08-30T23:59:59Z\"    # closed interval\n#date_range = \"2021-05-01T00:00:00Z/..\"                      # open interval - does not currently work with the CMR-STAC API\n#date_range = \"2021-05/2021-08\"\ndate_range = \"2020-09\"\n\n\n\nSpecify the STAC Collections\nSTAC Collection is synonomous with what we usually consider a NASA data product. Desired STAC Collections are submitted to the search API as a list containing the collection id. We can use the ids that we printed from our products for loop above. Let’s focus on S30 and L30 collections.\n\ncollections = ['HLSL30.v2.0', 'HLSS30.v2.0']\ncollections\n\n['HLSL30.v2.0', 'HLSS30.v2.0']"
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#search-the-cmr-stac-api-with-our-search-criteria",
    "href": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#search-the-cmr-stac-api-with-our-search-criteria",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Search the CMR-STAC API with our search criteria",
    "text": "Search the CMR-STAC API with our search criteria\nNow we can put all our search criteria together using catalog.search from the pystac_client package.\n\nsearch = catalog.search(\n    collections=collections,\n    intersects=roi,\n    datetime=date_range,\n    limit=100\n)\n\n\nPrint out how many STAC Items match our search query\n\nsearch.matched()\n\n4\n\n\nWe now have a search object containing the STAC Items that matched our query. Now, let’s pull out all of the STAC Items (as a PySTAC ItemCollection object) and explore the contents (i.e., the STAC Items)\n\nitem_collection = search.get_all_items()\n\nLet’s list some of the Items from our pystac item_collection:\n\nlist(item_collection)[0:5]\n\n[<Item id=HLS.L30.T10SGD.2020247T184028.v2.0>,\n <Item id=HLS.L30.T10SGD.2020256T183444.v2.0>,\n <Item id=HLS.L30.T10SGD.2020263T184033.v2.0>,\n <Item id=HLS.L30.T10SGD.2020272T183449.v2.0>]\n\n\nWe can view a single Item as a dictionary, as we did above with STAC Collections/products.\n\nitem_collection[0].to_dict()\n\n{'type': 'Feature',\n 'stac_version': '1.0.0',\n 'id': 'HLS.L30.T10SGD.2020247T184028.v2.0',\n 'properties': {'datetime': '2020-09-03T18:40:28.257000Z',\n  'start_datetime': '2020-09-03T18:40:28.257Z',\n  'end_datetime': '2020-09-03T18:40:52.144Z',\n  'eo:cloud_cover': 48},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-120.464742, 34.2267025],\n    [-120.1745662, 35.2100913],\n    [-120.8028976, 35.2231329],\n    [-120.8289365, 34.2336951],\n    [-120.464742, 34.2267025]]]},\n 'links': [{'rel': 'self',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/items/HLS.L30.T10SGD.2020247T184028.v2.0'},\n  {'rel': 'parent',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0'},\n  {'rel': 'collection',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0'},\n  {'rel': <RelType.ROOT: 'root'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/',\n   'type': <MediaType.JSON: 'application/json'>,\n   'title': 'LPCLOUD'},\n  {'rel': 'provider', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2153816661-LPCLOUD.json'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2153816661-LPCLOUD.umm_json'}],\n 'assets': {'B06': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B06.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B06.tif'},\n  'VZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.VZA.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.VZA.tif'},\n  'SAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.SAA.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.SAA.tif'},\n  'B11': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B11.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B11.tif'},\n  'VAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.VAA.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.VAA.tif'},\n  'B02': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B02.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B02.tif'},\n  'SZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.SZA.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.SZA.tif'},\n  'B09': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B09.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B09.tif'},\n  'B03': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B03.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B03.tif'},\n  'B07': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B07.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B07.tif'},\n  'B10': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B10.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B10.tif'},\n  'B04': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B04.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B04.tif'},\n  'B05': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B05.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B05.tif'},\n  'Fmask': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.Fmask.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.Fmask.tif'},\n  'B01': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B01.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B01.tif'},\n  'browse': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.jpg',\n   'type': 'image/jpeg',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.jpg'},\n  'metadata': {'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2153816661-LPCLOUD.xml',\n   'type': 'application/xml'}},\n 'bbox': [-120.828937, 34.226702, -120.174566, 35.223133],\n 'stac_extensions': ['https://stac-extensions.github.io/eo/v1.0.0/schema.json'],\n 'collection': 'HLSL30.v2.0'}"
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#filtering-stac-items",
    "href": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#filtering-stac-items",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Filtering STAC Items",
    "text": "Filtering STAC Items\nWhile the CMR-STAC API is a powerful search and discovery utility, it is still maturing and currently does not have the full gamut of filtering capabilities that the STAC API specification allows for. Hence, additional filtering is required if we want to filter by a property, for example cloud cover. Below we will loop through and filter the item_collection by a specified cloud cover as well as extract the band we’d need to do an Enhanced Vegetation Index (EVI) calculation for a future analysis.\nWe’ll make a cloudcover variable where we will set the maximum allowable cloud cover and extract the band links for those Items that match or are less than the max cloud cover.\n\ncloudcover = 100\n\nWe will also specify the STAC Assets (i.e., bands/layers) of interest for both the S30 and L30 collections (also in our collections variable above).\nIn this hypothetical workflow, we’ll extract the bands needed to calculate an enhanced vegetation index (EVI). Thus, the band needed include red, near infrared (NIR), and blue. We’ll also extract a quality band (i.e., Fmask) that we’d eventually use to perform per-pixel quality filtering.\nNotice that the band ids are in some case not one-to-one between the S30 and the L30 product. This is evident in the NIR band for each product where S30’s NIR band id is B8A and L30’s is B05. Note, the S30 product has an additional NIR band with a band id of B08, but the spectral ranges between B8A and B05 are more closely aligned. Visit the HLS Overview page to learn more about HLS spectral bands.\n\ns30_bands = ['B8A', 'B04', 'B02', 'Fmask']    # S30 bands for EVI calculation and quality filtering -> NIR, RED, BLUE, Quality \nl30_bands = ['B05', 'B04', 'B02', 'Fmask']    # L30 bands for EVI calculation and quality filtering -> NIR, RED, BLUE, Quality \n\nAnd now to loop through and filter the item_collection by cloud cover and bands:\n\nevi_band_links = []\n\nfor i in item_collection:\n    if i.properties['eo:cloud_cover'] <= cloudcover:\n        if i.collection_id == 'HLSS30.v2.0':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = s30_bands\n        elif i.collection_id == 'HLSL30.v2.0':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = l30_bands\n\n        for a in i.assets:\n            if any(b==a for b in evi_bands):\n                evi_band_links.append(i.assets[a].href)\n\nThe filtering done in the previous steps produces a list of links to STAC Assets. Let’s print out the first ten links.\n\nevi_band_links[:10]\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B02.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B05.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.Fmask.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020256T183444.v2.0/HLS.L30.T10SGD.2020256T183444.v2.0.Fmask.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020256T183444.v2.0/HLS.L30.T10SGD.2020256T183444.v2.0.B02.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020256T183444.v2.0/HLS.L30.T10SGD.2020256T183444.v2.0.B05.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020256T183444.v2.0/HLS.L30.T10SGD.2020256T183444.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020263T184033.v2.0/HLS.L30.T10SGD.2020263T184033.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020263T184033.v2.0/HLS.L30.T10SGD.2020263T184033.v2.0.Fmask.tif']\n\n\nNOTE that HLS data is mapped to the Universal Transverse Mercator (UTM) projection and is tiled using the Sentinel-2 Military Grid Reference System (MGRS) UTM grid. Notice that in the list of links we have multiple tiles, i.e. T14TKL & T13TGF, that intersect with our region of interest. In this case, these two tiles represent neighboring UTM zones. The tiles can be discern from the file name, which is the last element in a link (far right) following the last forward slash (/) - e.g., HLS.L30.T14TKL.2021133T172406.v1.5.B04.tif. The figure below explains where to find the tile/UTM zone from the file name.\n\nWe will now split the list of links into separate logical sub-lists."
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#split-data-links-list-into-logical-groupings",
    "href": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#split-data-links-list-into-logical-groupings",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Split Data Links List into Logical Groupings",
    "text": "Split Data Links List into Logical Groupings\nWe have a list of links to data assets that meet our search and filtering criteria. Below we’ll split our list from above into lists first by tile/UTM zone and then further by individual bands bands. The commands that follow will do the splitting with python routines.\n\nSplit by UTM tile specified in the file name (e.g., T14TKL & T13TGF)\n\ntile_dicts = defaultdict(list)    # https://stackoverflow.com/questions/26367812/appending-to-list-in-python-dictionary\n\n\nfor l in evi_band_links:\n    tile = l.split('.')[-6]\n    tile_dicts[tile].append(l)\n\n\nPrint dictionary keys and values, i.e. the data links\n\ntile_dicts.keys()\n\ndict_keys(['T10SGD'])\n\n\n\ntile_dicts['T10SGD'][:5]\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B02.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B05.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.Fmask.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020256T183444.v2.0/HLS.L30.T10SGD.2020256T183444.v2.0.Fmask.tif']\n\n\nNow we will create a separate list of data links for each tile\n\ntile_links_T10SGD = tile_dicts['T10SGD']\n\n\n\nPrint band/layer links for HLS tile T13TGF\n\n# tile_links_T10SGD[:10]\n\n\n\n\nSplit the links by band\n\nbands_dicts = defaultdict(list)\n\n\nfor b in tile_links_T10SGD:\n    band = b.split('.')[-2]\n    bands_dicts[band].append(b)\n\n\nbands_dicts.keys()\n\ndict_keys(['B02', 'B04', 'B05', 'Fmask'])\n\n\n\nbands_dicts['B04']\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020256T183444.v2.0/HLS.L30.T10SGD.2020256T183444.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020263T184033.v2.0/HLS.L30.T10SGD.2020263T184033.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020272T183449.v2.0/HLS.L30.T10SGD.2020272T183449.v2.0.B04.tif']"
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#save-links-to-a-text-file",
    "href": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#save-links-to-a-text-file",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Save links to a text file",
    "text": "Save links to a text file\nTo complete this exercise, we will save the individual link lists as separate text files with descriptive names.\nNOTE: If you are running the notebook from the tutorials-templates directory, please use the following path to write to the data directory: “../tutorials/data/{name}”\n\nWrite links from CMR-STAC API to a file\n\nfor k, v in bands_dicts.items():\n    name = (f'HTTPS_T10SGD_{k}_Links.txt')\n    with open(f'../data/{name}', 'w') as f:    # use ../tutorials/data/{name} as your path if running the notebook from \"tutorials-template\"\n        for l in v:\n            f.write(f\"{l}\" + '\\n')\n\n\n\nWrite links to file for S3 access\n\nfor k, v in bands_dicts.items():\n    name = (f'S3_T10SGD_{k}_Links.txt')\n    with open(f'../data/{name}', 'w') as f:    # use ../tutorials/data/{name} as your path if running the notebook from \"tutorials-template\"\n        for l in v:\n            s3l = l.replace('https://data.lpdaac.earthdatacloud.nasa.gov/', 's3://')\n            f.write(f\"{s3l}\" + '\\n')"
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#resources",
    "href": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html#resources",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Resources",
    "text": "Resources\n\nSTAC Specification Webpage\nSTAC API Documentation\nCMR-STAC API Github\nPySTAC Client Documentation\nhttps://stackoverflow.com/questions/26367812/appending-to-list-in-python-dictionary\nGeopandas\nHLS Overview"
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR_API.html#summary",
    "href": "tutorials/additional_resources/Data_Discovery_CMR_API.html#summary",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will walk through how to search for Earthdata data collections and granules. Along the way we will explore the available search parameters, information return, and specific contrains when using the CMR API. Our object is to identify assets to access that we would downloaded, or perform S3 direct access, within an analysis workflow\nWe will be querying CMR for Harmonized Landsat Sentinel-2 (HLS) collections/granules to identify assets we would downloaded, or perform S3 direct access, within an analysis workflow"
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR_API.html#requirements",
    "href": "tutorials/additional_resources/Data_Discovery_CMR_API.html#requirements",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Requirements",
    "text": "Requirements\n\n1. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up."
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR_API.html#learning-objectives",
    "href": "tutorials/additional_resources/Data_Discovery_CMR_API.html#learning-objectives",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nunderstand what CMR/CMR API is and what CMR/CMR API can be used for\nhow to use the requests package to search data collections and granules\n\nhow to parse the results of these searches."
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR_API.html#what-is-cmr",
    "href": "tutorials/additional_resources/Data_Discovery_CMR_API.html#what-is-cmr",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "What is CMR",
    "text": "What is CMR\nCMR is the Common Metadata Repository. It catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). It is the backend of Earthdata Search, the GUI search interface you are probably familiar with. More information about CMR can be found here.\nUnfortunately, the GUI for Earthdata Search is not accessible from a cloud instance - at least not without some work. Earthdata Search is also not immediately reproducible. What I mean by that is if you create a search using the GUI you would have to note the search criteria (date range, search area, collection name, etc), take a screenshot, copy the search url, or save the list of data granules returned by the search, in order to recreate the search. This information would have to be re-entered each time you or someone else wanted to do the search. You could make typos or other mistakes. A cleaner, reproducible solution is to search CMR programmatically using the CMR API."
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR_API.html#what-is-the-cmr-api",
    "href": "tutorials/additional_resources/Data_Discovery_CMR_API.html#what-is-the-cmr-api",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "What is the CMR API",
    "text": "What is the CMR API\nAPI stands for Application Programming Interface. It allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results."
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR_API.html#getting-started-how-to-search-cmr-from-python",
    "href": "tutorials/additional_resources/Data_Discovery_CMR_API.html#getting-started-how-to-search-cmr-from-python",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Getting Started: How to search CMR from Python",
    "text": "Getting Started: How to search CMR from Python\nThe first step is to import python packages. We will use:\n- requests This package does most of the work for us accessing the CMR API using HTTP methods. - pprint to pretty print the results of the search.\nA more in-depth tutorial on requests is here\n\nimport requests\nfrom pprint import pprint\n\nTo conduct a search using the CMR API, requests needs the url for the root CMR search endpoint. We’ll assign this url to a python variable as a string.\n\nCMR_OPS = 'https://cmr.earthdata.nasa.gov/search'\n\nCMR allows search by collections, which are datasets, and granules, which are files that contain data. Many of the same search parameters can be used for collections and granules but the type of results returned differ. Search parameters can be found in the API Documentation.\nWhether we search collections or granules is distinguished by adding \"collections\" or \"granules\" to the end of the CMR endpoint URL.\nWe are going to search collections first, so we add \"collections\" to the URL. We are using a python format string in the examples below.\n\nurl = f'{CMR_OPS}/{\"collections\"}'\n\nIn this first example, I want to retrieve a list of collections available from the Earthdata Cloud using the cloud_hosted parameter in the request.\nWe want to get the content in json (pronounced “jason”) format, so I pass a dictionary to the header keyword argument to say that I want results returned as json.\nThe .get() method is used to send this information to the CMR API. get() calls the HTTP method GET.\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                        },\n                        headers={\n                            'Accept': 'application/json',\n                        }\n                       )\n\nThe request returns a Response object.\nTo check that our request was successful we can print the response variable we saved the request to.\n\nresponse\n\n<Response [200]>\n\n\nA 200 response is what we want. This means that the requests was successful. For more information on HTTP status codes see https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\nA more explict way to check the status code is to use the status_code attribute. Both methods return a HTTP status code.\n\nresponse.status_code\n\n200\n\n\nThe response from requests.get returns the results of the search and metadata about those results in the headers.\nMore information about the response object can be found by typing help(response).\nheaders contains useful information in a case-insensitive dictionary. We requested (above) that the information be return in json which means the object return is a dictionary in our Python environment. We’ll iterate through the returned dictionary, looping throught each field (k) and its associated value (v). For more on interating through dictionary object click here.\n\nfor k, v in response.headers.items():\n    print(f'{k}: {v}')\n\nContent-Type: application/json;charset=utf-8\nContent-Length: 4517\nConnection: keep-alive\nDate: Mon, 04 Apr 2022 21:04:20 GMT\nX-Frame-Options: SAMEORIGIN\nAccess-Control-Allow-Origin: *\nX-XSS-Protection: 1; mode=block\nCMR-Request-Id: 6b187d39-c5b8-40f7-9b06-9228ea3c33b2\nStrict-Transport-Security: max-age=31536000\nCMR-Search-After: [0.0,18400.0,\"SENTINEL-1A_OCN\",\"1\",1214472977,1250]\nCMR-Hits: 1169\nAccess-Control-Expose-Headers: CMR-Hits, CMR-Request-Id, X-Request-Id, CMR-Scroll-Id, CMR-Search-After, CMR-Timed-Out, CMR-Shapefile-Original-Point-Count, CMR-Shapefile-Simplified-Point-Count\nX-Content-Type-Options: nosniff\nCMR-Took: 303\nX-Request-Id: zbAtpLRlRe2kPJi8PNdomG7u6C22HPSRbAy5E2m23uYIKga7V8DEfg==\nVary: Accept-Encoding, User-Agent\nContent-Encoding: gzip\nServer: ServerTokens ProductOnly\nX-Cache: Miss from cloudfront\nVia: 1.1 e9c8cd6cad69627cb7c9d88123e6e2cc.cloudfront.net (CloudFront)\nX-Amz-Cf-Pop: HIO50-C2\nX-Amz-Cf-Id: zbAtpLRlRe2kPJi8PNdomG7u6C22HPSRbAy5E2m23uYIKga7V8DEfg==\n\n\nEach item in the dictionary can be accessed in the normal way you access a python dictionary but the keys uniquely case-insensitive. Let’s take a look at the commonly used CMR-Hits key.\n\nresponse.headers['CMR-Hits']\n\n'1169'\n\n\nNote that “cmr-hits” works as well!\n\nresponse.headers['cmr-hits']\n\n'1169'\n\n\nIn some situations the response to your query can return a very large number of result, some of which may not be relevant. We can add additional query parameters to restrict the information returned. We’re going to restrict the search by the provider parameter.\nYou can modify the code below to explore all Earthdata data products hosted by the various providers. When searching by provider, use Cloud Provider to search for cloud-hosted datasets and On-Premises Provider to search for datasets archived at the DAACs. A partial list of providers is given below.\n\n\n\n\n\n\n\n\n\nDAAC\nShort Name\nCloud Provider\nOn-Premises Provider\n\n\n\n\nNSIDC\nNational Snow and Ice Data Center\nNSIDC_CPRD\nNSIDC_ECS\n\n\nGHRC DAAC\nGlobal Hydrometeorology Resource Center\nGHRC_DAAC\nGHRC_DAAC\n\n\nPO DAAC\nPhysical Oceanography Distributed Active Archive Center\nPOCLOUD\nPODAAC\n\n\nASF\nAlaska Satellite Facility\nASF\nASF\n\n\nORNL DAAC\nOak Ridge National Laboratory\nORNL_CLOUD\nORNL_DAAC\n\n\nLP DAAC\nLand Processes Distributed Active Archive Center\nLPCLOUD\nLPDAAC_ECS\n\n\nGES DISC\nNASA Goddard Earth Sciences (GES) Data and Information Services Center (DISC)\nGES_DISC\nGES_DISC\n\n\nOB DAAC\nNASA’s Ocean Biology Distributed Active Archive Center\n\nOB_DAAC\n\n\nSEDAC\nNASA’s Socioeconomic Data and Applications Center\n\nSEDAC\n\n\n\nWe’ll assign the provider to a variable as a string and insert the variable into the parameter argument in the request.\n\nprovider = 'LPCLOUD'\n\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                            'provider': provider,\n                        },\n                        headers={\n                            'Accept': 'application/json'\n                        }\n                       )\nresponse\n\n<Response [200]>\n\n\n\nresponse.headers['cmr-hits']\n\n'3'\n\n\nSearch results are contained in the content part of the Response object. However, response.content returns information in bytes.\n\nresponse.content\n\nb'{\"feed\":{\"updated\":\"2022-04-04T21:04:21.524Z\",\"id\":\"https://cmr.earthdata.nasa.gov:443/search/collections.json?cloud_hosted=True&has_granules=True&provider=LPCLOUD\",\"title\":\"ECHO dataset metadata\",\"entry\":[{\"processing_level_id\":\"3\",\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2013-04-11T00:00:00.000Z\",\"version_id\":\"2.0\",\"updated\":\"2015-12-03T10:57:07.000Z\",\"dataset_id\":\"HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"HLSL30\",\"organizations\":[\"LP DAAC\",\"NASA/IMPACT\"],\"title\":\"HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The Harmonized Landsat and Sentinel-2 (HLS) project provides consistent surface reflectance (SR) and top of atmosphere (TOA) brightness data from the Operational Land Imager (OLI) aboard the joint NASA/USGS Landsat 8 satellite and the Multi-Spectral Instrument (MSI) aboard Europe\\xe2\\x80\\x99s Copernicus Sentinel-2A and Sentinel-2B satellites. The combined measurement enables global observations of the land every 2\\xe2\\x80\\x933 days at 30-meter (m) spatial resolution. The HLS project uses a set of algorithms to obtain seamless products from OLI and MSI that include atmospheric correction, cloud and cloud-shadow masking, spatial co-registration and common gridding, illumination and view angle normalization, and spectral bandpass adjustment.\\\\r\\\\n\\\\r\\\\nThe HLSL30 product provides 30-m Nadir Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Landsat 8 OLI data products. The HLSS30 and HLSL30 products are gridded to the same resolution and Military Grid Reference System ([MGRS](https://hls.gsfc.nasa.gov/products-description/tiling-system/)) tiling system, and thus are \\xe2\\x80\\x9cstackable\\xe2\\x80\\x9d for time series analysis.\\\\r\\\\n\\\\r\\\\nThe HLSL30 product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate file. There are 11 bands included in the HLSL30 product along with one quality assessment (QA) band and four angle bands. See the User Guide for a more detailed description of the individual bands provided in the HLSL30 product.\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2021957657-LPCLOUD\",\"has_formats\":false,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"OTHER\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"LANDSAT-8\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2021957657-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/HLS/HLSL30.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1118/HLS_User_Guide_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/769/HLS_ATBD_V15_provisional.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/768/HLS_Quick_Guide_v011.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-tutorial/browse\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-super-script/browse\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://cmr.earthdata.nasa.gov/browse-scaler/browse_images/granules/G2095313663-LPCLOUD?h=512&w=512\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-bulk-download/browse\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/resources/e-learning/getting-started-with-cloud-native-harmonized-landsat-sentinel-2-hls-data-in-r/\"}]},{\"processing_level_id\":\"3\",\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2015-11-28T00:00:00.000Z\",\"version_id\":\"2.0\",\"updated\":\"2020-03-04T07:19:53.396Z\",\"dataset_id\":\"HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"HLSS30\",\"organizations\":[\"LP DAAC\",\"NASA/IMPACT\"],\"title\":\"HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The Harmonized Landsat and Sentinel-2 (HLS) project provides consistent surface reflectance data from the Operational Land Imager (OLI) aboard the joint NASA/USGS Landsat 8 satellite and the Multi-Spectral Instrument (MSI) aboard Europe\\xe2\\x80\\x99s Copernicus Sentinel-2A and Sentinel-2B satellites. The combined measurement enables global observations of the land every 2\\xe2\\x80\\x933 days at 30-meter (m) spatial resolution. The HLS project uses a set of algorithms to obtain seamless products from OLI and MSI that include atmospheric correction, cloud and cloud-shadow masking, spatial co-registration and common gridding, illumination and view angle normalization, and spectral bandpass adjustment. \\\\r\\\\n\\\\r\\\\nThe HLSS30 product provides 30-m Nadir Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Sentinel-2A and Sentinel-2B MSI data products. The HLSS30 and HLSL30 products are gridded to the same resolution and Military Grid Reference System (MGRS) (https://hls.gsfc.nasa.gov/products-description/tiling-system/) tiling system, and thus are \\xe2\\x80\\x9cstackable\\xe2\\x80\\x9d for time series analysis.\\\\r\\\\n\\\\r\\\\nThe HLSS30 product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate COG. There are 13 bands included in the HLSS30 product along with four angle bands and a quality assessment (QA) band. See the User Guide for a more detailed description of the individual bands provided in the HLSS30 product.\\\\r\\\\n\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2021957295-LPCLOUD\",\"has_formats\":false,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"OTHER\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"Sentinel-2A\",\"Sentinel-2B\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2021957295-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/HLS/HLSS30.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1118/HLS_User_Guide_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/769/HLS_ATBD_V15_provisional.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/768/HLS_Quick_Guide_v011.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-tutorial/browse\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-super-script/browse\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://cmr.earthdata.nasa.gov/browse-scaler/browse_images/granules/G2095548655-LPCLOUD?h=512&w=512\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-bulk-download/browse\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/resources/e-learning/getting-started-with-cloud-native-harmonized-landsat-sentinel-2-hls-data-in-r/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://wiki.earthdata.nasa.gov/pages/viewpage.action?pageId=195432390\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://worldview.earthdata.nasa.gov/?v=-191.12397594071413,-86.52209371542455,182.39111268571813,89.5139950740695&t=2020-10-15-T16%3A46%3A06Z&l=Reference_Labels(hidden),Reference_Features(hidden),Coastlines,HLS_S30_Nadir_BRDF_Adjusted_Reflectance(hidden),VIIRS_NOAA20_CorrectedReflectance_TrueColor(hidden),VIIRS_SNPP_CorrectedReflectance_TrueColor(hidden),MODIS_Aqua_CorrectedReflectance_TrueColor(hidden),MODIS_Terra_CorrectedReflectance_TrueColor&tr=hls_intro \"}]},{\"processing_level_id\":\"3\",\"boxes\":[\"-83 -180 82 180\"],\"time_start\":\"2000-03-01T00:00:00.000Z\",\"version_id\":\"003\",\"updated\":\"2015-09-30T10:42:35.418Z\",\"dataset_id\":\"ASTER Global Digital Elevation Model V003\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"ASTGTM\",\"organizations\":[\"LP DAAC\",\"NASA/JPL/ASTER\"],\"title\":\"ASTER Global Digital Elevation Model V003\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The ASTER Global Digital Elevation Model (GDEM) Version 3 (ASTGTM) provides a global digital elevation model (DEM) of land areas on Earth at a spatial resolution of 1 arc second (approximately 30 meter horizontal posting at the equator).\\\\r\\\\n\\\\r\\\\nThe development of the ASTER GDEM data products is a collaborative effort between National Aeronautics and Space Administration (NASA) and Japan\\xe2\\x80\\x99s Ministry of Economy, Trade, and Industry (METI). The ASTER GDEM data products are created by the Sensor Information Laboratory Corporation (SILC) in Tokyo. \\\\r\\\\n\\\\r\\\\nThe ASTER GDEM Version 3 data product was created from the automated processing of the entire ASTER Level 1A (https://doi.org/10.5067/ASTER/AST_L1A.003) archive of scenes acquired between March 1, 2000, and November 30, 2013. Stereo correlation was used to produce over one million individual scene based ASTER DEMs, to which cloud masking was applied. All cloud screened DEMs and non-cloud screened DEMs were stacked. Residual bad values and outliers were removed. In areas with limited data stacking, several existing reference DEMs were used to supplement ASTER data to correct for residual anomalies. Selected data were averaged to create final pixel values before partitioning the data into 1 degree latitude by 1 degree longitude tiles with a one pixel overlap. To correct elevation values of water body surfaces, the ASTER Global Water Bodies Database (ASTWBD) (https://doi.org/10.5067/ASTER/ASTWBD.001) Version 1 data product was also generated. \\\\r\\\\n\\\\r\\\\nThe geographic coverage of the ASTER GDEM extends from 83\\xc2\\xb0 North to 83\\xc2\\xb0 South. Each tile is distributed in GeoTIFF format and projected on the 1984 World Geodetic System (WGS84)/1996 Earth Gravitational Model (EGM96) geoid. Each of the 22,912 tiles in the collection contain at least 0.01% land area. \\\\r\\\\n\\\\r\\\\nProvided in the ASTER GDEM product are layers for DEM and number of scenes (NUM). The NUM layer indicates the number of scenes that were processed for each pixel and the source of the data.\\\\r\\\\n\\\\r\\\\nWhile the ASTER GDEM Version 3 data products offer substantial improvements over Version 2, users are advised that the products still may contain anomalies and artifacts that will reduce its usability for certain applications. \\\\r\\\\n\\\\r\\\\nImprovements/Changes from Previous Versions \\\\r\\\\n\\xe2\\x80\\xa2 Expansion of acquisition coverage to increase the amount of cloud-free input scenes from about 1.5 million in Version 2 to about 1.88 million scenes in Version 3.\\\\r\\\\n\\xe2\\x80\\xa2 Separation of rivers from lakes in the water body processing. \\\\r\\\\n\\xe2\\x80\\xa2 Minimum water body detection size decreased from 1 km2 to 0.2 km2. \",\"time_end\":\"2013-11-30T23:59:59.999Z\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1711961296-LPCLOUD\",\"has_formats\":false,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"Terra\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q= C1711961296-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/ASTER/ASTGTM.003\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://asterweb.jpl.nasa.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/434/ASTGTM_User_Guide_V3.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://cmr.earthdata.nasa.gov/browse-scaler/browse_images/granules/G1726374754-LPCLOUD?h=512&w=512\"}]}]}}'\n\n\nA more convenient way to work with this information is to use json formatted data. I’m using pretty print pprint to print the data in an easy to read way.\nNote - response.json() will format our response in json - ['feed']['entry'] returns all entries that CMR returned in the request (not the same as CMR-Hits) - [0] returns the first entry. Reminder that python starts indexing at 0, not 1!\n\npprint(response.json()['feed']['entry'][0])\n\n{'archive_center': 'LP DAAC',\n 'boxes': ['-90 -180 90 180'],\n 'browse_flag': True,\n 'collection_data_type': 'OTHER',\n 'consortiums': ['GEOSS', 'EOSDIS'],\n 'coordinate_system': 'CARTESIAN',\n 'data_center': 'LPCLOUD',\n 'dataset_id': 'HLS Landsat Operational Land Imager Surface Reflectance and '\n               'TOA Brightness Daily Global 30m v2.0',\n 'has_formats': False,\n 'has_spatial_subsetting': False,\n 'has_temporal_subsetting': False,\n 'has_transforms': False,\n 'has_variables': False,\n 'id': 'C2021957657-LPCLOUD',\n 'links': [{'href': 'https://search.earthdata.nasa.gov/search?q=C2021957657-LPCLOUD',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n           {'href': 'https://doi.org/10.5067/HLS/HLSL30.002',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/1118/HLS_User_Guide_V2.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/769/HLS_ATBD_V15_provisional.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/768/HLS_Quick_Guide_v011.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-tutorial/browse',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-super-script/browse',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://cmr.earthdata.nasa.gov/browse-scaler/browse_images/granules/G2095313663-LPCLOUD?h=512&w=512',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n           {'href': 'https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-bulk-download/browse',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/resources/e-learning/getting-started-with-cloud-native-harmonized-landsat-sentinel-2-hls-data-in-r/',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'}],\n 'online_access_flag': True,\n 'orbit_parameters': {},\n 'organizations': ['LP DAAC', 'NASA/IMPACT'],\n 'original_format': 'UMM_JSON',\n 'platforms': ['LANDSAT-8'],\n 'processing_level_id': '3',\n 'service_features': {'esi': {'has_formats': False,\n                              'has_spatial_subsetting': False,\n                              'has_temporal_subsetting': False,\n                              'has_transforms': False,\n                              'has_variables': False},\n                      'harmony': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False},\n                      'opendap': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False}},\n 'short_name': 'HLSL30',\n 'summary': 'The Harmonized Landsat and Sentinel-2 (HLS) project provides '\n            'consistent surface reflectance (SR) and top of atmosphere (TOA) '\n            'brightness data from the Operational Land Imager (OLI) aboard the '\n            'joint NASA/USGS Landsat 8 satellite and the Multi-Spectral '\n            'Instrument (MSI) aboard Europe’s Copernicus Sentinel-2A and '\n            'Sentinel-2B satellites. The combined measurement enables global '\n            'observations of the land every 2–3 days at 30-meter (m) spatial '\n            'resolution. The HLS project uses a set of algorithms to obtain '\n            'seamless products from OLI and MSI that include atmospheric '\n            'correction, cloud and cloud-shadow masking, spatial '\n            'co-registration and common gridding, illumination and view angle '\n            'normalization, and spectral bandpass adjustment.\\r\\n'\n            '\\r\\n'\n            'The HLSL30 product provides 30-m Nadir Bidirectional Reflectance '\n            'Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is '\n            'derived from Landsat 8 OLI data products. The HLSS30 and HLSL30 '\n            'products are gridded to the same resolution and Military Grid '\n            'Reference System '\n            '([MGRS](https://hls.gsfc.nasa.gov/products-description/tiling-system/)) '\n            'tiling system, and thus are “stackable” for time series '\n            'analysis.\\r\\n'\n            '\\r\\n'\n            'The HLSL30 product is provided in Cloud Optimized GeoTIFF (COG) '\n            'format, and each band is distributed as a separate file. There '\n            'are 11 bands included in the HLSL30 product along with one '\n            'quality assessment (QA) band and four angle bands. See the User '\n            'Guide for a more detailed description of the individual bands '\n            'provided in the HLSL30 product.',\n 'time_start': '2013-04-11T00:00:00.000Z',\n 'title': 'HLS Landsat Operational Land Imager Surface Reflectance and TOA '\n          'Brightness Daily Global 30m v2.0',\n 'updated': '2015-12-03T10:57:07.000Z',\n 'version_id': '2.0'}\n\n\nThe first response contains a lot more information than we need. We’ll narrow in on a few fields to get a feel for what we have. We’ll print the name of the dataset (dataset_id) and the concept id (id). We can build this variable and print statement like we did above with the url variable.\n\ncollections = response.json()['feed']['entry']\n\n\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} | {collection[\"dataset_id\"]} | {collection[\"id\"]}')\n\nLP DAAC | HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 | C2021957657-LPCLOUD\nLP DAAC | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | C2021957295-LPCLOUD\nLP DAAC | ASTER Global Digital Elevation Model V003 | C1711961296-LPCLOUD\n\n\nIn some situations we may be expecting a certain number of results. Only 10 datasets are return be default. This can be modified by setting the page_size parameter to a different any value less than or equal to 2000 (2000 is the maximum number of results return by CMR). Note, this is different that what we see from CMR-Hits in the header, which is the number of entries found that are available for request."
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR_API.html#searching-for-granules",
    "href": "tutorials/additional_resources/Data_Discovery_CMR_API.html#searching-for-granules",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Searching for Granules",
    "text": "Searching for Granules\nIn NASA speak, Granules are files or groups of files. In this example, we will search for ECO2LSTE version 1 for a specified region of interest and datetime range.\nWe need to change the resource url to look for granules instead of collections\n\nurl = f'{CMR_OPS}/{\"granules\"}'\n\nWe will search by concept_id, temporal, and bounding_box. Details about these search parameters can be found in the CMR API Documentation.\nThe formatting of the values for each parameter is quite specific.\nconcept_id parameter is one to many collections/collection id(s) assigned to a list\ntemporal parameter are dates in ISO 8061 format yyyy-MM-ddTHH:mm:ssZ\nbounding_box parameter are coordinates in the order: lower left longitude, lower left latitude, upper right longitude, upper right latitude\n\ncollection_id = ['C2021957657-LPCLOUD', 'C2021957295-LPCLOUD']\ndate_range = '2020-10-17T00:00:00Z,2020-11-18T23:59:59Z'\nbbox = '-120.45264628,34.51050622,-120.40432448,34.53239876'\n\n\nresponse = requests.get(url, \n                        params={\n                            'concept_id': collection_id,\n                            'temporal': date_range,\n                            'bounding_box': bbox,\n                            'page_size': 200,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.status_code)\n\n200\n\n\n\nprint(response.headers['CMR-Hits'])\n\n10\n\n\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"data_center\"]} | {granule[\"dataset_id\"]} | {granule[\"id\"]}')\n\nLPCLOUD | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | G2167580011-LPCLOUD\nLPCLOUD | HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 | G2152756095-LPCLOUD\nLPCLOUD | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | G2167475882-LPCLOUD\nLPCLOUD | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | G2167295721-LPCLOUD\nLPCLOUD | HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 | G2152682346-LPCLOUD\nLPCLOUD | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | G2166867404-LPCLOUD\nLPCLOUD | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | G2166768089-LPCLOUD\nLPCLOUD | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | G2166693426-LPCLOUD\nLPCLOUD | HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 | G2152120260-LPCLOUD\nLPCLOUD | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | G2165191762-LPCLOUD\n\n\n\npprint(granules[0])\n\n{'browse_flag': True,\n 'collection_concept_id': 'C2021957295-LPCLOUD',\n 'coordinate_system': 'GEODETIC',\n 'data_center': 'LPCLOUD',\n 'dataset_id': 'HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance '\n               'Daily Global 30m v2.0',\n 'day_night_flag': 'DAY',\n 'id': 'G2167580011-LPCLOUD',\n 'links': [{'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B09.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B09.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B09.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B07.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B07.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B07.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B01.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B01.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B01.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B04.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B04.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B04.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B12.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B12.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B12.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.SZA.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.SZA.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.SZA.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B03.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B03.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B03.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B11.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B11.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B11.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B02.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B02.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B02.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B08.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B08.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B08.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B10.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B10.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B10.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.SAA.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.SAA.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.SAA.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.VZA.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.VZA.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.VZA.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B05.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B05.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B05.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B06.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B06.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B06.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B8A.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B8A.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B8A.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.Fmask.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.Fmask.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.Fmask.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.VAA.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.VAA.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.VAA.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0_stac.json',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0_stac.json '\n                     '(EXTENDED METADATA)'},\n           {'href': 's3://lp-prod-public/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0_stac.json',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule (EXTENDED METADATA)'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.cmr.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.cmr.xml '\n                     '(EXTENDED METADATA)'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.cmr.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule (EXTENDED METADATA)'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'api endpoint to retrieve temporary credentials valid for '\n                     'same-region direct s3 access (VIEW RELATED INFORMATION)'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.jpg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.jpg'},\n           {'href': 's3://lp-prod-public/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.jpg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://search.earthdata.nasa.gov/search?q=C2021957295-LPCLOUD',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n           {'href': 'https://doi.org/10.5067/HLS/HLSS30.002',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/1118/HLS_User_Guide_V2.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/769/HLS_ATBD_V15_provisional.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/768/HLS_Quick_Guide_v011.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-tutorial/browse',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-super-script/browse',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-bulk-download/browse',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/resources/e-learning/getting-started-with-cloud-native-harmonized-landsat-sentinel-2-hls-data-in-r/',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'}],\n 'online_access_flag': True,\n 'original_format': 'ECHO10',\n 'polygons': [['34.2068165 -119.6382697 35.1952508 -119.5980052 35.2231329 '\n               '-120.8028976 34.2336951 -120.8289365 34.2068165 -119.6382697']],\n 'producer_granule_id': 'HLS.S30.T10SGD.2020292T184411',\n 'time_end': '2020-10-18T18:54:56.060Z',\n 'time_start': '2020-10-18T18:54:56.060Z',\n 'title': 'HLS.S30.T10SGD.2020292T184411.v2.0',\n 'updated': '2021-11-17T08:22:16.691Z'}"
  },
  {
    "objectID": "tutorials/additional_resources/Finding_collection_s3_location.html#finding-s3-location-information-from-the-po.daac-portal",
    "href": "tutorials/additional_resources/Finding_collection_s3_location.html#finding-s3-location-information-from-the-po.daac-portal",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Finding S3 Location information from the PO.DAAC Portal",
    "text": "Finding S3 Location information from the PO.DAAC Portal\nThe easiest of which is through the PO.DAAC Cloud Dataset Listing page: https://podaac.jpl.nasa.gov/cloud-datasets\n\n\n\nS3 Data Locations from Portal\n\n\nFor each dataset, the ‘Data Access’ tab will have various information, but will always contain the S3 paths listed specifically. Data files will always be found under the ‘protected’ bucket."
  },
  {
    "objectID": "tutorials/additional_resources/Finding_collection_s3_location.html#finding-s3-location-from-earthdata-search",
    "href": "tutorials/additional_resources/Finding_collection_s3_location.html#finding-s3-location-from-earthdata-search",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Finding S3 Location from Earthdata Search",
    "text": "Finding S3 Location from Earthdata Search\nFrom the Earthdata Search Client (search.earthdata.nasa.gov), collection level information can be found by clicking the ‘i’ on a collection search result. An example of this is seen below:\n\n\n\nS3 Data Locations from Search 1\n\n\nOnce on the collection inforamtion screen, the S3 bucket locations can be found by scrolling to the bottom of the information panel. The SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1 example is shown below.\n\n\n\nS3 Data Locations from Search 2"
  },
  {
    "objectID": "tutorials/additional_resources/Finding_collection_s3_location.html#finding-s3-location-from-cmr",
    "href": "tutorials/additional_resources/Finding_collection_s3_location.html#finding-s3-location-from-cmr",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Finding S3 Location from CMR",
    "text": "Finding S3 Location from CMR\nOne can query the collection identifier to get information from CMR:\nhttps://cmr.earthdata.nasa.gov/search/concepts/C2152045877-POCLOUD.umm_json\nThe identifier is found on the PO.DAAC Cloud Data Set Listing page entries, called ‘Collection Concept ID’\nResults returned will look like the following:\n{\n    ...\n    \"DirectDistributionInformation\": {\n        \"Region\": \"us-west-2\",\n        \"S3BucketAndObjectPrefixNames\": [\n            \"podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/\",\n            \"podaac-ops-cumulus-public/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/\"\n        ],\n        \"S3CredentialsAPIEndpoint\": \"https://archive.podaac.earthdata.nasa.gov/s3credentials\",\n        \"S3CredentialsAPIDocumentationURL\": \"https://archive.podaac.earthdata.nasa.gov/s3credentialsREADME\"\n    },\n    ...\n}"
  },
  {
    "objectID": "tutorials/additional_resources/On-prem_Cloud_example.html#accessing-and-harmonizing-data-located-within-and-outside-of-the-nasa-earthdata-cloud",
    "href": "tutorials/additional_resources/On-prem_Cloud_example.html#accessing-and-harmonizing-data-located-within-and-outside-of-the-nasa-earthdata-cloud",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Accessing and harmonizing data located within and outside of the NASA Earthdata Cloud",
    "text": "Accessing and harmonizing data located within and outside of the NASA Earthdata Cloud"
  },
  {
    "objectID": "tutorials/additional_resources/On-prem_Cloud_example.html#summary",
    "href": "tutorials/additional_resources/On-prem_Cloud_example.html#summary",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Summary",
    "text": "Summary\n\nThis tutorial will combine several workflow steps and components from the previous days, demonstrating the process of using the geolocation of data available outside of the Earthdata Cloud to then access coincident variables of cloud-accessible data. This may be a common use case as NASA Earthdata continues to migrate to the cloud, producing a “hybrid” data archive across Amazon Web Services (AWS) and original on-premise data storage systems. Additionally, you may also want to combine field measurements with remote sensing data available on the Earthdata Cloud.\nThis specific example explores the pairing of the ICESat-2 ATL07 Sea Ice Height data product, currently (as of November 2021) available publicly via direct download at the NSIDC DAAC, along with Sea Surface Temperature (SST) from the GHRSST MODIS L2 dataset (MODIS_A-JPL-L2P-v2019.0) available from PO.DAAC on the Earthdata Cloud.\nThe use case we’re looking at today centers over an area north of Greenland for a single day in June, where a melt pond was observed using the NASA OpenAltimetry application. Melt ponds are an important feature of Arctic sea ice dynamics, leading to an decrease in sea ice albedo and other changes in heat balance. Many NASA Earthdata datasets produce variables including sea ice albedo, sea surface temperature, air temperature, and sea ice height, which can be used to better understand these dynamics."
  },
  {
    "objectID": "tutorials/additional_resources/On-prem_Cloud_example.html#requirements",
    "href": "tutorials/additional_resources/On-prem_Cloud_example.html#requirements",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Requirements",
    "text": "Requirements\n\n\nAWS instance running in us-west 2\nEarthdata Login\n.netrc file"
  },
  {
    "objectID": "tutorials/additional_resources/On-prem_Cloud_example.html#learning-objectives",
    "href": "tutorials/additional_resources/On-prem_Cloud_example.html#learning-objectives",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\n\nSearch for data programmatically using the Common Metadata Repository (CMR), determining granule (file) coverage across two datasets over an area of interest.\nDownload data from an on-premise storage system to our cloud environment.\nRead in 1-dimensional trajectory data (ICESat-2 ATL07) into xarray and perform attribute conversions.\nSelect and read in sea surface temperature (SST) data (MODIS_A-JPL-L2P-v2019.0) from the Earthdata Cloud into xarray.\nExtract, resample, and plot coincident SST data based on ICESat-2 geolocation."
  },
  {
    "objectID": "tutorials/additional_resources/On-prem_Cloud_example.html#import-packages",
    "href": "tutorials/additional_resources/On-prem_Cloud_example.html#import-packages",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Import packages",
    "text": "Import packages\n\nimport os\nfrom pathlib import Path\nfrom pprint import pprint\n\n# Access via download\nimport requests\n\n# Access AWS S3\nimport s3fs\n\n# Read and work with datasets\nimport xarray as xr\nimport numpy as np\nimport h5py\n\n# For plotting\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nfrom shapely.geometry import box\n\n# For resampling\nimport pyresample"
  },
  {
    "objectID": "tutorials/additional_resources/On-prem_Cloud_example.html#specify-data-time-range-and-area-of-interest",
    "href": "tutorials/additional_resources/On-prem_Cloud_example.html#specify-data-time-range-and-area-of-interest",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Specify data, time range, and area of interest",
    "text": "Specify data, time range, and area of interest\nWe are going to focus on getting data for an area north of Greenland for a single day in June.\nThese bounding_box and temporal variables will be used for data search, subset, and access below.\nThe same search and access steps for both datasets can be performed via Earthdata Search using the same spatial and temporal filtering options. See the Earthdata Search tutorial for more information on how to use Earthdata Search to discover and access data from the Earthdata Cloud.\n\n# Bounding Box spatial parameter in decimal degree 'W,S,E,N' format.\nbounding_box = '-62.8,81.7,-56.4,83'\n\n# Each date in yyyy-MM-ddTHH:mm:ssZ format; date range in start,end format\ntemporal = '2019-06-22T00:00:00Z,2019-06-22T23:59:59Z'\n\nSee the Data Discovery with CMR tutorial for more details on how to navigate the NASA Common Metadata Repository (CMR) Application Programming Interface, or API. For some background, the CMR catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). The CMR API allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results.\nFor this tutorial, we have already identified the unique identifier, or concept_id for each dataset:\n\nmodis_concept_id = 'C1940473819-POCLOUD'\nicesat2_concept_id = 'C2003771980-NSIDC_ECS'\n\nThis Earthdata Search Project also provides the same data access links that we will identify in the following steps for each dataset (note that you will need an Earthdata Login account to access this project)."
  },
  {
    "objectID": "tutorials/additional_resources/On-prem_Cloud_example.html#search-and-download-icesat-2-atl07-files",
    "href": "tutorials/additional_resources/On-prem_Cloud_example.html#search-and-download-icesat-2-atl07-files",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Search and download ICESat-2 ATL07 files",
    "text": "Search and download ICESat-2 ATL07 files\nPerform a granule search over our time and area of interest. How many granules are returned?\n\ngranule_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n\n\nresponse = requests.get(granule_url,\n                       params={\n                           'concept_id': icesat2_concept_id,\n                           'temporal': temporal,\n                           'bounding_box': bounding_box,\n                           'page_size': 200,\n                       },\n                       headers={\n                           'Accept': 'application/json'\n                       }\n                      )\nprint(response.headers['CMR-Hits'])\n\n2\n\n\nPrint the file names, size, and links:\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"producer_granule_id\"]} {granule[\"granule_size\"]} {granule[\"links\"][0][\"href\"]}')\n\nATL07-01_20190622055317_12980301_004_01.h5 237.0905504227 https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL07.004/2019.06.22/ATL07-01_20190622055317_12980301_004_01.h5\nATL07-01_20190622200154_13070301_004_01.h5 230.9151573181 https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL07.004/2019.06.22/ATL07-01_20190622200154_13070301_004_01.h5\n\n\n\nDownload ATL07 files\nAlthough several services are supported for ICESat-2 data, we are demonstrating direct access through the “on-prem” file system at NSIDC for simplicity.\nSome of these services include: - icepyx - From the icepyx documentation: “icepyx is both a software library and a community composed of ICESat-2 data users, developers, and the scientific community. We are working together to develop a shared library of resources - including existing resources, new code, tutorials, and use-cases/examples - that simplify the process of querying, obtaining, analyzing, and manipulating ICESat-2 datasets to enable scientific discovery.” - NSIDC DAAC Data Access and Service API - The API provided by the NSIDC DAAC allows you to access data programmatically using specific temporal and spatial filters. The same subsetting, reformatting, and reprojection services available on select data sets through NASA Earthdata Search can also be applied using this API. - IceFlow - The IceFlow python library simplifies accessing and combining data from several of NASA’s cryospheric altimetry missions, including ICESat/GLAS, Operation IceBridge, and ICESat-2. In particular, IceFlow harmonizes the various file formats and georeferencing parameters across several of the missions’ data sets, allowing you to analyze data across the multi-decadal time series.\nWe’ve found 2 granules. We’ll download the first one and write it to a file with the same name as the producer_granule_id.\nWe need the url for the granule as well. This is href links we printed out above.\n\nicesat_id = granules[0]['producer_granule_id']\nicesat_url = granules[0]['links'][0]['href']\n\nTo retrieve the granule data, we use the requests.get() method, which will utilize the .netrc file on the backend to authenticate the request against Earthdata Login.\n\nr = requests.get(icesat_url)\n\nThe response returned by requests has the same structure as all the other responses: a header and contents. The header information has information about the response, including the size of the data we downloaded in bytes.\n\nfor k, v in r.headers.items():\n    print(f'{k}: {v}')\n\nDate: Sun, 12 Dec 2021 01:52:31 GMT\nServer: Apache\nVary: User-Agent\nContent-Disposition: attachment\nContent-Length: 248607461\nKeep-Alive: timeout=15, max=100\nConnection: Keep-Alive\n\n\nThe contents needs to be saved to a file. To keep the directory clean, we will create a downloads directory to store the file. We can use a shell command to do this or use the makedirs method from the os package.\n\nos.makedirs(\"downloads\", exist_ok=True)\n\nYou should see a downloads directory in the file browser.\nTo write the data to a file, we use open to open a file. We need to specify that the file is open for writing by using the write-mode w. We also need to specify that we want to write bytes by setting the binary-mode b. This is important because the response contents are bytes. The default mode for open is text-mode. So make sure you use b.\nWe’ll use the with statement context-manager to open the file, write the contents of the response, and then close the file. Once the data in r.content is written sucessfully to the file, or if there is an error, the file is closed by the context-manager.\nWe also need to prepend the downloads path to the filename. We do this using Path from the pathlib package in the standard library.\n\noutfile = Path('downloads', icesat_id)\n\n\nif not outfile.exists():\n    with open(outfile, 'wb') as f:\n        f.write(r.content)\n\nATL07-01_20190622055317_12980301_004_01.h5 is an HDF5 file. xarray can open this but you need to tell it which group to read the data from. In this case we read the sea ice segment height data for ground-track 1 left-beam. You can explore the variable hierarchy in Earthdata Search, by selecting the Customize option under Download Data.\nThis code block performs the following operations: - Extracts the height_segment_height variable from the heights group, along with the dimension variables contained in the higher level sea_ice_segments group, - Convert attributes from bytestrings to strings, - Drops the HDF attribute DIMENSION_LIST, - Sets _FillValue to NaN\n\nvariable_names = [\n    '/gt1l/sea_ice_segments/latitude',\n    '/gt1l/sea_ice_segments/longitude',\n    '/gt1l/sea_ice_segments/delta_time',\n    '/gt1l/sea_ice_segments/heights/height_segment_height'\n    ]\nwith h5py.File(outfile, 'r') as h5:\n    data_vars = {}\n    for varname in variable_names:\n        var = h5[varname]\n        name = varname.split('/')[-1]\n        # Convert attributes\n        attrs = {}\n        for k, v in var.attrs.items():\n            if k != 'DIMENSION_LIST':\n                if isinstance(v, bytes):\n                    attrs[k] = v.decode('utf-8')\n                else:\n                    attrs[k] = v\n        data = var[:]\n        if '_FillValue' in attrs:\n            data = np.where(data < attrs['_FillValue'], data, np.nan)\n        data_vars[name] = (['segment'], data, attrs)\n    is2_ds = xr.Dataset(data_vars)\n    \nis2_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                (segment: 235584)\nDimensions without coordinates: segment\nData variables:\n    latitude               (segment) float64 82.38 82.38 82.38 ... 72.61 72.61\n    longitude              (segment) float64 -55.11 -55.11 ... 145.1 145.1\n    delta_time             (segment) float64 4.642e+07 4.642e+07 ... 4.642e+07\n    height_segment_height  (segment) float32 nan nan nan ... -0.4335 -0.4463xarray.DatasetDimensions:segment: 235584Coordinates: (0)Data variables: (4)latitude(segment)float6482.38 82.38 82.38 ... 72.61 72.61contentType :referenceInformationcoordinates :delta_time longitudedescription :Latitude, WGS84, North=+, Lat of segment centerlong_name :Latitudesource :ATBD, section 4.4standard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0array([82.38431982, 82.38431982, 82.38431982, ..., 72.60984638,\n       72.60977493, 72.60970985])longitude(segment)float64-55.11 -55.11 ... 145.1 145.1contentType :referenceInformationcoordinates :delta_time latitudedescription :Longitude, WGS84, East=+,Lon of segment centerlong_name :Longitudesource :ATBD, section 4.4standard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0array([-55.10896068, -55.10896068, -55.10896068, ..., 145.05396164,\n       145.05392851, 145.05389832])delta_time(segment)float644.642e+07 4.642e+07 ... 4.642e+07CLASS :DIMENSION_SCALENAME :gt1l/sea_ice_segments/delta_timeREFERENCE_LIST :[(<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)]contentType :physicalMeasurementcoordinates :latitude longitudedescription :Number of GPS seconds since the ATLAS SDP epoch. The ATLAS Standard Data Products (SDP) epoch offset is defined within /ancillary_data/atlas_sdp_gps_epoch as the number of GPS seconds between the GPS epoch (1980-01-06T00:00:00.000000Z UTC) and the ATLAS SDP epoch. By adding the offset contained within atlas_sdp_gps_epoch to delta time parameters, the time in gps_seconds relative to the GPS epoch can be computed.long_name :Elapsed GPS secondssource :telemetrystandard_name :timeunits :seconds since 2018-01-01array([46419293.64266939, 46419293.64266939, 46419293.64266939, ...,\n       46419681.87646231, 46419681.87759533, 46419681.87862704])height_segment_height(segment)float32nan nan nan ... -0.4335 -0.4463_FillValue :3.4028235e+38contentType :referenceInformationcoordinates :../delta_time ../latitude ../longitudedescription :Mean height from along-track segment fit detremined by the sea ice algorithm. The sea ice height is relative to the tide-free MSS.long_name :height of segment surfacesource :ATBD, section 4.2.2.4units :metersarray([        nan,         nan,         nan, ..., -0.46550068,\n       -0.43347716, -0.4462675 ], dtype=float32)Attributes: (0)\n\n\n\nis2_ds.height_segment_height.plot() ;"
  },
  {
    "objectID": "tutorials/additional_resources/On-prem_Cloud_example.html#determine-the-ghrsst-modis-l2-granules-returned-from-our-time-and-area-of-interest",
    "href": "tutorials/additional_resources/On-prem_Cloud_example.html#determine-the-ghrsst-modis-l2-granules-returned-from-our-time-and-area-of-interest",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Determine the GHRSST MODIS L2 granules returned from our time and area of interest",
    "text": "Determine the GHRSST MODIS L2 granules returned from our time and area of interest\n\nresponse = requests.get(granule_url, \n                        params={\n                            'concept_id': modis_concept_id,\n                            'temporal': temporal,\n                            'bounding_box': bounding_box,\n                            'page_size': 200,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.headers['CMR-Hits'])\n\n14\n\n\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"title\"]} {granule[\"granule_size\"]} {granule[\"links\"][0][\"href\"]}')\n\n20190622000501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.71552562713623 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622000501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622014501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622014501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622032501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 21.307741165161133 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622032501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622050001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622050001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622050501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.065649032592773 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622050501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622064001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0 18.602201461791992 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0.nc\n20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 18.665077209472656 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622082001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.782299995422363 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622082001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.13440227508545 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622113501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.3239164352417 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622113501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622114001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622114001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622163001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.257243156433105 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622163001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.93498420715332 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc"
  },
  {
    "objectID": "tutorials/additional_resources/On-prem_Cloud_example.html#load-data-into-xarray-via-s3-direct-access",
    "href": "tutorials/additional_resources/On-prem_Cloud_example.html#load-data-into-xarray-via-s3-direct-access",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Load data into xarray via S3 direct access",
    "text": "Load data into xarray via S3 direct access\nOur CMR granule search returned 14 files for our time and area of interest. However, not all granules will be suitable for analysis.\nI’ve identified the image with granule id G1956158784-POCLOUD as a good candidate, this is the 9th granule. In this image, our area of interest is close to nadir. This means that the instantaneous field of view over the area of interest cover a smaller area than at the edge of the image.\nWe are looking for the link for direct download access via s3. This is a url but with a prefix s3://. This happens to be the first href link in the metadata.\nFor a single granule we can cut and paste the s3 link. If we have several granules, the s3 links can be extracted with some simple code.\n\ngranule = granules[9]\n\nfor link in granule['links']:\n    if link['href'].startswith('s3://'):\n        s3_link = link['href']\n        \ns3_link\n\n's3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc'\n\n\n\nGet S3 credentials\nAs with the previous S3 download tutorials we need credentials to access data from s3: access keys and tokens.\n\ns3_credentials = requests.get('https://archive.podaac.earthdata.nasa.gov/s3credentials').json()\n\nEssentially, what we are doing in this step is to “mount” the s3 bucket as a file system. This allows us to treat the S3 bucket in a similar way to a local file system.\n\ns3_fs = s3fs.S3FileSystem(\n    key=s3_credentials[\"accessKeyId\"],\n    secret=s3_credentials[\"secretAccessKey\"],\n    token=s3_credentials[\"sessionToken\"],\n)\n\n\n\nOpen a s3 file\nNow we have the S3FileSystem set up, we can access the granule. xarray cannot open a S3File directly, so we use the open method for the S3FileSystem to open the granule using the endpoint url we extracted from the metadata. We also have to set the mode='rb'. This opens the granule in read-only mode and in byte-mode. Byte-mode is important. By default, open opens a file as text - in this case it would just be a string of characters - and xarray doesn’t know what to do with that.\nWe then pass the S3File object f to xarray.open_dataset. For this dataset, we also have to set decode_cf=False. This switch tells xarray not to use information contained in variable attributes to generate human readable coordinate variables. Normally, this should work for netcdf files but for this particular cloud-hosted dataset, variable attribute data is not in the form expected by xarray. We’ll fix this.\n\nf = s3_fs.open(s3_link, mode='rb')\nmodis_ds = xr.open_dataset(f, decode_cf=False)\n\nIf you click on the Show/Hide Attributes icon (the first document-like icon to the right of coordinate variable metadata) you can see that attributes are one-element arrays containing bytestrings.\n\nmodis_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (nj: 2030, ni: 1354, time: 1)\nCoordinates:\n  * time                     (time) int32 1214042401\nDimensions without coordinates: nj, ni\nData variables:\n    lat                      (nj, ni) float32 ...\n    lon                      (nj, ni) float32 ...\n    sea_surface_temperature  (time, nj, ni) int16 ...\n    sst_dtime                (time, nj, ni) int16 ...\n    quality_level            (time, nj, ni) int8 ...\n    sses_bias                (time, nj, ni) int8 ...\n    sses_standard_deviation  (time, nj, ni) int8 ...\n    l2p_flags                (time, nj, ni) int16 ...\n    chlorophyll_a            (time, nj, ni) float32 ...\n    K_490                    (time, nj, ni) int16 ...\n    wind_speed               (time, nj, ni) int8 ...\n    dt_analysis              (time, nj, ni) int8 ...\nAttributes: (12/49)\n    Conventions:                [b'CF-1.7, ACDD-1.3']\n    title:                      [b'MODIS Aqua L2P SST']\n    summary:                    [b'Sea surface temperature retrievals produce...\n    references:                 [b'GHRSST Data Processing Specification v2r5']\n    institution:                [b'NASA/JPL/OBPG/RSMAS']\n    history:                    [b'MODIS L2P created at JPL PO.DAAC']\n    ...                         ...\n    publisher_email:            [b'ghrsst-po@nceo.ac.uk']\n    processing_level:           [b'L2P']\n    cdm_data_type:              [b'swath']\n    startDirection:             [b'Ascending']\n    endDirection:               [b'Descending']\n    day_night_flag:             [b'Day']xarray.DatasetDimensions:nj: 2030ni: 1354time: 1Coordinates: (1)time(time)int321214042401long_name :[b'reference time of sst file']standard_name :[b'time']units :[b'seconds since 1981-01-01 00:00:00']comment :[b'time of first sensor observation']coverage_content_type :[b'coordinate']array([1214042401], dtype=int32)Data variables: (12)lat(nj, ni)float32...long_name :[b'latitude']standard_name :[b'latitude']units :[b'degrees_north']_FillValue :[-999.]valid_min :[-90.]valid_max :[90.]comment :[b'geographical coordinates, WGS84 projection']coverage_content_type :[b'coordinate'][2748620 values with dtype=float32]lon(nj, ni)float32...long_name :[b'longitude']standard_name :[b'longitude']units :[b'degrees_east']_FillValue :[-999.]valid_min :[-180.]valid_max :[180.]comment :[b'geographical coordinates, WGS84 projection']coverage_content_type :[b'coordinate'][2748620 values with dtype=float32]sea_surface_temperature(time, nj, ni)int16...long_name :[b'sea surface temperature']standard_name :[b'sea_surface_skin_temperature']units :[b'kelvin']_FillValue :[-32767]valid_min :[-1000]valid_max :[10000]comment :[b'sea surface temperature from thermal IR (11 um) channels']scale_factor :[0.005]add_offset :[273.15]source :[b'NASA and University of Miami']coordinates :[b'lon lat']coverage_content_type :[b'physicalMeasurement'][2748620 values with dtype=int16]sst_dtime(time, nj, ni)int16...long_name :[b'time difference from reference time']units :[b'seconds']_FillValue :[-32768]valid_min :[-32767]valid_max :[32767]comment :[b'time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981']coordinates :[b'lon lat']coverage_content_type :[b'referenceInformation'][2748620 values with dtype=int16]quality_level(time, nj, ni)int8...long_name :[b'quality level of SST pixel']_FillValue :[-128]valid_min :[0]valid_max :[5]comment :[b'thermal IR SST proximity confidence value; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value']coordinates :[b'lon lat']flag_values :[0 1 2 3 4 5]flag_meanings :[b'no_data bad_data worst_quality low_quality acceptable_quality best_quality']coverage_content_type :[b'qualityInformation'][2748620 values with dtype=int8]sses_bias(time, nj, ni)int8...long_name :[b'SSES bias error based on proximity confidence flags']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'thermal IR SST bias error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value']scale_factor :[0.15748031]add_offset :[0.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]sses_standard_deviation(time, nj, ni)int8...long_name :[b'SSES standard deviation error based on proximity confidence flags']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'thermal IR SST standard deviation error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value']scale_factor :[0.07874016]add_offset :[10.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]l2p_flags(time, nj, ni)int16...long_name :[b'L2P flags']valid_min :[0]valid_max :[16]comment :[b'These flags can be used to further filter data variables']coordinates :[b'lon lat']flag_meanings :[b'microwave land ice lake river']flag_masks :[ 1  2  4  8 16]coverage_content_type :[b'qualityInformation'][2748620 values with dtype=int16]chlorophyll_a(time, nj, ni)float32...long_name :[b'Chlorophyll Concentration, OC3 Algorithm']units :[b'mg m^-3']_FillValue :[-32767.]valid_min :[0.001]valid_max :[100.]comment :[b'non L2P core field']coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=float32]K_490(time, nj, ni)int16...long_name :[b'Diffuse attenuation coefficient at 490 nm (OBPG)']units :[b'm^-1']_FillValue :[-32767]valid_min :[50]valid_max :[30000]comment :[b'non L2P core field']scale_factor :[0.0002]add_offset :[0.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int16]wind_speed(time, nj, ni)int8...long_name :[b'10m wind speed']standard_name :[b'wind_speed']units :[b'm s-1']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'Wind at 10 meters above the sea surface']scale_factor :[0.2]add_offset :[25.]source :[b'TBD.  Placeholder.  Currently empty']coordinates :[b'lon lat']grid_mapping :[b'TBD']time_offset :[2.]height :[b'10 m']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]dt_analysis(time, nj, ni)int8...long_name :[b'deviation from SST reference climatology']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'TBD']scale_factor :[0.1]add_offset :[0.]source :[b'TBD. Placeholder.  Currently empty']coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]Attributes: (49)Conventions :[b'CF-1.7, ACDD-1.3']title :[b'MODIS Aqua L2P SST']summary :[b'Sea surface temperature retrievals produced at the NASA OBPG for the MODIS Aqua sensor.  These have been reformatted to GHRSST GDS specifications by the JPL PO.DAAC']references :[b'GHRSST Data Processing Specification v2r5']institution :[b'NASA/JPL/OBPG/RSMAS']history :[b'MODIS L2P created at JPL PO.DAAC']comment :[b'L2P Core without DT analysis or other ancillary fields; Day, Start Node:Ascending, End Node:Descending; WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value; Refined']license :[b'GHRSST and PO.DAAC protocol allow data use as free and open.']id :[b'MODIS_A-JPL-L2P-v2019.0']naming_authority :[b'org.ghrsst']product_version :[b'2019.0']uuid :[b'f6e1f61d-c4a4-4c17-8354-0c15e12d688b']gds_version_id :[b'2.0']netcdf_version_id :[b'4.1']date_created :[b'20200221T085224Z']file_quality_level :[3]spatial_resolution :[b'1km']start_time :[b'20190622T100001Z']time_coverage_start :[b'20190622T100001Z']stop_time :[b'20190622T100459Z']time_coverage_end :[b'20190622T100459Z']northernmost_latitude :[89.9862]southernmost_latitude :[66.2723]easternmost_longitude :[-45.9467]westernmost_longitude :[152.489]source :[b'MODIS sea surface temperature observations for the OBPG']platform :[b'Aqua']sensor :[b'MODIS']metadata_link :[b'http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MODIS_A-JPL-L2P-v2019.0']keywords :[b'Oceans > Ocean Temperature > Sea Surface Temperature']keywords_vocabulary :[b'NASA Global Change Master Directory (GCMD) Science Keywords']standard_name_vocabulary :[b'NetCDF Climate and Forecast (CF) Metadata Convention']geospatial_lat_units :[b'degrees_north']geospatial_lat_resolution :[0.01]geospatial_lon_units :[b'degrees_east']geospatial_lon_resolution :[0.01]acknowledgment :[b'The MODIS L2P sea surface temperature data are sponsored by NASA']creator_name :[b'Ed Armstrong, JPL PO.DAAC']creator_email :[b'edward.m.armstrong@jpl.nasa.gov']creator_url :[b'http://podaac.jpl.nasa.gov']project :[b'Group for High Resolution Sea Surface Temperature']publisher_name :[b'The GHRSST Project Office']publisher_url :[b'http://www.ghrsst.org']publisher_email :[b'ghrsst-po@nceo.ac.uk']processing_level :[b'L2P']cdm_data_type :[b'swath']startDirection :[b'Ascending']endDirection :[b'Descending']day_night_flag :[b'Day']\n\n\nTo fix this, we need to extract array elements as scalars, and convert those scalars from bytestrings to strings. We use the decode method to do this. The bytestrings are encoded as utf-8, which is a unicode character format. This is the default encoding for decode but we’ve included it as an argument to be explicit.\nNot all attributes are bytestrings. Some are floats. Take a look at _FillValue, and valid_min and valid_max. To avoid an error, we use the isinstance function to check if the value of an attributes is type bytes - a bytestring. If it is, then we decode it. If not, we just extract the scalar and do nothing else.\nWe also fix the global attributes.\n\ndef fix_attributes(da):\n    '''Decodes bytestring attributes to strings'''\n    for attr, value in da.attrs.items():\n        if isinstance(value[0], bytes):\n            da.attrs[attr] = value[0].decode('utf-8')\n        else:\n            da.attrs[attr] = value[0]\n    return\n\n# Fix variable attributes\nfor var in modis_ds.variables:\n    da = modis_ds[var]\n    fix_attributes(da)\n            \n# Fix global attributes\nfix_attributes(modis_ds)\n\nWith this done, we can use the xarray function decode_cf to convert the attributes.\n\nmodis_ds = xr.decode_cf(modis_ds)\n\n\nmodis_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (nj: 2030, ni: 1354, time: 1)\nCoordinates:\n    lat                      (nj, ni) float32 ...\n    lon                      (nj, ni) float32 ...\n  * time                     (time) datetime64[ns] 2019-06-22T10:00:01\nDimensions without coordinates: nj, ni\nData variables:\n    sea_surface_temperature  (time, nj, ni) float32 ...\n    sst_dtime                (time, nj, ni) timedelta64[ns] ...\n    quality_level            (time, nj, ni) float32 ...\n    sses_bias                (time, nj, ni) float32 ...\n    sses_standard_deviation  (time, nj, ni) float32 ...\n    l2p_flags                (time, nj, ni) int16 ...\n    chlorophyll_a            (time, nj, ni) float32 ...\n    K_490                    (time, nj, ni) float32 ...\n    wind_speed               (time, nj, ni) float32 ...\n    dt_analysis              (time, nj, ni) float32 ...\nAttributes: (12/49)\n    Conventions:                CF-1.7, ACDD-1.3\n    title:                      MODIS Aqua L2P SST\n    summary:                    Sea surface temperature retrievals produced a...\n    references:                 GHRSST Data Processing Specification v2r5\n    institution:                NASA/JPL/OBPG/RSMAS\n    history:                    MODIS L2P created at JPL PO.DAAC\n    ...                         ...\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L2P\n    cdm_data_type:              swath\n    startDirection:             Ascending\n    endDirection:               Descending\n    day_night_flag:             Dayxarray.DatasetDimensions:nj: 2030ni: 1354time: 1Coordinates: (3)lat(nj, ni)float32...long_name :latitudestandard_name :latitudeunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[2748620 values with dtype=float32]lon(nj, ni)float32...long_name :longitudestandard_name :longitudeunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[2748620 values with dtype=float32]time(time)datetime64[ns]2019-06-22T10:00:01long_name :reference time of sst filestandard_name :timecomment :time of first sensor observationcoverage_content_type :coordinatearray(['2019-06-22T10:00:01.000000000'], dtype='datetime64[ns]')Data variables: (10)sea_surface_temperature(time, nj, ni)float32...long_name :sea surface temperaturestandard_name :sea_surface_skin_temperatureunits :kelvinvalid_min :-1000valid_max :10000comment :sea surface temperature from thermal IR (11 um) channelssource :NASA and University of Miamicoverage_content_type :physicalMeasurement[2748620 values with dtype=float32]sst_dtime(time, nj, ni)timedelta64[ns]...long_name :time difference from reference timevalid_min :-32767valid_max :32767comment :time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981coverage_content_type :referenceInformation[2748620 values with dtype=timedelta64[ns]]quality_level(time, nj, ni)float32...long_name :quality level of SST pixelvalid_min :0valid_max :5comment :thermal IR SST proximity confidence value; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valueflag_values :0flag_meanings :no_data bad_data worst_quality low_quality acceptable_quality best_qualitycoverage_content_type :qualityInformation[2748620 values with dtype=float32]sses_bias(time, nj, ni)float32...long_name :SSES bias error based on proximity confidence flagsunits :kelvinvalid_min :-127valid_max :127comment :thermal IR SST bias error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]sses_standard_deviation(time, nj, ni)float32...long_name :SSES standard deviation error based on proximity confidence flagsunits :kelvinvalid_min :-127valid_max :127comment :thermal IR SST standard deviation error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]l2p_flags(time, nj, ni)int16...long_name :L2P flagsvalid_min :0valid_max :16comment :These flags can be used to further filter data variablesflag_meanings :microwave land ice lake riverflag_masks :1coverage_content_type :qualityInformation[2748620 values with dtype=int16]chlorophyll_a(time, nj, ni)float32...long_name :Chlorophyll Concentration, OC3 Algorithmunits :mg m^-3valid_min :0.001valid_max :100.0comment :non L2P core fieldcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]K_490(time, nj, ni)float32...long_name :Diffuse attenuation coefficient at 490 nm (OBPG)units :m^-1valid_min :50valid_max :30000comment :non L2P core fieldcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]wind_speed(time, nj, ni)float32...long_name :10m wind speedstandard_name :wind_speedunits :m s-1valid_min :-127valid_max :127comment :Wind at 10 meters above the sea surfacesource :TBD.  Placeholder.  Currently emptygrid_mapping :TBDtime_offset :2.0height :10 mcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]dt_analysis(time, nj, ni)float32...long_name :deviation from SST reference climatologyunits :kelvinvalid_min :-127valid_max :127comment :TBDsource :TBD. Placeholder.  Currently emptycoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]Attributes: (49)Conventions :CF-1.7, ACDD-1.3title :MODIS Aqua L2P SSTsummary :Sea surface temperature retrievals produced at the NASA OBPG for the MODIS Aqua sensor.  These have been reformatted to GHRSST GDS specifications by the JPL PO.DAACreferences :GHRSST Data Processing Specification v2r5institution :NASA/JPL/OBPG/RSMAShistory :MODIS L2P created at JPL PO.DAACcomment :L2P Core without DT analysis or other ancillary fields; Day, Start Node:Ascending, End Node:Descending; WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value; Refinedlicense :GHRSST and PO.DAAC protocol allow data use as free and open.id :MODIS_A-JPL-L2P-v2019.0naming_authority :org.ghrsstproduct_version :2019.0uuid :f6e1f61d-c4a4-4c17-8354-0c15e12d688bgds_version_id :2.0netcdf_version_id :4.1date_created :20200221T085224Zfile_quality_level :3spatial_resolution :1kmstart_time :20190622T100001Ztime_coverage_start :20190622T100001Zstop_time :20190622T100459Ztime_coverage_end :20190622T100459Znorthernmost_latitude :89.9862southernmost_latitude :66.2723easternmost_longitude :-45.9467westernmost_longitude :152.489source :MODIS sea surface temperature observations for the OBPGplatform :Aquasensor :MODISmetadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MODIS_A-JPL-L2P-v2019.0keywords :Oceans > Ocean Temperature > Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventiongeospatial_lat_units :degrees_northgeospatial_lat_resolution :0.01geospatial_lon_units :degrees_eastgeospatial_lon_resolution :0.01acknowledgment :The MODIS L2P sea surface temperature data are sponsored by NASAcreator_name :Ed Armstrong, JPL PO.DAACcreator_email :edward.m.armstrong@jpl.nasa.govcreator_url :http://podaac.jpl.nasa.govproject :Group for High Resolution Sea Surface Temperaturepublisher_name :The GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L2Pcdm_data_type :swathstartDirection :AscendingendDirection :Descendingday_night_flag :Day\n\n\nLet’s make a quick plot to take a look at the sea_surface_temperature variable.\n\nmodis_ds.sea_surface_temperature.plot() ;\n\n\n\n\n\n\nPlot MODIS and ICESat-2 data on a map\n\nmap_proj = ccrs.NorthPolarStereo()\n\nfig = plt.figure(figsize=(10,5))\nax = fig.add_subplot(projection=map_proj)\nax.coastlines()\n\n# Plot MODIS sst, save object as sst_img, so we can add colorbar\nsst_img = ax.pcolormesh(modis_ds.lon, modis_ds.lat, modis_ds.sea_surface_temperature[0,:,:], \n                        vmin=240, vmax=270,  # Set max and min values for plotting\n                        cmap='viridis', shading='auto',   # shading='auto' to avoid warning\n                        transform=ccrs.PlateCarree())  # coords are lat,lon but map if NPS \n\n# Plot IS2 surface height \nis2_img = ax.scatter(is2_ds.longitude, is2_ds.latitude,\n                     c=is2_ds.height_segment_height, \n                     vmax=1.5,  # Set max height to plot\n                     cmap='Reds', alpha=0.6, s=2,\n                     transform=ccrs.PlateCarree())\n\n# Add colorbars\nfig.colorbar(sst_img, label='MODIS SST (K)')\nfig.colorbar(is2_img, label='ATL07 Height (m)')\n\n\n<matplotlib.colorbar.Colorbar at 0x7fb3944adb50>\n\n\n\n\n\n\n\nExtract SST coincident with ICESat-2 track\nThe MODIS SST is swath data, not a regularly-spaced grid of sea surface temperatures. ICESat-2 sea surface heights are irregularly spaced segments along one ground-track traced by the ATLAS instrument on-board ICESat-2. Fortunately, pyresample allows us to resample swath data.\npyresample has many resampling methods. We’re going to use the nearest neighbour resampling method, which is implemented using a k-dimensional tree algorithm or K-d tree. K-d trees are data structures that improve search efficiency for large data sets.\nThe first step is to define the geometry of the ICESat-2 and MODIS data. To do this we use the latitudes and longitudes of the datasets.\n\nis2_geometry = pyresample.SwathDefinition(lons=is2_ds.longitude,\n                                          lats=is2_ds.latitude)\n\n\nmodis_geometry = pyresample.SwathDefinition(lons=modis_ds.lon, lats=modis_ds.lat)\n\nWe then implement the resampling method, passing the two geometries we have defined, the data array we want to resample - in this case sea surface temperature, and a search radius. The resampling method expects a numpy.Array rather than an xarray.DataArray, so we use values to get the data as a numpy.Array.\nWe set the search radius to 1000 m. The MODIS data is nominally 1km spacing.\n\nsearch_radius=1000.\nfill_value = np.nan\nis2_sst = pyresample.kd_tree.resample_nearest(\n    modis_geometry,\n    modis_ds.sea_surface_temperature.values,\n    is2_geometry,\n    search_radius,\n    fill_value=fill_value\n)\n\n\nis2_sst\n\narray([263.375, 263.375, 263.375, ...,     nan,     nan,     nan],\n      dtype=float32)\n\n\n\nis2_ds['sea_surface_temperature'] = xr.DataArray(is2_sst, dims='segment')\nis2_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (segment: 235584)\nDimensions without coordinates: segment\nData variables:\n    latitude                 (segment) float64 82.38 82.38 82.38 ... 72.61 72.61\n    longitude                (segment) float64 -55.11 -55.11 ... 145.1 145.1\n    delta_time               (segment) float64 4.642e+07 4.642e+07 ... 4.642e+07\n    height_segment_height    (segment) float32 nan nan nan ... -0.4335 -0.4463\n    sea_surface_temperature  (segment) float32 263.4 263.4 263.4 ... nan nan nanxarray.DatasetDimensions:segment: 235584Coordinates: (0)Data variables: (5)latitude(segment)float6482.38 82.38 82.38 ... 72.61 72.61contentType :referenceInformationcoordinates :delta_time longitudedescription :Latitude, WGS84, North=+, Lat of segment centerlong_name :Latitudesource :ATBD, section 4.4standard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0array([82.38431982, 82.38431982, 82.38431982, ..., 72.60984638,\n       72.60977493, 72.60970985])longitude(segment)float64-55.11 -55.11 ... 145.1 145.1contentType :referenceInformationcoordinates :delta_time latitudedescription :Longitude, WGS84, East=+,Lon of segment centerlong_name :Longitudesource :ATBD, section 4.4standard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0array([-55.10896068, -55.10896068, -55.10896068, ..., 145.05396164,\n       145.05392851, 145.05389832])delta_time(segment)float644.642e+07 4.642e+07 ... 4.642e+07CLASS :DIMENSION_SCALENAME :gt1l/sea_ice_segments/delta_timeREFERENCE_LIST :[(<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)]contentType :physicalMeasurementcoordinates :latitude longitudedescription :Number of GPS seconds since the ATLAS SDP epoch. The ATLAS Standard Data Products (SDP) epoch offset is defined within /ancillary_data/atlas_sdp_gps_epoch as the number of GPS seconds between the GPS epoch (1980-01-06T00:00:00.000000Z UTC) and the ATLAS SDP epoch. By adding the offset contained within atlas_sdp_gps_epoch to delta time parameters, the time in gps_seconds relative to the GPS epoch can be computed.long_name :Elapsed GPS secondssource :telemetrystandard_name :timeunits :seconds since 2018-01-01array([46419293.64266939, 46419293.64266939, 46419293.64266939, ...,\n       46419681.87646231, 46419681.87759533, 46419681.87862704])height_segment_height(segment)float32nan nan nan ... -0.4335 -0.4463_FillValue :3.4028235e+38contentType :referenceInformationcoordinates :../delta_time ../latitude ../longitudedescription :Mean height from along-track segment fit detremined by the sea ice algorithm. The sea ice height is relative to the tide-free MSS.long_name :height of segment surfacesource :ATBD, section 4.2.2.4units :metersarray([        nan,         nan,         nan, ..., -0.46550068,\n       -0.43347716, -0.4462675 ], dtype=float32)sea_surface_temperature(segment)float32263.4 263.4 263.4 ... nan nan nanarray([263.375, 263.375, 263.375, ...,     nan,     nan,     nan],\n      dtype=float32)Attributes: (0)\n\n\n\n\nPlot SST and Height along track\nThis is a quick plot of the extracted data. We’re using matplotlib so we can use latitude as the x-value:\n\nis2_ds = is2_ds.set_coords(['latitude'])\n\nfig, ax1 = plt.subplots(figsize=(15, 7))\nax1.set_xlim(82.,88.)\nax1.plot(is2_ds.latitude, is2_ds.sea_surface_temperature, \n         color='orange', label='SST', zorder=3)\nax1.set_ylabel('SST (K)')\n\nax2 = ax1.twinx()\nax2.plot(is2_ds.latitude, is2_ds.height_segment_height, label='Height')\nax2.set_ylabel('Height (m)')\n\nfig.legend()\n\n<matplotlib.legend.Legend at 0x7fb39fcd8040>"
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#step-1.-login-to-the-hub",
    "href": "tutorials/Getting_started_setup.html#step-1.-login-to-the-hub",
    "title": "Setup for tutorials",
    "section": "Step 1. Login to the Hub",
    "text": "Step 1. Login to the Hub\nPlease go to Jupyter Hub and Log in with your GitHub Account, and select “Small”.\nAlternatively, you can also click this badge to launch the Hub:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: It takes a few minutes for the Hub to load. Please be patient!\n\nWhile the Hub loads, we’ll:\n\nDiscuss cloud environments\n\nSee how my Desktop is setup\n\nDiscuss python and conda environments\n\nThen, when the Hub is loaded, we’ll get oriented in the Hub."
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#discussion-cloud-environment",
    "href": "tutorials/Getting_started_setup.html#discussion-cloud-environment",
    "title": "Setup for tutorials",
    "section": "Discussion: Cloud environment",
    "text": "Discussion: Cloud environment\nA brief overview. See NASA Openscapes Cloud Environment in the 2021-Cloud-Hackathon book for more detail.\n\nCloud infrastructure\n\nCloud: AWS us-west-2\n\nData: AWS S3 (cloud) and NASA DAAC data centers (on-prem).\n\nCloud compute environment: 2i2c Jupyterhub deployment\n\nIDE: JupyterLab"
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#discussion-my-desktop-setup",
    "href": "tutorials/Getting_started_setup.html#discussion-my-desktop-setup",
    "title": "Setup for tutorials",
    "section": "Discussion: My desktop setup",
    "text": "Discussion: My desktop setup\nI’ll screenshare to show and/or talk through how I have oriented the following software we’re using:\n\nWorkshop Book\nSlack"
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#discussion-python-and-conda-environments",
    "href": "tutorials/Getting_started_setup.html#discussion-python-and-conda-environments",
    "title": "Setup for tutorials",
    "section": "Discussion: Python and Conda environments",
    "text": "Discussion: Python and Conda environments\nWhy Python?\n\n\n\nPython Data Stack. Source: Jake VanderPlas, “The State of the Stack,” SciPy Keynote (SciPy 2015).\n\n\nDefault Python Environment:\nWe’ve set up the Python environment with conda.\n\n\n\n\n\n\nConda environment\n\n\n\n\n\nname: openscapes\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.9\n  - pangeo-notebook\n  - awscli~=1.20\n  - boto3~=1.19\n  - gdal~=3.3\n  - rioxarray~=0.8\n  - xarray~=0.19\n  - h5netcdf~=0.11\n  - netcdf4~=1.5\n  - h5py~=2.10\n  - geoviews~=1.9\n  - matplotlib-base~=3.4\n  - hvplot~=0.7\n  - pyproj~=3.2\n  - bqplot~=0.12\n  - geopandas~=0.10\n  - zarr~=2.10\n  - cartopy~=0.20\n  - shapely==1.7.1\n  - pyresample~=1.22\n  - joblib~=1.1\n  - pystac-client~=0.3\n  - s3fs~=2021.7\n  - ipyleaflet~=0.14\n  - sidecar~=0.5\n  - jupyterlab-geojson~=3.1\n  - jupyterlab-git\n  - jupyter-resource-usage\n  - ipympl~=0.6\n  - conda-lock~=0.12\n  - pooch~=1.5\n  - pip\n  - pip:\n    - tqdm\n    - harmony-py\n    - earthdata\n    - zarr-eosdis-store\n\n\n\n\nBash terminal and installed software\nLibraries that are available from the terminal\n\ngdal 3.3 commands ( gdalinfo, gdaltransform…)\nhdf5 commands ( h5dump, h5ls..)\nnetcdf4 commands (ncdump, ncinfo …)\njq (parsing json files or streams from curl)\ncurl (fetch resources from the web)\nawscli (AWS API client, to interact with AWS cloud services)\nvim (editor)\ntree ( directory tree)\nmore …\n\n\n\nUpdating the environment\nScientific Python is a vast space and we only included libraries that are needed in our tutorials. Our default environment can be updated to include any Python library that’s available on pip or conda.\nThe project used to create our default environment is called corn (as it can include many Python kernels).\nIf we want to update a library or install a whole new environment we need to open an issue on this repository.\n\n\ncorn 🌽"
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#step-2.-jupyterhub-orientation",
    "href": "tutorials/Getting_started_setup.html#step-2.-jupyterhub-orientation",
    "title": "Setup for tutorials",
    "section": "Step 2. JupyterHub orientation",
    "text": "Step 2. JupyterHub orientation\nNow that the Hub is loaded, let’s get oriented.\n\n\n\n\n\n\nFirst impressions\n\nLauncher & the big blue button\n“home directory”"
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#step-3.-navigate-to-the-workshop-folder",
    "href": "tutorials/Getting_started_setup.html#step-3.-navigate-to-the-workshop-folder",
    "title": "Setup for tutorials",
    "section": "Step 3. Navigate to the Workshop folder",
    "text": "Step 3. Navigate to the Workshop folder\nThe workshop folder 2022-ECOSTRESS-Cloud-Workshop is in the shared folder on JupyterHub."
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#jupyter-notebooks",
    "href": "tutorials/Getting_started_setup.html#jupyter-notebooks",
    "title": "Setup for tutorials",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nLet’s get oriented to Jupyter notebooks, which we’ll use in all the tutorials."
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#how-do-i-end-my-session",
    "href": "tutorials/Getting_started_setup.html#how-do-i-end-my-session",
    "title": "Setup for tutorials",
    "section": "How do I end my session?",
    "text": "How do I end my session?\n(Also see How do I end my Openscapes session? Will I lose all of my work?) When you are finished working for the day it is important to explicitly log out of your Openscapes session. The reason for this is it will save money and is a good habit to be in. When you keep a session active it uses up AWS resources and keeps a series of virtual machines deployed.\nStopping the server happens automatically when you log out, so navigate to “File -> Log Out” and click “Log Out”!\n\n\n\nhub-control-panel-button (credit: UW Hackweek)\n\n\n!!! NOTE “logging out” - Logging out will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day."
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials Overview",
    "section": "",
    "text": "These tutorials are a combination of narrative, links, code, and outputs. They have been developed for live demos during the Workshop, and are available for self-paced learning.\nHands-on exercises will be executed from a Jupyter Lab instance in 2i2c. Please pass along your Github Username to get access.\nTutorials are markdown (.md) and Jupyter (.ipynb) notebooks, and are available on GitHub:\nhttps://github.com/NASA-Openscapes/2022-ECOSTRESS-Cloud-Workshop/tree/main/tutorials."
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR_API_EDL_Token.html#summary",
    "href": "tutorials/Data_Discovery_CMR_API_EDL_Token.html#summary",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we will walk through how to search for Earthdata data collections and granules. Along the way we will explore the available search parameters, information return, and specific contrains when using the CMR API. Our object is to identify assets to access that we would downloaded, or perform S3 direct access, within an analysis workflow\nWe will be querying CMR for ECOSTRESS collections/granules to identify assets we would downloaded, or perform S3 direct access, within an analysis workflow"
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR_API_EDL_Token.html#requirements",
    "href": "tutorials/Data_Discovery_CMR_API_EDL_Token.html#requirements",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Requirements",
    "text": "Requirements\n\n1. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n2. ECOSTRESS Early Adopter\nECOSTRESS build 7 is only open to individuals identified as early adopters. As such ECOSTRESS discovery and access is managed by an access control list. If you are not on the access control list, you will not be able to complete the exercise as written below."
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR_API_EDL_Token.html#learning-objectives",
    "href": "tutorials/Data_Discovery_CMR_API_EDL_Token.html#learning-objectives",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nunderstand what CMR/CMR API is and what CMR/CMR API can be used for\nhow to use the requests package to search data collections and granules\nhow to use an Earthdata Login token to search for data with access control lists\nhow to parse the results of these searches."
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR_API_EDL_Token.html#what-is-cmr",
    "href": "tutorials/Data_Discovery_CMR_API_EDL_Token.html#what-is-cmr",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "What is CMR",
    "text": "What is CMR\nCMR is the Common Metadata Repository. It catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). It is the backend of Earthdata Search, the GUI search interface you are probably familiar with. More information about CMR can be found here.\nUnfortunately, the GUI for Earthdata Search is not accessible from a cloud instance - at least not without some work. Earthdata Search is also not immediately reproducible. What I mean by that is if you create a search using the GUI you would have to note the search criteria (date range, search area, collection name, etc), take a screenshot, copy the search url, or save the list of data granules returned by the search, in order to recreate the search. This information would have to be re-entered each time you or someone else wanted to do the search. You could make typos or other mistakes. A cleaner, reproducible solution is to search CMR programmatically using the CMR API."
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR_API_EDL_Token.html#what-is-the-cmr-api",
    "href": "tutorials/Data_Discovery_CMR_API_EDL_Token.html#what-is-the-cmr-api",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "What is the CMR API",
    "text": "What is the CMR API\nAPI stands for Application Programming Interface. It allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results."
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR_API_EDL_Token.html#getting-started-how-to-search-cmr-from-python",
    "href": "tutorials/Data_Discovery_CMR_API_EDL_Token.html#getting-started-how-to-search-cmr-from-python",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Getting Started: How to search CMR from Python",
    "text": "Getting Started: How to search CMR from Python\nThe first step is to import python packages. We will use:\n- requests This package does most of the work for us accessing the CMR API using HTTP methods. - pprint to pretty print the results of the search.\nA more in-depth tutorial on requests is here\n\nimport requests\nimport json\nfrom pprint import pprint\n\nTo conduct a search using the CMR API, requests needs the url for the root CMR search endpoint. We’ll assign this url to a python variable as a string.\n\nCMR_OPS = 'https://cmr.earthdata.nasa.gov/search'\n\nCMR allows search by collections, which are datasets, and granules, which are files that contain data. Many of the same search parameters can be used for collections and granules but the type of results returned differ. Search parameters can be found in the API Documentation.\nWhether we search collections or granules is distinguished by adding \"collections\" or \"granules\" to the end of the CMR endpoint URL.\nWe are going to search collections first, so we add \"collections\" to the URL. We are using a python format string in the examples below.\n\nurl = f'{CMR_OPS}/{\"collections\"}'\n\nIn this first example, I want to retrieve a list of ECOSTRESS collections in the Earthdata Cloud. This includes ECOSTRESS collections from build 7 which at the time of this tutorial, is hidden to all except early adopters. Because of this, an extra parameter needs to be passed in each CMR request that indicates you are part of the access list. An Earthdata Login token will be passed to the token parameter, which is generated using your Earthdata Login credentials.\nTwo options are available to generate an Earthdata Login token. 1. Generate a token from the Earthdata Login interface by logging into Earthdata Login and Click Generate Token. 2. Programatically generate an Earthdata Login token. Use the NASA_Earthdata_Login_Token notebook to generate and save a token for use in this notebook.\nWe can read in our token after it has been generated and saved using the NASA_Earthdata_Login_Token notebook. The json file produce can be found here: /home/jovyan/.hidden_dir/edl_token.json. We’ll read to token into a variable named token.\n\nwith open('../../.hidden_dir/edl_token.json') as js:\n    token = json.load(js)['access_token']\n\nWe’ll want to get the content in json (pronounced “jason”) format, so I pass a dictionary to the header keyword argument to say that I want results returned as json.\nThe .get() method is used to send this information to the CMR API. get() calls the HTTP method GET.\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                        },\n                        headers={\n                            'Accept': 'application/json',\n                        }\n                       )\n\nThe request returns a Response object.\nTo check that our request was successful we can print the response variable we saved the request to.\n\nresponse\n\nA 200 response is what we want. This means that the requests was successful. For more information on HTTP status codes see https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\nA more explict way to check the status code is to use the status_code attribute. Both methods return a HTTP status code.\n\nresponse.status_code\n\nThe response from requests.get returns the results of the search and metadata about those results in the headers.\nMore information about the response object can be found by typing help(response).\nheaders contains useful information in a case-insensitive dictionary. We requested (above) that the information be return in json which means the object return is a dictionary in our Python environment. We’ll iterate through the returned dictionary, looping throught each field (k) and its associated value (v). For more on interating through dictionary object click here.\n\nfor k, v in response.headers.items():\n    print(f'{k}: {v}')\n\nEach item in the dictionary can be accessed in the normal way you access a python dictionary but the keys uniquely case-insensitive. Let’s take a look at the commonly used CMR-Hits key.\n\nresponse.headers['CMR-Hits']\n\nNote that “cmr-hits” works as well!\n\nresponse.headers['cmr-hits']\n\nIn some situations the response to your query can return a very large number of result, some of which may not be relevant. We can add additional query parameters to restrict the information returned. We’re going to restrict the search by the provider parameter.\nYou can modify the code below to explore all Earthdata data products hosted by the various providers. When searching by provider, use Cloud Provider to search for cloud-hosted datasets and On-Premises Provider to search for datasets archived at the DAACs. A partial list of providers is given below.\n\n\n\n\n\n\n\n\n\nDAAC\nShort Name\nCloud Provider\nOn-Premises Provider\n\n\n\n\nNSIDC\nNational Snow and Ice Data Center\nNSIDC_CPRD\nNSIDC_ECS\n\n\nGHRC DAAC\nGlobal Hydrometeorology Resource Center\nGHRC_DAAC\nGHRC_DAAC\n\n\nPO DAAC\nPhysical Oceanography Distributed Active Archive Center\nPOCLOUD\nPODAAC\n\n\nASF\nAlaska Satellite Facility\nASF\nASF\n\n\nORNL DAAC\nOak Ridge National Laboratory\nORNL_CLOUD\nORNL_DAAC\n\n\nLP DAAC\nLand Processes Distributed Active Archive Center\nLPCLOUD\nLPDAAC_ECS\n\n\nGES DISC\nNASA Goddard Earth Sciences (GES) Data and Information Services Center (DISC)\nGES_DISC\nGES_DISC\n\n\nOB DAAC\nNASA’s Ocean Biology Distributed Active Archive Center\n\nOB_DAAC\n\n\nSEDAC\nNASA’s Socioeconomic Data and Applications Center\n\nSEDAC\n\n\n\nWe’ll assign the provider to a variable as a string and insert the variable into the parameter argument in the request. We’ll also assign the term ‘ECOSTRESS’ to a varible so we don’t need to repeatedly add it to the requests parameters.\n\nprovider = 'LPCLOUD'\nproject = 'ECOSTRESS'\n\n\nheaders = {\n    'Authorization': f'Bearer {token}',\n    'Accept': 'application/json',\n}\n\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                            'provider': provider,\n                            'project': project,\n                        },\n                        headers=headers\n                       )\nresponse\n\n\nresponse.headers['cmr-hits']\n\nSearch results are contained in the content part of the Response object. However, response.content returns information in bytes.\n\nresponse.content\n\nA more convenient way to work with this information is to use json formatted data. I’m using pretty print pprint to print the data in an easy to read way.\nNote - response.json() will format our response in json - ['feed']['entry'] returns all entries that CMR returned in the request (not the same as CMR-Hits) - [0] returns the first entry. Reminder that python starts indexing at 0, not 1!\n\npprint(response.json()['feed']['entry'][0])\n\nThe first response contains a lot more information than we need. We’ll narrow in on a few fields to get a feel for what we have. We’ll print the name of the dataset (dataset_id) and the concept id (id). We can build this variable and print statement like we did above with the url variable.\n\ncollections = response.json()['feed']['entry']\n\n\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} | {collection[\"dataset_id\"]} | {collection[\"id\"]}')\n\nIn some situations we may be expecting a certain number of results. Note here that we only have 10 datasets are printed. We know from CMR-Hits that there are more than 10 datasets. This is because CMR restricts the number of results returned by each query. The default is 10 but it can be set to a maximum of 2000. We’ll set the page_size parameter to 25 so we return all results in a single query.\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                            'provider': provider,\n                            'project': project,\n                            'page_size': 25\n                        },\n                        headers=headers\n                       )\nresponse\n\n\nresponse.headers['cmr-hits']\n\nNow, when we can re-run our for loop for the collections we now have all of the available collections listed.\n\ncollections = response.json()['feed']['entry']\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} | {collection[\"dataset_id\"]} | {collection[\"id\"]}')"
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR_API_EDL_Token.html#searching-for-granules",
    "href": "tutorials/Data_Discovery_CMR_API_EDL_Token.html#searching-for-granules",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Searching for Granules",
    "text": "Searching for Granules\nIn NASA speak, Granules are files or groups of files. In this example, we will search for ECO2LSTE version 1 for a specified region of interest and datetime range.\nWe need to change the resource url to look for granules instead of collections\n\nurl = f'{CMR_OPS}/{\"granules\"}'\n\nWe will search by concept_id, temporal, and bounding_box. Details about these search parameters can be found in the CMR API Documentation.\nThe formatting of the values for each parameter is quite specific.\nTemporal parameters are in ISO 8061 format yyyy-MM-ddTHH:mm:ssZ.\nBounding box coordinates are lower left longitude, lower left latitude, upper right longitude, upper right latitude.\n\ncollection_id = 'C2076090826-LPCLOUD'\ndate_range = '2022-04-01T00:00:00Z,2022-04-30T23:59:59Z'\nbbox = '-120.45264628,34.51050622,-120.40432448,34.53239876'\n\n\nresponse = requests.get(url, \n                        params={\n                            'concept_id': collection_id,\n                            'temporal': date_range,\n                            'bounding_box': bbox,\n                            'token': token,\n                            'page_size': 200\n                            },\n                        headers=headers\n                       )\nprint(response.status_code)\n\n\nprint(response.headers['CMR-Hits'])\n\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"data_center\"]} | {granule[\"dataset_id\"]} | {granule[\"id\"]}')\n\n\npprint(granules[0])"
  },
  {
    "objectID": "tutorials/Data_Discovery_CMR_API_EDL_Token.html#get-urls-to-cloud-data-assets",
    "href": "tutorials/Data_Discovery_CMR_API_EDL_Token.html#get-urls-to-cloud-data-assets",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Get URLs to cloud data assets",
    "text": "Get URLs to cloud data assets\n\nhttps_urls = [l['href'] for l in granules[0]['links'] if 'https' in l['href'] and '.tif' in l['href']]\nhttps_urls\n\n\ns3_urls = [l['href'] for l in granules[0]['links'] if 's3' in l['href'] and '.tif' in l['href']]\ns3_urls"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Welcome",
    "text": "Welcome\n\nWelcome to the 2022 ECOSTRESS Cloud Workshop hosted by NASA’s Land Processes Distributed Activate Archive (LP DAAC) with support from NASA Openscapes.\nThe workshop will take place virtually daily on April 12 and 13, 2022 from 2pm-5:30pm PST (UTC-7)."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "About",
    "text": "About\n\nWorkshop Goal\nThe goal of the workshop is expose ECOSTRESS data users to ECOSTRESS version 2 (v2) data products in the cloud. Learning objectives focus on how to find and access ECOSTRESS v2 data from Earthdata Cloud either by downloading or accessing the data on the cloud. The LP DAAC is the NASA archive for ECOSTRESS data products. ECOSTRESS v2 data products will hosted in the NASA Earthdata Cloud, hosted in AWS.\n\n\nWorkshop Description\nThe goal of the workshop is expose ECOSTRESS data users to ECOSTRESS version 2 data products in the cloud. Learning objectives focus on how to finda and access ECOSTRESS version 2 data from Earthdata Cloud either by downloading or accessing the data on the cloud. The LP DAAC is the NASA archive for ECOSTRESS data products. ECOSTRESS version 2 data products will hosted in the NASA Earthdata Cloud, hosted in AWS.\nThe workshop will demonstrate how to find, access, and download ECOSTRESS v2 data from the Earthdata Cloud. Participants will learn how to search for and download data from NASA’s Earthdata Search Client, a graphical user interface (GUI) for search, discovery, and download application for also EOSDIS data assets. Participants will also learn how to perform in-could data search, access, and processing routines where no data download is required, and data analysis can take place next to the data in the cloud.\n\n\nWorkshop Outcomes\nAt the end of the two days, participants should be able to find and access ECOSTRESS v2 data in the NASA Earthdata Cloud (hosted in AWS). Workshop materials will be available for future reference following the completion of the workshop/ECOSTRESS Science Team meeting\n\nNOTE: ECOSTRESS v2 data will only be available to approved individuals. Please work with Christine Lee (christine.m.lee@jpl.nasa.gov) to have your name added to the allowlist."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n2022 ECOSTRESS Cloud Workshop is hosted by NASA’s LP DAAC with support from the NASA Openscapes Project, with cloud computing infrastructure by 2i2c."
  }
]